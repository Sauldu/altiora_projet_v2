# Structure et contenu du projet

## Arborescence du Projet

```
/
|-- .gitignore
|-- README.md
|-- anythingllm
|   |-- agents
|   |   |-- batch_processor.json
|   |   |-- code_generator.json
|   |   \-- qa_analyst.json
|   |-- chat_templates
|   |   |-- analysis_template.json
|   |   |-- batch_template.json
|   |   |-- coding_template.json
|   |   \-- debug_template.json
|   |-- datasources_priority.md
|   |-- embedding_config.json
|   |-- legacy
|   |   \-- Altiora.html
|   |-- prompts
|   |   |-- qwen3_direct.txt
|   |   |-- qwen3_thinking.txt
|   |   \-- starcoder2_generation.txt
|   \-- workspace_config.json
|-- backend
|   |-- __init__.py
|   |-- altiora
|   |   |-- __init__.py
|   |   |-- __version__.py
|   |   |-- api
|   |   |   |-- __init__.py
|   |   |   |-- app.py
|   |   |   |-- dependencies.py
|   |   |   |-- gateway
|   |   |   |   \-- api_gateway.py
|   |   |   |-- middleware
|   |   |   |   |-- __init__.py
|   |   |   |   |-- advanced_rate_limiter.py
|   |   |   |   |-- cache_middleware.py
|   |   |   |   \-- rbac_middleware.py
|   |   |   |-- openapi.py
|   |   |   \-- v1
|   |   |       |-- __init__.py
|   |   |       |-- analysis.py
|   |   |       |-- batch.py
|   |   |       |-- health.py
|   |   |       |-- models.py
|   |   |       \-- schemas.py
|   |   |-- config
|   |   |   |-- __init__.py
|   |   |   |-- settings.py
|   |   |   \-- validators.py
|   |   |-- core
|   |   |   |-- __init__.py
|   |   |   |-- batch
|   |   |   |   |-- __init__.py
|   |   |   |   |-- batch_processor.py
|   |   |   |   |-- scheduler.py
|   |   |   |   \-- strategies.py
|   |   |   |-- factories
|   |   |   |   \-- model_factory.py
|   |   |   |-- feedback
|   |   |   |   \-- feedback_collector.py
|   |   |   |-- models
|   |   |   |   |-- __init__.py
|   |   |   |   |-- ensemble
|   |   |   |   |   |-- __init__.py
|   |   |   |   |   |-- multi_model.py
|   |   |   |   |   \-- router.py
|   |   |   |   |-- model_swapper.py
|   |   |   |   |-- qwen3
|   |   |   |   |   |-- __init__.py
|   |   |   |   |   |-- cache_optimizer.py
|   |   |   |   |   |-- config.py
|   |   |   |   |   |-- model_manager.py
|   |   |   |   |   |-- prompt_builder.py
|   |   |   |   |   \-- quantizer.py
|   |   |   |   \-- starcoder2
|   |   |   |       |-- __init__.py
|   |   |   |       |-- code_generator.py
|   |   |   |       |-- config.py
|   |   |   |       \-- model_manager.py
|   |   |   |-- modules
|   |   |   |   |-- __init__.py
|   |   |   |   |-- psychodesign
|   |   |   |   |   |-- __init__.py
|   |   |   |   |   |-- altiora_core.py
|   |   |   |   |   |-- personality_evolution.py
|   |   |   |   |   \-- personality_quiz.py
|   |   |   |   |-- voice_anythingllm.py
|   |   |   |   \-- voice_assistant.py
|   |   |   |-- optimization
|   |   |   |   \-- memory_optimizer.py
|   |   |   |-- orchestrator.py
|   |   |   |-- plugins
|   |   |   |   |-- __init__.py
|   |   |   |   |-- plugin_interface.py
|   |   |   |   \-- plugin_system.py
|   |   |   |-- policies
|   |   |   |   |-- __init__.py
|   |   |   |   |-- business_rules.py
|   |   |   |   |-- excel_policy.py
|   |   |   |   |-- privacy_policy.py
|   |   |   |   \-- toxicity_policy.py
|   |   |   |-- post_processing
|   |   |   |   |-- __init__.py
|   |   |   |   |-- code_validator.py
|   |   |   |   |-- excel_formatter.py
|   |   |   |   \-- output_sanitizer.py
|   |   |   |-- qa
|   |   |   |   |-- multimodal_qa.py
|   |   |   |   \-- qa_system.py
|   |   |   |-- training
|   |   |   |   |-- __init__.py
|   |   |   |   |-- advanced_trainer.py
|   |   |   |   |-- auto_fine_tuner.py
|   |   |   |   \-- feedback_system.py
|   |   |   \-- validation
|   |   |       \-- continuous_validator.py
|   |   |-- infrastructure
|   |   |   |-- __init__.py
|   |   |   |-- audit
|   |   |   |   |-- __init__.py
|   |   |   |   |-- audit_logger.py
|   |   |   |   |-- decorator.py
|   |   |   |   |-- models.py
|   |   |   |   |-- ring_buffer.py
|   |   |   |   |-- rotation.py
|   |   |   |   \-- writer.py
|   |   |   |-- database.py
|   |   |   |-- events
|   |   |   |   |-- __init__.py
|   |   |   |   \-- event_bus.py
|   |   |   |-- monitoring
|   |   |   |   |-- __init__.py
|   |   |   |   |-- health.py
|   |   |   |   |-- healthcheck.py
|   |   |   |   \-- metrics
|   |   |   |       \-- model_metrics.py
|   |   |   |-- queue
|   |   |   |   \-- redis_queue.py
|   |   |   |-- repositories
|   |   |   |   |-- __init__.py
|   |   |   |   |-- base_repository.py
|   |   |   |   \-- scenario_repository.py
|   |   |   \-- scaling
|   |   |       |-- __init__.py
|   |   |       \-- auto_scaler.py
|   |   |-- monitoring
|   |   |   \-- memory_monitor.py
|   |   |-- security
|   |   |   |-- auth
|   |   |   |   |-- __init__.py
|   |   |   |   |-- dependencies.py
|   |   |   |   \-- jwt_handler.py
|   |   |   |-- guardrails
|   |   |   |   |-- __init__.py
|   |   |   |   |-- admin_control_system.py
|   |   |   |   |-- admin_dashboard.py
|   |   |   |   |-- emergency_handler.py
|   |   |   |   |-- ethical_safeguards.py
|   |   |   |   |-- interaction_guardrail.py
|   |   |   |   |-- policy_enforcer.py
|   |   |   |   \-- toxicity_guardrail.py
|   |   |   |-- rbac
|   |   |   |   |-- __init__.py
|   |   |   |   |-- manager.py
|   |   |   |   \-- models.py
|   |   |   \-- secret.py
|   |   |-- services
|   |   |   |-- __init__.py
|   |   |   |-- alm
|   |   |   |   |-- README.md
|   |   |   |   |-- __init__.py
|   |   |   |   \-- alm_service.py
|   |   |   |-- base.py
|   |   |   |-- dash
|   |   |   |   |-- README.md
|   |   |   |   \-- app.py
|   |   |   |-- excel
|   |   |   |   |-- README.md
|   |   |   |   |-- __init__.py
|   |   |   |   \-- excel_service.py
|   |   |   |-- ocr
|   |   |   |   |-- README.md
|   |   |   |   |-- __init__.py
|   |   |   |   |-- doctopus_ocr
|   |   |   |   |   \-- __init__.py
|   |   |   |   \-- ocr_wrapper.py
|   |   |   \-- playwright
|   |   |       |-- README.md
|   |   |       |-- __init__.py
|   |   |       |-- optimized_runner.py
|   |   |       \-- playwright_runner.py
|   |   \-- utils
|   |       |-- __init__.py
|   |       |-- errors.py
|   |       |-- helpers.py
|   |       \-- logging.py
|   \-- tests
|       |-- __init__.py
|       |-- conftest.py
|       |-- integration
|       |   |-- __init__.py
|       |   |-- conftest.py
|       |   |-- makefile
|       |   |-- test_full_pipeline.py
|       |   |-- test_performance.py
|       |   \-- test_services_integration.py
|       |-- performance
|       |   |-- config.yaml
|       |   |-- test_load_testing.py
|       |   |-- test_pipeline_load.py
|       |   \-- test_redis_performance.py
|       |-- regression
|       |   |-- __init__.py
|       |   |-- regression_config.yaml
|       |   |-- run_regression.py
|       |   \-- test_regression_suite.py
|       |-- test_admin_control.py
|       |-- test_altiora_core.py
|       |-- test_ethical_safeguards.py
|       |-- test_fine_tuning.py
|       |-- test_integration.py
|       |-- test_interfaces.py
|       |-- test_model_swapper.py
|       |-- test_ocr_wrapper.py
|       |-- test_orchestrator.py
|       |-- test_personality_quiz.py
|       |-- test_playwright_runner.py
|       |-- test_retry_handler.py
|       \-- test_services.py
|-- cli
|   |-- altiora_cli
|   |   |-- __init__.py
|   |   |-- commands
|   |   |   |-- __init__.py
|   |   |   |-- batch.py
|   |   |   |-- benchmark.py
|   |   |   |-- chat.py
|   |   |   |-- doctor.py
|   |   |   |-- init.py
|   |   |   |-- models.py
|   |   |   |-- quickstart.py
|   |   |   |-- start.py
|   |   |   |-- test.py
|   |   |   \-- voice_anything.py
|   |   \-- main.py
|   |-- requirements.txt
|   \-- setup.py
|-- configs
|   \-- prometheus.yml
|-- docker
|   \-- docker-compose.yml
|-- docs
|   |-- ADVANCED_CONFIGURATION.md
|   |-- API_REFERENCE.md
|   |-- ARCHITECTURE.md
|   |-- DEVELOPER_GUIDE.md
|   |-- MODEL_TRAINING.md
|   |-- conf.py
|   |-- env-documentation.md
|   |-- examples
|   |   |-- Altiora.html
|   |   |-- login_test.py
|   |   |-- minimal_sfd.txt
|   |   \-- test_scenarios.json
|   |-- generate_docs.py
|   |-- installation_guide.md
|   |-- requirements.txt
|   \-- source
|       \-- guides
|           |-- deployment.md
|           |-- migration_v2.md
|           \-- prompting_qwen3.md
|-- requirements
|   |-- base.txt
|   |-- dev.txt
|   \-- prod.txt
|-- requirements.txt
\-- scripts
    |-- backup
    |   |-- backup_redis.sh
    |   \-- configure_swap.sh
    |-- models
    |   |-- qwen3_modelfile
    |   \-- starcoder2_modelfile
    |-- monitoring
    |   |-- audit_query.py
    |   |-- diagnose_ollama.py
    |   \-- generate_performance_report.py
    |-- setup
    |   |-- cpu_optimization_script.py
    |   |-- create_directories.sh
    |   |-- create_ephemeral_env.sh
    |   |-- generate_keys.py
    |   |-- run_performance_tests.sh
    |   |-- setup_integration_tests.sh
    |   |-- start_dev.sh
    |   \-- validate_setup.py
    \-- training
        \-- auto_fine_tuner.py
```

---

## Fichier : `.gitignore`

```text
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
pip-log.txt
pip-delete-this-directory.txt

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Docker
.dockerignore

# Logs
*.log
logs/

# Data & cache
.cache/
.cache.db
*.tmp
*.temp
data/models/
data/temp/
results/*.json
reports/*.html
artifacts/*.zip

# Secrets
.env
.env.local

# Jupyter
.ipynb_checkpoints/

# Pytest
.coverage
.pytest_cache/
htmlcov/
.tox/
```

---

## Fichier : `README.md`

```markdown
# üöÄ Altiora V2 ‚Äì Assistant IA de Gestion de Tests Logiciels

> **Projet RAG + Orchestration IA + Micro-services optimis√© pour 32 GB de RAM**

---

## üìñ Vue d‚Äôensemble

Altiora est une plate-forme open-source qui automatise l‚Äôanalyse, la g√©n√©ration et le suivi des tests logiciels.  
Elle combine deux grands mod√®les open-source et une interface **RAG** (AnythingLLM) :

| Mod√®le | R√¥le | Charge m√©moire |
|--------|------|----------------|
| **Qwen3-32B** | R√©flexion, analyse m√©tier, orchestration | ~20 GB |
| **StarCoder2-15B** | G√©n√©ration de code (test, scripts, Playwright) | ~15 GB |
| **DocToPus** | OCR / extraction de texte dans PDF & images | < 1 GB |

> ‚ö†Ô∏è **Contrainte 32 GB RAM** : un seul mod√®le charg√© √† la fois gr√¢ce au **ModelSwapper**.

---

## üß© Fonctionnalit√©s principales

| Fonctionnalit√© | Description |
|----------------|-------------|
| üìä **Analyse intelligente** | Qwen3 analyse les specs, les rapports de bug et propose des sc√©narios de test |
| ü§ñ **G√©n√©ration de code** | StarCoder2 produit des tests Playwright, des scripts Python ou des suites de tests Excel |
| üîç **Recherche s√©mantique** | AnythingLLM interroge documents PDF, images, feuilles Excel, code source |
| üì¶ **Batch processing** | Traite par lots des dossiers complets de specs ou de rapports |
| üîê **S√©curit√© renforc√©e** | Guardrails √©thique, filtrage des injections, RBAC, audit complet |
| üîÑ **Swap m√©moire** | Chargement dynamique des mod√®les pour rester sous 32 GB |

---

## üèóÔ∏è Architecture
```

---

## Fichier : `requirements.txt`

```text
# requirements.txt - Fichier principal avec toutes les d√©pendances
# Ce fichier rassemble les 3 environnements (base, dev, prod) pour installation globale
# En production, utilisez plut√¥t les fichiers sp√©cifiques

# === CORE API ===
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
httpx==0.25.2

# === IA & Mod√®les ===
llama-cpp-python==0.2.20
transformers==4.52.1
torch==2.1.1
datasets==2.14.6
peft==0.7.1
bitsandbytes==0.41.3          # Pour quantification GPU
accelerate==0.25.0

# === Infrastructure ===
redis==5.0.1
aioredis==2.0.1
asyncio-mqtt==0.15.0

# === Data & Files ===
pandas==2.1.4
openpyxl==3.1.2
aiofiles==23.2.1
zstandard==0.22.0
pypdf2==3.0.1
python-docx==1.1.0

# === OCR & Vision ===
pytesseract==0.3.10
pillow==11.3.0
opencv-python==4.8.1.78

# === Testing & Quality ===
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
black==24.3.0
ruff==0.1.6
mypy==1.7.1

# === Monitoring & Logging ===
prometheus-client==0.19.0
psutil==5.9.6
structlog==23.2.0
python-json-logger==2.0.7

# === Security & Validation ===
cryptography>=42.0.5,<46
python-jose[cryptography]==3.4.0
passlib[bcrypt]==1.7.4
email-validator==2.1.0

# === Development Tools ===
pre-commit==3.5.0
ipython==8.18.1
rich==13.7.0

# === Optional GPU Support ===
nvidia-ml-py==12.535.133

# === Environment ===
python-dotenv==1.0.0
click==8.1.7
typer==0.9.0
setuptools~=80.9.0
PyYAML~=6.0.2
aiohttp~=3.12.14
SQLAlchemy~=2.0.41
tenacity~=8.2.3
numpy~=1.26.4
SpeechRecognition~=3.14.3
PyJWT~=2.10.1
plotly~=6.2.0
pyttsx3~=2.99
```

---

## Fichier : `anythingllm\datasources_priority.md`

```markdown
# Guide d'Indexation Prioritaire - AnythingLLM "Altiora Knowledge"

## üéØ Priorit√© 1 - Documents Critiques (Indexer en premier)

### 1. Sp√©cifications & Cas de Test
- `data/scenarios/` - **TOUTES les sp√©cifications de test**
- `docs/examples/` - **Exemples de cas d'usage**
- `data/training/` - **Donn√©es d'entra√Ænement**

### 2. Documentation Technique
- `docs/api/` - **Documentation API compl√®te**
- `docs/guides/` - **Guides utilisateur et techniques**

## üéØ Priorit√© 2 - Scripts & Configurations

### 3. Scripts d'Automatisation
- `scripts/setup/` - **Scripts d'installation et configuration**
- `scripts/monitoring/` - **Scripts de monitoring et audit**

### 4. Configuration Syst√®me
- `config/base.yaml` - **Configuration principale**
- `.env.example` - **Variables d'environnement**

## üéØ Priorit√© 3 - Mod√®les & Assets

### 5. Mod√®les IA
- `models/` - **Mod√®les GGUF Qwen3 et Starcoder2**
- `data/models/` - **Mod√®les ML sauvegard√©s**

### 6. Ressources Compl√©mentaires
- `frontend/src/components/` - **Composants UI**
- `docs/examples/playwright_scripts/` - **Scripts exemples**

## üìã Instructions d'Indexation

1. **D√©marrer par Priorit√© 1** (documents critiques)
2. **Indexer par dossier complet** (pas fichier par fichier)
3. **V√©rifier l'embedding** apr√®s chaque dossier
4. **Tester les requ√™tes** avec cas d'usage r√©el

## üîÑ Maintenance

- **Re-indexer** apr√®s chaque mise √† jour critique
- **Versionner** les embeddings avec git LFS
- **Monitore** la taille du vector store

```

---

## Fichier : `anythingllm\embedding_config.json`

```json
{
  "provider": "ollama",
  "model": "nomic-embed-text",
  "dimensions": 768,
  "chunk_size": 1000,
  "chunk_overlap": 200,
  "batch_size": 8
}
```

---

## Fichier : `anythingllm\workspace_config.json`

```json
{
  "name": "Altiora Knowledge",
  "description": "Workspace d√©di√© √† l'assistant Altiora pour la gestion du cycle de vie des tests logiciels. Contient sp√©cifications, sc√©narios de test, scripts et documentation.",
  "vector_db": "lancedb",
  "embedding": {
    "provider": "ollama",
    "model": "nomic-embed-text",
    "dimensions": 768,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "batch_size": 8
  },
  "max_document_size_mb": 50,
  "auto_sync": true,
  "default_prompt_mode": "thinking"
}
```

---

## Fichier : `anythingllm\agents\batch_processor.json`

```json
{
  "name": "Batch Processor",
  "description": "Agent sp√©cialis√© dans les traitements batch.",
  "system_prompt": "Tu es un architecte batch senior. Expert en orchestration de traitements volumineux, gestion de queues, retry, monitoring, alerting et scaling.",
  "tools": [
    "batch_scheduler",
    "queue_manager",
    "retry_engine",
    "monitoring_dashboard"
  ],
  "model": "qwen3-32b",
  "temperature": 0.5
}
```

---

## Fichier : `anythingllm\agents\code_generator.json`

```json
{
  "name": "Code Generator",
  "description": "Agent sp√©cialis√© dans la g√©n√©ration de code de test.",
  "system_prompt": "Tu es un d√©veloppeur test senior. Expert en Python, JavaScript, TypeScript. Ma√Ætrise pytest, selenium, playwright, cypress. G√©n√®re code maintenable et testable.",
  "tools": [
    "code_generator",
    "syntax_validator",
    "dependency_checker",
    "security_linter"
  ],
  "model": "qwen3-32b",
  "temperature": 0.3
}
```

---

## Fichier : `anythingllm\agents\qa_analyst.json`

```json
{
  "name": "QA Analyst",
  "description": "Agent sp√©cialis√© dans l‚Äôanalyse QA et la g√©n√©ration de tests.",
  "system_prompt": "Tu es un analyste QA senior avec 15 ans d‚Äôexp√©rience. Expert en techniques de test : unitaires, d‚Äôint√©gration, fonctionnels, non-fonctionnels, s√©curit√©, performance. Conna√Æt les standards ISTQB, TMap, ATDD.",
  "tools": [
    "test_case_generator",
    "coverage_analyzer",
    "performance_profiler",
    "vulnerability_scanner"
  ],
  "model": "qwen3-32b",
  "temperature": 0.4
}
```

---

## Fichier : `anythingllm\chat_templates\analysis_template.json`

```json
{
  "name": "Analyse QA",
  "description": "Template pour l‚Äôanalyse d√©taill√©e de sp√©cifications et la g√©n√©ration de cas de test.",
  "system_prompt": "Tu es un expert QA senior. Analyse la sp√©cification fournie et produis des cas de test complets couvrant fonctionnels, non-fonctionnels, edge-cases et r√©gression.",
  "user_prompt": "Voici la sp√©cification √† analyser :\n\n{context}\n\nProduis une analyse QA structur√©e avec priorit√©s.",
  "model": "qwen3-32b",
  "temperature": 0.4,
  "max_tokens": 4000
}
```

---

## Fichier : `anythingllm\chat_templates\batch_template.json`

```json
{
  "name": "Traitement Batch",
  "description": "Template pour l‚Äôorchestration de traitements batch.",
  "system_prompt": "Tu es un architecte batch. D√©finis des strat√©gies de traitement par lots optimis√©es, avec retry, monitoring et gestion d‚Äôerreurs.",
  "user_prompt": "Planifie un traitement batch pour :\n\n{description}\n\nVolumes: {volumes}\nD√©lai: {deadline}",
  "model": "qwen3-32b",
  "temperature": 0.5,
  "max_tokens": 4000
}
```

---

## Fichier : `anythingllm\chat_templates\coding_template.json`

```json
{
  "name": "G√©n√©ration Code",
  "description": "Template pour g√©n√©rer du code de test ou d‚Äôautomatisation.",
  "system_prompt": "Tu es un d√©veloppeur test senior. G√©n√®re du code Python ou JavaScript conforme aux standards PEP8 / ESLint, avec tests unitaires et documentation.",
  "user_prompt": "G√©n√®re le code pour : {request}\n\nLangage: {language}\nFramework: {framework}\nContraintes: {constraints}",
  "model": "qwen3-32b",
  "temperature": 0.2,
  "max_tokens": 4000
}
```

---

## Fichier : `anythingllm\chat_templates\debug_template.json`

```json
{
  "name": "D√©bogage",
  "description": "Template pour l‚Äôanalyse de logs et le d√©bogage.",
  "system_prompt": "Tu es un expert debugging. Analyse les logs fournis, identifie la cause racine et propose des corrections.",
  "user_prompt": "Logs √† analyser :\n```\n{logs}\n```\n\nComportement attendu : {expected}\nEnvironnement : {environment}",
  "model": "qwen3-32b",
  "temperature": 0.3,
  "max_tokens": 4000
}
```

---

## Fichier : `anythingllm\legacy\Altiora.html`

```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Altiora - Interface Conversationnelle Avanc√©e</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <style>
        :root {
            /* Light Theme Colors */
            --bg-primary: #f8f9fa;
            --bg-secondary: #ffffff;
            --bg-tertiary: #e9ecef;
            --text-primary: #212529;
            --text-secondary: #6c757d;
            --border-color: #dee2e6;
            --primary: #4361ee;
            --primary-dark: #3a56d4;
            --secondary: #7209b7;
            --success: #4cc9f0;
            --warning: #f72585;
            --shadow: 0 4px 12px rgba(0,0,0,0.08);
            
            /* Dark Theme Colors */
            --dark-bg-primary: #121212;
            --dark-bg-secondary: #1e1e1e;
            --dark-bg-tertiary: #2d2d2d;
            --dark-text-primary: #f8f9fa;
            --dark-text-secondary: #adb5bd;
            --dark-border-color: #495057;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', system-ui, sans-serif;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            height: 100vh;
            display: flex;
            flex-direction: column;
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-theme {
            background-color: var(--dark-bg-primary);
            color: var(--dark-text-primary);
        }

        .header {
            background: linear-gradient(120deg, var(--primary), var(--secondary));
            color: white;
            padding: 1rem 1.5rem;
            box-shadow: var(--shadow);
            z-index: 10;
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-size: 1.5rem;
            font-weight: 700;
        }

        .logo i {
            font-size: 1.75rem;
        }

        .theme-controls {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .theme-toggle {
            background: rgba(255, 255, 255, 0.2);
            border: none;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s;
        }

        .theme-toggle:hover {
            background: rgba(255, 255, 255, 0.3);
        }

        .features {
            display: flex;
            gap: 1.5rem;
            font-size: 0.9rem;
        }

        .feature-tag {
            background: rgba(255, 255, 255, 0.2);
            padding: 0.25rem 0.75rem;
            border-radius: 999px;
            backdrop-filter: blur(4px);
        }

        .chat-container {
            flex: 1;
            display: flex;
            max-width: 1200px;
            width: 100%;
            margin: 1.5rem auto;
            gap: 1.5rem;
            padding: 0 1.5rem;
        }

        .sidebar {
            width: 280px;
            background: var(--bg-secondary);
            border-radius: 12px;
            box-shadow: var(--shadow);
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        body.dark-theme .sidebar {
            background: var(--dark-bg-secondary);
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        }

        .sidebar-section {
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 1.5rem;
        }

        body.dark-theme .sidebar-section {
            border-bottom: 1px solid var(--dark-border-color);
        }

        .sidebar-section:last-child {
            border-bottom: none;
            padding-bottom: 0;
        }

        .sidebar h3 {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-secondary);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        body.dark-theme .sidebar h3 {
            color: var(--dark-text-secondary);
        }

        .context-item {
            padding: 0.75rem;
            border-radius: 8px;
            background: var(--bg-tertiary);
            margin-bottom: 0.5rem;
            font-size: 0.85rem;
        }

        body.dark-theme .context-item {
            background: var(--dark-bg-tertiary);
        }

        .context-key {
            font-weight: 600;
            color: var(--primary);
        }

        .suggestions {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .suggestion-chip {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 999px;
            padding: 0.4rem 0.8rem;
            font-size: 0.8rem;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        body.dark-theme .suggestion-chip {
            background: var(--dark-bg-tertiary);
            border: 1px solid var(--dark-border-color);
        }

        .suggestion-chip:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .main-chat {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: var(--bg-secondary);
            border-radius: 12px;
            box-shadow: var(--shadow);
            overflow: hidden;
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        body.dark-theme .main-chat {
            background: var(--dark-bg-secondary);
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        }

        .chat-header {
            padding: 1rem 1.5rem;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        body.dark-theme .chat-header {
            border-bottom: 1px solid var(--dark-border-color);
        }

        .chat-title {
            font-weight: 600;
        }

        .status {
            font-size: 0.85rem;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        body.dark-theme .status {
            color: var(--dark-text-secondary);
        }

        .status-indicator {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background: var(--success);
        }

        .messages-container {
            flex: 1;
            padding: 1.5rem;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .message {
            max-width: 80%;
            padding: 1rem 1.25rem;
            border-radius: 18px;
            line-height: 1.5;
            position: relative;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .user-message {
            align-self: flex-end;
            background: var(--primary);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .bot-message {
            align-self: flex-start;
            background: var(--bg-tertiary);
            border-bottom-left-radius: 4px;
        }

        body.dark-theme .bot-message {
            background: var(--dark-bg-tertiary);
        }

        .message-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        .bot-message .message-header {
            color: var(--primary-dark);
        }

        body.dark-theme .bot-message .message-header {
            color: #6a9eff; /* Lighter blue for dark theme */
        }

        .message-content p {
            margin: 0.5rem 0;
        }

        .message-context {
            margin-top: 0.75rem;
            padding: 0.75rem;
            background: rgba(67, 97, 238, 0.05);
            border-radius: 8px;
            font-size: 0.85rem;
        }

        body.dark-theme .message-context {
            background: rgba(67, 97, 238, 0.15);
        }

        .message-context h4 {
            margin-bottom: 0.5rem;
            color: var(--primary);
        }

        .attachments {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.75rem;
        }

        .attachment {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem;
            background: rgba(0, 0, 0, 0.03);
            border-radius: 6px;
            font-size: 0.8rem;
        }

        body.dark-theme .attachment {
            background: rgba(255, 255, 255, 0.05);
        }

        .input-area {
            padding: 1.25rem;
            border-top: 1px solid var(--border-color);
            background: var(--bg-secondary);
            transition: background-color 0.3s, border-color 0.3s;
        }

        body.dark-theme .input-area {
            border-top: 1px solid var(--dark-border-color);
            background: var(--dark-bg-secondary);
        }

        .input-form {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .message-input-row {
            display: flex;
            gap: 0.75rem;
        }

        .message-input {
            flex: 1;
            padding: 0.9rem 1.25rem;
            border: 1px solid var(--border-color);
            border-radius: 999px;
            font-size: 1rem;
            transition: border-color 0.2s, background-color 0.3s, color 0.3s;
            background: var(--bg-secondary);
            color: var(--text-primary);
        }

        body.dark-theme .message-input {
            border: 1px solid var(--dark-border-color);
            background: var(--dark-bg-secondary);
            color: var(--dark-text-primary);
        }

        .message-input:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 3px rgba(67, 97, 238, 0.2);
        }

        .send-button {
            background: var(--primary);
            color: white;
            border: none;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s;
            align-self: flex-end;
        }

        .send-button:hover {
            background: var(--primary-dark);
        }

        .attachment-button {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            width: 50px;
            height: 50px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s, border-color 0.2s;
            align-self: flex-end;
        }

        body.dark-theme .attachment-button {
            background: var(--dark-bg-tertiary);
            border: 1px solid var(--dark-border-color);
        }

        .attachment-button:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .file-input {
            display: none;
        }

        .suggestions-bar {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .file-preview {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem;
            background: var(--bg-tertiary);
            border-radius: 6px;
            font-size: 0.85rem;
            max-width: 100%;
            overflow: hidden;
        }

        body.dark-theme .file-preview {
            background: var(--dark-bg-tertiary);
        }

        .file-preview-name {
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .remove-file {
            cursor: pointer;
            color: var(--warning);
        }

        .storage-warning {
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            color: #856404;
            padding: 0.75rem;
            border-radius: 0.5rem;
            font-size: 0.85rem;
            margin-bottom: 1rem;
            display: flex;
            align-items: flex-start;
            gap: 0.5rem;
        }

        body.dark-theme .storage-warning {
            background-color: #343a40;
            border-color: #495057;
            color: #e9ecef;
        }

        .workflow-steps {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px dashed var(--border-color);
        }

        body.dark-theme .workflow-steps {
            border-top: 1px dashed var(--dark-border-color);
        }

        .workflow-step {
            display: flex;
            align-items: center;
            gap: 0.4rem;
            background: var(--bg-tertiary);
            border-radius: 999px;
            padding: 0.3rem 0.7rem;
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        body.dark-theme .workflow-step {
            background: var(--dark-bg-tertiary);
        }

        .workflow-step:hover {
            background: var(--primary);
            color: white;
        }

        .step-number {
            background: var(--primary);
            color: white;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7rem;
        }

        .workflow-step:hover .step-number {
            background: white;
            color: var(--primary);
        }

        @media (max-width: 768px) {
            .chat-container {
                flex-direction: column;
                margin: 1rem;
                padding: 0;
            }
            
            .sidebar {
                width: 100%;
            }
            
            .message {
                max-width: 90%;
            }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="header-content">
            <div class="logo">
                <i class="fas fa-robot"></i>
                <span>Altiora</span>
            </div>
            <div class="theme-controls">
                <button class="theme-toggle" id="themeToggle" title="Basculer le mode sombre">
                    <i class="fas fa-moon"></i>
                </button>
                <div class="features">
                    <div class="feature-tag">M√©moire Contextuelle</div>
                    <div class="feature-tag">Suggestions Intelligentes</div>
                </div>
            </div>
        </div>
    </header>

    <div class="chat-container">
        <aside class="sidebar">
            <div class="storage-warning">
                <i class="fas fa-exclamation-triangle"></i>
                <div>
                    <strong>Limitation d'affichage :</strong> Le th√®me s√©lectionn√© (sombre/clair) ne sera pas sauvegard√© lors du rechargement de la page dans cet environnement.
                </div>
            </div>
            <div class="sidebar-section">
                <h3><i class="fas fa-brain"></i> Contexte Actif</h3>
                <div class="context-item">
                    <span class="context-key">Projet:</span> Migration syst√®me de paiement
                </div>
                <div class="context-item">
                    <span class="context-key">Technologie:</span> Node.js, PostgreSQL
                </div>
                <div class="context-item">
                    <span class="context-key">Derni√®re SFD:</span> Authentification utilisateur
                </div>
                <div class="context-item">
                    <span class="context-key">Tests G√©n√©r√©s:</span> 12 cas de test Playwright
                </div>
            </div>

            <div class="sidebar-section">
                <h3><i class="fas fa-history"></i> Historique R√©cent</h3>
                <div class="context-item">
                    <span class="context-key">Action:</span> Analyse SFD "Authentification"
                </div>
                <div class="context-item">
                    <span class="context-key">Action:</span> G√©n√©ration tests Playwright
                </div>
            </div>

            <div class="sidebar-section">
                <h3><i class="fas fa-lightbulb"></i> Suggestions</h3>
                <div class="suggestions">
                    <div class="suggestion-chip">Correction textuelle</div>
                    <div class="suggestion-chip">Analyse des SFD</div>
                    <div class="suggestion-chip">Cr√©er une matrice de test</div>
                    <div class="suggestion-chip">G√©n√©rer les tests Playwright</div>
                    <div class="suggestion-chip">Ex√©cuter les tests</div>
                </div>
            </div>
        </aside>

        <main class="main-chat">
            <div class="chat-header">
                <div class="chat-title">Assistant QA IA</div>
                <div class="status">
                    <div class="status-indicator"></div>
                    <span>En ligne - M√©moire contextuelle active</span>
                </div>
            </div>

            <div class="messages-container" id="messagesContainer">
                <div class="message bot-message">
                    <div class="message-header">
                        <i class="fas fa-robot"></i>
                        <span>Altiora</span>
                    </div>
                    <div class="message-content">
                        <p>Bonjour ! Je suis Altiora, votre assistant QA IA. Je me souviens de notre contexte : vous travaillez sur une migration de syst√®me de paiement avec Node.js.</p>
                        <p>Comment puis-je vous aider aujourd'hui ?</p>
                    </div>
                </div>

                <div class="message user-message">
                    <div class="message-header">
                        <i class="fas fa-user"></i>
                        <span>Vous</span>
                    </div>
                    <div class="message-content">
                        <p>J'ai besoin d'analyser la sp√©cification fonctionnelle d√©taill√©e pour le module d'authentification.</p>
                        <div class="attachments">
                            <div class="attachment">
                                <i class="fas fa-file-pdf"></i>
                                <span>SFD_authentification_v2.pdf</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="message bot-message">
                    <div class="message-header">
                        <i class="fas fa-robot"></i>
                        <span>Altiora</span>
                    </div>
                    <div class="message-content">
                        <p>Merci pour le fichier ! J'ai analys√© votre SFD "Authentification". Voici ce que j'ai identifi√© :</p>
                        <p><strong>5 sc√©narios de test potentiels :</strong></p>
                        <ol>
                            <li>Connexion utilisateur avec identifiants valides [Critique]</li>
                            <li>Tentative de connexion avec mot de passe incorrect [Haute]</li>
                            <li>R√©initialisation du mot de passe via email [Moyenne]</li>
                            <li>Verrouillage du compte apr√®s 3 tentatives [Moyenne]</li>
                            <li>D√©connexion s√©curis√©e [Normale]</li>
                        </ol>
                        <p>Souhaitez-vous que je g√©n√®re les tests Playwright correspondants ?</p>
                    </div>
                </div>
            </div>

            <div class="input-area">
                <form class="input-form" id="messageForm">
                    <div class="message-input-row">
                        <input type="text" class="message-input" id="messageInput" placeholder="Tapez votre message..." autocomplete="off">
                        <input type="file" id="fileInput" class="file-input" accept=".pdf,.doc,.docx,.txt,.xlsx">
                        <label for="fileInput" class="attachment-button" title="Joindre un fichier">
                            <i class="fas fa-paperclip"></i>
                        </label>
                        <button type="submit" class="send-button">
                            <i class="fas fa-paper-plane"></i>
                        </button>
                    </div>
                    <div id="filePreviewContainer"></div>
                    <!-- Suggestions rapides retir√©es ici -->
                </form>
            </div>
        </main>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const body = document.body;
            const themeToggle = document.getElementById('themeToggle');
            const themeIcon = themeToggle.querySelector('i');
            const messagesContainer = document.getElementById('messagesContainer');
            const messageForm = document.getElementById('messageForm');
            const messageInput = document.getElementById('messageInput');
            const fileInput = document.getElementById('fileInput');
            const filePreviewContainer = document.getElementById('filePreviewContainer');
            
            // Initialiser le th√®me clair par d√©faut
            // Note: Pas de persistance en localStorage √† cause du sandbox
            let isDarkTheme = false;
            
            // Basculer le th√®me
            themeToggle.addEventListener('click', function() {
                isDarkTheme = !isDarkTheme;
                body.classList.toggle('dark-theme', isDarkTheme);
                
                if (isDarkTheme) {
                    themeIcon.classList.remove('fa-moon');
                    themeIcon.classList.add('fa-sun');
                } else {
                    themeIcon.classList.remove('fa-sun');
                    themeIcon.classList.add('fa-moon');
                }
            });
            
            // Fonction pour ajouter un message
            function addMessage(sender, text, attachments = []) {
                const messageDiv = document.createElement('div');
                messageDiv.classList.add('message');
                messageDiv.classList.add(sender === 'user' ? 'user-message' : 'bot-message');
                
                const messageHeader = document.createElement('div');
                messageHeader.classList.add('message-header');
                messageHeader.innerHTML = `
                    <i class="fas ${sender === 'user' ? 'fa-user' : 'fa-robot'}"></i>
                    <span>${sender === 'user' ? 'Vous' : 'Altiora'}</span>
                `;
                
                const messageContent = document.createElement('div');
                messageContent.classList.add('message-content');
                messageContent.innerHTML = `<p>${text}</p>`;
                
                messageDiv.appendChild(messageHeader);
                messageDiv.appendChild(messageContent);
                
                // Ajouter les pi√®ces jointes si pr√©sentes
                if (attachments.length > 0) {
                    const attachmentsDiv = document.createElement('div');
                    attachmentsDiv.classList.add('attachments');
                    
                    attachments.forEach(file => {
                        const attachmentDiv = document.createElement('div');
                        attachmentDiv.classList.add('attachment');
                        attachmentDiv.innerHTML = `
                            <i class="fas ${getFileIcon(file.type)}"></i>
                            <span>${file.name}</span>
                        `;
                        attachmentsDiv.appendChild(attachmentDiv);
                    });
                    
                    messageContent.appendChild(attachmentsDiv);
                }
                
                messagesContainer.appendChild(messageDiv);
                messagesContainer.scrollTop = messagesContainer.scrollHeight;
            }
            
            // D√©terminer l'ic√¥ne de fichier
            function getFileIcon(fileType) {
                if (fileType.includes('pdf')) return 'fa-file-pdf';
                if (fileType.includes('word') || fileType.includes('doc')) return 'fa-file-word';
                if (fileType.includes('excel') || fileType.includes('sheet')) return 'fa-file-excel';
                if (fileType.includes('image')) return 'fa-file-image';
                if (fileType.includes('text') || fileType.includes('txt')) return 'fa-file-alt';
                return 'fa-file';
            }
            
            // Gestion de l'envoi du message
            messageForm.addEventListener('submit', function(e) {
                e.preventDefault();
                const message = messageInput.value.trim();
                const files = Array.from(fileInput.files);
                
                if (message || files.length > 0) {
                    addMessage('user', message || "Fichier joint", files);
                    messageInput.value = '';
                    fileInput.value = '';
                    filePreviewContainer.innerHTML = '';
                    
                    // Simulation de r√©ponse de l'assistant
                    setTimeout(() => {
                        let response = "";
                        if (message.toLowerCase().includes('g√©n√®re') && message.toLowerCase().includes('test')) {
                            response = "J'ai g√©n√©r√© 5 tests Playwright pour le module d'authentification. Vous les trouverez dans le dossier /tests/playwright/auth/. Souhaitez-vous que je les ex√©cute maintenant ?";
                        } else if (message.toLowerCase().includes('matrice')) {
                            response = "Voici la matrice de tra√ßabilit√© entre les exigences de la SFD et les cas de test que j'ai g√©n√©r√©s. Elle est disponible au format Excel dans /docs/matrice_authentification.xlsx";
                        } else if (message.toLowerCase().includes('ex√©cute')) {
                            response = "J'ex√©cute les tests Playwright pour le module d'authentification...<br><br>‚úÖ Test 1: Connexion r√©ussie - PASS<br>‚úÖ Test 2: Mot de passe incorrect - PASS<br>‚ö†Ô∏è Test 3: R√©initialisation mot de passe - FAIL (Timeout)<br>‚úÖ Test 4: Verrouillage compte - PASS<br>‚úÖ Test 5: D√©connexion - PASS<br><br>Un rapport d√©taill√© est disponible dans /reports/auth_test_report.html";
                        } else {
                            response = "J'ai bien not√© votre demande. En utilisant le contexte de notre projet de migration de syst√®me de paiement, je peux vous aider √† analyser des SFD, g√©n√©rer des tests, cr√©er des matrices ou ex√©cuter des suites de tests. Que souhaitez-vous faire maintenant ?";
                        }
                        
                        addMessage('bot', response);
                    }, 1000);
                }
            });
            
            // Gestion des suggestions
            document.querySelectorAll('.suggestion-chip').forEach(element => {
                element.addEventListener('click', function() {
                    const action = this.getAttribute('data-action') || this.textContent;
                    messageInput.value = action;
                    messageInput.focus();
                });
            });
            
            // Gestion de l'aper√ßu des fichiers
            fileInput.addEventListener('change', function() {
                filePreviewContainer.innerHTML = '';
                const files = Array.from(fileInput.files);
                
                if (files.length > 0) {
                    const previewDiv = document.createElement('div');
                    previewDiv.classList.add('file-preview');
                    
                    const fileIcon = document.createElement('i');
                    fileIcon.classList.add('fas', getFileIcon(files[0].type));
                    
                    const fileNameSpan = document.createElement('span');
                    fileNameSpan.classList.add('file-preview-name');
                    fileNameSpan.textContent = files[0].name;
                    
                    const removeIcon = document.createElement('i');
                    removeIcon.classList.add('fas', 'fa-times', 'remove-file');
                    removeIcon.addEventListener('click', function() {
                        fileInput.value = '';
                        filePreviewContainer.innerHTML = '';
                    });
                    
                    previewDiv.appendChild(fileIcon);
                    previewDiv.appendChild(fileNameSpan);
                    previewDiv.appendChild(removeIcon);
                    
                    filePreviewContainer.appendChild(previewDiv);
                }
            });
        });
    </script>
</body>
</html>
```

---

## Fichier : `anythingllm\prompts\qwen3_direct.txt`

```text
You are Altiora, a concise QA assistant. Provide direct answers without showing reasoning. Be accurate but brief. Focus on actionable insights.
```

---

## Fichier : `anythingllm\prompts\qwen3_thinking.txt`

```text
You are Altiora, an expert QA assistant. Use the <thinking> tags to show your reasoning process. Be precise, methodical and provide detailed analysis. Always validate edge cases and suggest improvements. Think step by step before giving the final answer.
```

---

## Fichier : `anythingllm\prompts\starcoder2_generation.txt`

```text
You are an expert test automation engineer. Generate clean, well-documented code following best practices. Include unit tests, error handling, and clear comments. Prefer Python with pytest framework. Respect PEP8 standards.
```

---

## Fichier : `backend\__init__.py`

```python

```

---

## Fichier : `backend\altiora\__init__.py`

```python
# backend/altiora/__init__.py
"""
Package principal Altiora.

Fournit l‚ÄôAPI, la logique m√©tier, les services et l‚Äôinfrastructure
n√©cessaires √† l‚Äôassistant QA intelligent.
"""

__all__ = ["api", "core", "services", "security", "config", "utils"]
```

---

## Fichier : `backend\altiora\__version__.py`

```python
# backend/altiora/__version__.py
"""
Gestion centralis√©e des versions pour Altiora V2.

Cette source unique de v√©rit√© est import√©e partout dans la base de code
pour assurer une gestion coh√©rente des versions √† travers le backend, la CLI et les images Docker.
"""

from __future__ import annotations

__title__ = "altiora"
__description__ = (
    "Altiora V2 - Intelligent QA Assistant with Qwen3-32B and Starcoder2-15B"
)
__version__ = "2.0.0"
__author__ = "Paul DE MOURA"
__email__ = "pauldemoura@ik.me"
__license__ = "MIT"
__copyright__ = f"2025 {__author__}"

# Alias de compatibilit√© API
VERSION = __version__
VERSION_INFO = tuple(map(int, __version__.split(".")))

# M√©tadonn√©es de pr√©-version / build
__build__ = None  # D√©fini par le pipeline CI/CD
__commit__ = None  # Hash du commit Git, d√©fini par le pipeline CI/CD

# API Publique
__all__ = [
    "__title__",
    "__description__",
    "__version__",
    "__author__",
    "__email__",
    "__license__",
    "__copyright__",
    "VERSION",
    "VERSION_INFO",
]
```

---

## Fichier : `backend\altiora\api\app.py`

```python
# backend/altiora/api/app.py
"""
Bootstrap de l'application FastAPI.

G√®re :
- Le routage de l'API v1
- Les middlewares globaux
- Les √©v√©nements de cycle de vie
- La documentation OpenAPI
"""

from __future__ import annotations

import logging
import time
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import ORJSONResponse

from backend.altiora.__version__ import __title__, __version__
from backend.altiora.api.v1 import analysis, batch, health, models
from backend.altiora.config.settings import settings
from backend.altiora.utils.logging import setup_logging
from backend.altiora.api.openapi import custom_openapi



logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(_: FastAPI) -> AsyncGenerator[None, None]:
    """Cycle de vie de d√©marrage / arr√™t."""
    setup_logging()
    logger.info("üöÄ %s v%s starting", __title__, __version__)
    yield
    logger.info("‚èπÔ∏è  %s stopped", __title__)


app = FastAPI(
    title=__title__,
    version=__version__,
    description="Altiora V2 ‚Äì QA assistant API",
    docs_url="/docs",
    redoc_url="/redoc",
    default_response_class=ORJSONResponse,
    lifespan=lifespan,
)

# Middleware global
app.add_middleware(GZipMiddleware, minimum_size=1024)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Routeurs
app.include_router(health.router, prefix="/health", tags=["health"])
app.include_router(analysis.router, prefix="/api/v1/analysis", tags=["analysis"])
app.include_router(batch.router, prefix="/api/v1/batch", tags=["batch"])
app.include_router(models.router, prefix="/api/v1/models", tags=["models"])
```

---

## Fichier : `backend\altiora\api\dependencies.py`

```python
# backend/altiora/api/dependencies.py
"""
D√©pendances FastAPI r√©utilisables.

Fournit : authentification, DB, cache, mod√®les.
"""
from typing import Annotated, Optional
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.orm import Session
from redis import Redis

from backend.altiora.config.settings import settings
from backend.altiora.core.models.model_swapper import ModelSwapper
from backend.altiora.infrastructure.cache.unified_cache import UnifiedCache
from backend.altiora.security.auth.jwt_handler import decode_access_token
from backend.altiora.infrastructure.database import get_db

security = HTTPBearer()

# Singleton instances
_model_swapper: Optional[ModelSwapper] = None
_cache: Optional[UnifiedCache] = None

def get_model_swapper() -> ModelSwapper:
    global _model_swapper
    if _model_swapper is None:
        _model_swapper = ModelSwapper()
    return _model_swapper

def get_cache() -> UnifiedCache:
    global _cache
    if _cache is None:
        _cache = UnifiedCache()
    return _cache

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> dict:
    try:
        payload = decode_access_token(credentials.credentials)
        return payload
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=str(e),
            headers={"WWW-Authenticate": "Bearer"},
        )

# Type aliases for dependency injection
CurrentUserDep = Annotated[dict, Depends(get_current_user)]
ModelSwapperDep = Annotated[ModelSwapper, Depends(get_model_swapper)]
CacheDep = Annotated[UnifiedCache, Depends(get_cache)]
DBSessionDep = Annotated[Session, Depends(get_db)]
RedisCacheDep = Annotated[Redis, Depends(lambda: Redis.from_url(settings.redis_url))]
```

---

## Fichier : `backend\altiora\api\openapi.py`

```python
# backend/altiora/api/openapi.py
"""Module pour la personnalisation de la sp√©cification OpenAPI (Swagger/Redoc) de l'API Altiora.

Ce module permet de d√©finir des m√©tadonn√©es suppl√©mentaires pour la documentation
de l'API, telles que le titre, la version, la description, et d'ajouter des
exemples de requ√™tes/r√©ponses pour am√©liorer la clart√© de la documentation
g√©n√©r√©e automatiquement par FastAPI.
"""

from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi


def custom_openapi(app: FastAPI):
    """G√©n√®re et personnalise la sp√©cification OpenAPI pour l'application FastAPI."

    Cette fonction est appel√©e par FastAPI pour construire la documentation
    interactive (Swagger UI, ReDoc). Elle ajoute des informations sp√©cifiques
    au projet et des exemples pour les endpoints.

    Args:
        app: L'instance de l'application FastAPI.

    Returns:
        Le sch√©ma OpenAPI personnalis√©.
    """
    # Si le sch√©ma a d√©j√† √©t√© g√©n√©r√©, le retourne directement pour √©viter de le recr√©er.
    if app.openapi_schema:
        return app.openapi_schema

    # G√©n√®re le sch√©ma OpenAPI de base √† partir des routes de l'application.
    openapi_schema = get_openapi(
        title="Altiora API",
        version="1.0.0",
        description="API pour l'assistant QA Altiora, permettant l'automatisation des tests et l'analyse de sp√©cifications.",
        routes=app.routes,
    )

    # Ajoute des exemples personnalis√©s pour des endpoints sp√©cifiques.
    # Ceci am√©liore la lisibilit√© de la documentation Swagger/Redoc.
    if "/analyze-sfd" in openapi_schema["paths"] and "post" in openapi_schema["paths"]["/analyze-sfd"]:
        openapi_schema["paths"]["/analyze-sfd"]["post"]["requestBody"]["content"]["application/json"]["example"] = {
            "content": "Sp√©cification fonctionnelle d√©taill√©e du module de connexion utilisateur...",
            "project_id": "proj-123"
        }

    # Stocke le sch√©ma g√©n√©r√© dans l'application pour les appels futurs.
    app.openapi_schema = openapi_schema
    return app.openapi_schema

```

---

## Fichier : `backend\altiora\api\__init__.py`

```python
# backend/altiora/api/__init__.py
"""
Couche API ‚Äì Routeurs FastAPI, middlewares, d√©pendances.
"""
```

---

## Fichier : `backend\altiora\api\gateway\api_gateway.py`

```python
# backend/altiora/api/gateway/api_gateway.py
"""Passerelle API principale pour l'application Altiora.

Ce module impl√©mente une passerelle API bas√©e sur FastAPI qui expose
des points de terminaison pour interagir avec les fonctionnalit√©s
de l'assistant QA. Il int√®gre des mesures de s√©curit√© comme la limitation
de d√©bit (rate limiting) et l'ajout d'en-t√™tes de s√©curit√© HTTP.
"""

import time
import logging

from fastapi import FastAPI, Request, HTTPException, status
from pydantic import BaseModel, Field
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

# Importation du syst√®me de QA (Question Answering).
from src.qa_system.qa_system import QASystem

logger = logging.getLogger(__name__)

# Initialisation du limiteur de d√©bit.
limiter = Limiter(key_func=get_remote_address)

# Initialisation de l'application FastAPI.
app = FastAPI(
    title="Altiora API Gateway",
    description="Passerelle API pour l'assistant QA Altiora.",
    version="1.0.0",
)
app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)


# --- Mod√®les Pydantic pour les requ√™tes et r√©ponses --- #
class QARequest(BaseModel):
    """Mod√®le de requ√™te pour le syst√®me de Question-R√©ponse (QA)."""
    question: str = Field(..., description="La question pos√©e par l'utilisateur.")
    context: Optional[str] = Field(None, description="Contexte optionnel pour aider √† r√©pondre √† la question.")
    model: str = Field("qwen", description="Le mod√®le d'IA √† utiliser pour la r√©ponse (ex: 'qwen', 'starcoder').")
    temperature: float = Field(0.7, ge=0.0, le=1.0, description="Temp√©rature pour la g√©n√©ration de la r√©ponse (contr√¥le la cr√©ativit√©).")


class QAResponse(BaseModel):
    """Mod√®le de r√©ponse du syst√®me de Question-R√©ponse (QA)."""
    answer: str = Field(..., description="La r√©ponse g√©n√©r√©e par le mod√®le.")
    confidence: float = Field(..., description="Le niveau de confiance de la r√©ponse (entre 0 et 1).")
    model_used: str = Field(..., description="Le nom du mod√®le d'IA utilis√© pour g√©n√©rer la r√©ponse.")
    processing_time: float = Field(..., description="Le temps de traitement de la requ√™te en secondes.")


# Initialisation du syst√®me de QA.
qa_system = QASystem()


# --- Middlewares --- #
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    """Middleware pour ajouter des en-t√™tes de s√©curit√© HTTP aux r√©ponses."

    Ces en-t√™tes aident √† prot√©ger l'application contre certaines vuln√©rabilit√©s
    web courantes comme le Cross-Site Scripting (XSS) et le Clickjacking.
    """
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    return response


# --- Points de terminaison (Endpoints) --- #
@app.post("/api/v1/qa/answer", response_model=QAResponse)
@limiter.limit("10/minute") # Limite √† 10 requ√™tes par minute par adresse IP.
async def answer_question(request: QARequest) -> QAResponse:
    """Point de terminaison principal pour poser une question au syst√®me QA."

    Args:
        request: L'objet `QARequest` contenant la question et les param√®tres.

    Returns:
        Un objet `QAResponse` avec la r√©ponse du mod√®le.

    Raises:
        HTTPException: En cas d'erreur interne du serveur.
    """
    start_time = time.time()

    try:
        # Appelle le syst√®me de QA pour obtenir une r√©ponse.
        answer = await qa_system.answer_async(
            question=request.question,
            context=request.context,
            model=request.model,
            temperature=request.temperature
        )

        return QAResponse(
            answer=answer.text,
            confidence=answer.confidence,
            model_used=request.model,
            processing_time=time.time() - start_time
        )
    except Exception as e:
        logger.error(f"Erreur lors de la r√©ponse √† la question : {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))


@app.post("/api/v1/analyze")
@limiter.limit("5/minute") # Limite √† 5 requ√™tes par minute pour l'analyse SFD.
async def analyze_sfd(request: Request, sfd_content: str):
    """Point de terminaison pour analyser une Sp√©cification Fonctionnelle D√©taill√©e (SFD)."

    Args:
        request: L'objet `Request` de FastAPI.
        sfd_content: Le contenu de la SFD √† analyser.

    Returns:
        Un dictionnaire avec le r√©sultat de l'analyse (actuellement un placeholder).

    Raises:
        HTTPException: En cas d'erreur interne du serveur.
    """
    logger.info(f"Requ√™te d'analyse SFD re√ßue. Contenu : {sfd_content[:100]}...")
    # TODO: Impl√©menter la logique r√©elle d'analyse SFD ici.
    # Cela impliquerait d'appeler l'orchestrateur ou un service d√©di√©.
    return {"message": "Analyse SFD en cours de d√©veloppement.", "received_content_length": len(sfd_content)}


# ------------------------------------------------------------------
# Point d'entr√©e Uvicorn
# ------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger.info("Lancement de la passerelle API sur http://0.0.0.0:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
```

---

## Fichier : `backend\altiora\api\middleware\advanced_rate_limiter.py`

```python
# backend/altiora/api/middleware/advanced_rate_limiter.py
"""Module impl√©mentant un limiteur de d√©bit (rate limiter) avanc√©.

Ce limiteur de d√©bit permet de contr√¥ler le nombre de requ√™tes qu'un utilisateur
ou une entit√© peut effectuer dans une p√©riode donn√©e. Il supporte diff√©rentes
cat√©gories de limites (ex: par d√©faut, analyse, g√©n√©ration) et est con√ßu pour
√™tre utilis√© de mani√®re asynchrone.
"""

from collections import defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)


class AdvancedRateLimiter:
    """Impl√©mente un limiteur de d√©bit configurable avec diff√©rentes cat√©gories de limites."""

    def __init__(self):
        """Initialise le limiteur de d√©bit avec des limites pr√©d√©finies."

        `limits` est un dictionnaire o√π chaque cl√© est une cat√©gorie (ex: "default", "analysis").
        Chaque cat√©gorie contient le nombre maximal de requ√™tes et la fen√™tre de temps en secondes.
        `requests` stocke les horodatages des requ√™tes pour chaque cl√© (utilisateur/IP).
        """
        self.limits: Dict[str, Dict[str, int]] = {
            "default": {"requests": 100, "window": 3600}, # 100 requ√™tes par heure.
            "analysis": {"requests": 20, "window": 3600}, # 20 requ√™tes d'analyse par heure.
            "generation": {"requests": 50, "window": 3600}, # 50 requ√™tes de g√©n√©ration par heure.
        }
        self.requests: Dict[str, List[datetime]] = defaultdict(list)

    async def check_limit(self, key: str, category: str = "default") -> bool:
        """V√©rifie si une requ√™te est autoris√©e selon les limites de d√©bit."

        Args:
            key: La cl√© unique pour laquelle la limite est v√©rifi√©e (ex: adresse IP, ID utilisateur).
            category: La cat√©gorie de limite √† appliquer (ex: "default", "analysis").

        Returns:
            True si la requ√™te est autoris√©e, False si la limite est d√©pass√©e.
        """
        now = datetime.now()
        limit_config = self.limits.get(category, self.limits["default"])

        # Nettoie les anciennes requ√™tes qui sont en dehors de la fen√™tre de temps.
        cutoff = now - timedelta(seconds=limit_config["window"])
        self.requests[f"{category}:{key}"] = [
            req for req in self.requests[f"{category}:{key}"]
            if req > cutoff
        ]

        # V√©rifie si le nombre de requ√™tes actuelles d√©passe la limite.
        if len(self.requests[f"{category}:{key}"]) >= limit_config["requests"]:
            logger.warning(f"Limite de d√©bit d√©pass√©e pour la cl√© '{key}' dans la cat√©gorie '{category}'.")
            return False

        # Enregistre la nouvelle requ√™te.
        self.requests[f"{category}:{key}"].append(now)
        return True


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        limiter = AdvancedRateLimiter()

        print("\n--- D√©monstration de la limite par d√©faut (100/heure) ---")
        user_ip = "192.168.1.100"
        for i in range(5):
            allowed = await limiter.check_limit(user_ip)
            print(f"Requ√™te {i+1} par {user_ip} : {'Autoris√©e' if allowed else 'Bloqu√©e'}")
            await asyncio.sleep(0.01) # Petite pause.

        print("\n--- D√©monstration de la limite 'analysis' (20/heure) ---")
        user_id = "user_alice"
        for i in range(25):
            allowed = await limiter.check_limit(user_id, category="analysis")
            print(f"Requ√™te d'analyse {i+1} par {user_id} : {'Autoris√©e' if allowed else 'Bloqu√©e'}")
            if not allowed:
                break
            await asyncio.sleep(0.01)

        print("\n--- D√©monstration de la r√©initialisation apr√®s fen√™tre de temps ---")
        # Simule le passage du temps pour r√©initialiser la limite.
        limiter.limits["short_test"] = {"requests": 2, "window": 1}
        test_key = "short_lived_user"

        print("Requ√™te 1 (short_test) :", "Autoris√©e" if await limiter.check_limit(test_key, "short_test") else "Bloqu√©e")
        print("Requ√™te 2 (short_test) :", "Autoris√©e" if await limiter.check_limit(test_key, "short_test") else "Bloqu√©e")
        print("Requ√™te 3 (short_test) :", "Autoris√©e" if await limiter.check_limit(test_key, "short_test") else "Bloqu√©e")

        print("Attente de 1.1 seconde pour la r√©initialisation...")
        await asyncio.sleep(1.1)

        print("Requ√™te 4 (short_test) apr√®s r√©initialisation :", "Autoris√©e" if await limiter.check_limit(test_key, "short_test") else "Bloqu√©e")

        print("D√©monstration du limiteur de d√©bit termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\api\middleware\cache_middleware.py`

```python
# backend/altiora/api/middleware/cache_middleware.py
"""Middleware de cache pour les requ√™tes HTTP.

Ce middleware intercepte les requ√™tes HTTP et tente de servir les r√©ponses
depuis un cache Redis. Si la r√©ponse n'est pas en cache, la requ√™te est
transmise √† l'application, et la r√©ponse est ensuite stock√©e dans Redis
pour les requ√™tes futures. Il utilise la compression pour optimiser
l'espace de stockage dans Redis.
"""

import time
import logging

from fastapi import Request, Response
from src.infrastructure.redis_config import get_redis_client
from src.utils.compression import compress_data, decompress_data

logger = logging.getLogger(__name__)


async def cache_middleware(request: Request, call_next):
    """Middleware de cache pour les requ√™tes FastAPI."

    Args:
        request: L'objet `Request` de FastAPI.
        call_next: La fonction pour passer la requ√™te au prochain middleware ou √† l'endpoint.

    Returns:
        L'objet `Response` de FastAPI, potentiellement servi depuis le cache.
    """
    start_time = time.time()
    redis_client = await get_redis_client()
    
    # G√©n√®re une cl√© de cache unique bas√©e sur la m√©thode et l'URL de la requ√™te.
    cache_key = f"cache:{request.method}:{request.url.path}"

    # Tente de r√©cup√©rer la r√©ponse depuis le cache Redis.
    cached_response = await redis_client.get(cache_key)
    
    if cached_response:
        try:
            # Si la r√©ponse est en cache, la d√©compresse et la d√©code.
            response_data = decompress_data(cached_response)
            response = Response(content=response_data, media_type="application/json")
            response.headers["X-Cache"] = "HIT"
            logger.info(f"Cache HIT pour {request.url.path}")
        except Exception as e:
            logger.error(f"Erreur lors de la lecture/d√©compression du cache pour {request.url.path}: {e}")
            # En cas d'erreur de cache, on passe √† l'application.
            response = await call_next(request)
            response.headers["X-Cache"] = "MISS_ERROR"
    else:
        # Si la r√©ponse n'est pas en cache, passe la requ√™te √† l'application.
        response = await call_next(request)
        response.headers["X-Cache"] = "MISS"
        logger.info(f"Cache MISS pour {request.url.path}")

        # Si la r√©ponse est un succ√®s (200 OK), la stocke dans le cache.
        if response.status_code == 200:
            # Lit le corps de la r√©ponse pour le mettre en cache.
            # Note: response.body est un bytes, il faut le d√©coder pour le compresser en string.
            response_body = response.body.decode('utf-8')
            compressed_data = compress_data(response_body)
            # Stocke dans Redis avec une expiration de 5 minutes (300 secondes).
            await redis_client.setex(cache_key, 300, compressed_data)
            logger.info(f"R√©ponse pour {request.url.path} mise en cache.")

    # Ajoute un en-t√™te pour le temps de r√©ponse.
    response.headers["X-Response-Time"] = str(time.time() - start_time)
    return response


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    from fastapi import FastAPI
    from fastapi.testclient import TestClient
    import uvicorn

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    app = FastAPI()

    # Applique le middleware de cache.
    app.middleware("http")(cache_middleware)

    @app.get("/items/{item_id}")
    async def read_item(item_id: int):
        """Endpoint de d√©monstration qui simule un travail long."""
        logger.info(f"Traitement de la requ√™te pour item_id: {item_id} (non mis en cache)...")
        await asyncio.sleep(1) # Simule un travail long.
        return {"item_id": item_id, "data": "Donn√©es g√©n√©r√©es", "timestamp": datetime.datetime.now().isoformat()}

    async def run_demo_client():
        print("\n--- Lancement du client de d√©monstration ---")
        client = TestClient(app)

        print("Premier appel √† /items/1 (devrait √™tre MISS)...")
        response1 = client.get("/items/1")
        print(f"Statut: {response1.status_code}, Cache: {response1.headers.get('X-Cache')}, Temps: {response1.headers.get('X-Response-Time')[:5]}s")

        print("\nDeuxi√®me appel √† /items/1 (devrait √™tre HIT)...")
        response2 = client.get("/items/1")
        print(f"Statut: {response2.status_code}, Cache: {response2.headers.get('X-Cache')}, Temps: {response2.headers.get('X-Response-Time')[:5]}s")

        print("\nTroisi√®me appel √† /items/2 (nouvelle cl√©, devrait √™tre MISS)...")
        response3 = client.get("/items/2")
        print(f"Statut: {response3.status_code}, Cache: {response3.headers.get('X-Cache')}, Temps: {response3.headers.get('X-Response-Time')[:5]}s")

        print("\nAttente de 6 secondes pour l'expiration du cache...")
        await asyncio.sleep(6)

        print("\nQuatri√®me appel √† /items/1 (apr√®s expiration, devrait √™tre MISS)...")
        response4 = client.get("/items/1")
        print(f"Statut: {response4.status_code}, Cache: {response4.headers.get('X-Cache')}, Temps: {response4.headers.get('X-Response-Time')[:5]}s")

        print("D√©monstration du cache middleware termin√©e.")

    # Lance le serveur Uvicorn en arri√®re-plan pour la d√©mo.
    # Assurez-vous qu'un serveur Redis est en cours d'ex√©cution sur localhost:6379.
    async def run_server_and_client():
        config = uvicorn.Config(app, host="0.0.0.0", port=8000, log_level="warning")
        server = uvicorn.Server(config)
        server_task = asyncio.create_task(server.serve())
        await asyncio.sleep(1) # Donne le temps au serveur de d√©marrer.
        await run_demo_client()
        server_task.cancel()

    asyncio.run(run_server_and_client())
```

---

## Fichier : `backend\altiora\api\middleware\rbac_middleware.py`

```python
# backend/altiora/api/middleware/rbac_middleware.py
"""Middleware pour le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC).

Ce middleware s'int√®gre aux applications FastAPI pour v√©rifier les permissions
des utilisateurs avant d'autoriser l'acc√®s √† certaines ressources ou actions.
Il utilise un gestionnaire RBAC centralis√© pour d√©terminer si un utilisateur
poss√®de les droits n√©cessaires.
"""

from __future__ import annotations

import logging
from pathlib import Path

from fastapi import HTTPException, status

from src.rbac.manager import RBACManager
from src.rbac.models import User # Assurez-vous que le mod√®le User est correctement import√©.

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# Instance globale du gestionnaire RBAC
# ------------------------------------------------------------------
# Le chemin vers le fichier de configuration des r√¥les (ex: configs/roles.yaml).
# Assurez-vous que ce fichier existe et est correctement configur√©.
rbac_manager = RBACManager(Path("configs/roles.yaml"))


async def verify_permission(
    user: User,
    resource: str,
    action: str,
) -> None:
    """V√©rifie si un utilisateur a la permission d'effectuer une action sur une ressource."

    Args:
        user: L'objet `User` repr√©sentant l'utilisateur authentifi√©.
        resource: La ressource √† laquelle l'acc√®s est demand√© (ex: "sfd:analysis", "user:management").
        action: L'action que l'utilisateur tente d'effectuer (ex: "read", "write", "delete").

    Raises:
        HTTPException: Si l'utilisateur n'a pas la permission requise (statut 403 Forbidden).
    """
    logger.debug(f"V√©rification de permission pour l'utilisateur '{user.username}' sur ressource '{resource}' avec action '{action}'.")
    if not rbac_manager.has_permission(user, resource, action):
        logger.warning(f"Acc√®s refus√© pour l'utilisateur '{user.username}' : permission '{action}' sur '{resource}' manquante.")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=f"L'utilisateur '{user.username}' n'a pas la permission d'effectuer l'action '{action}' sur la ressource '{resource}'."
        )
    logger.debug(f"Acc√®s autoris√© pour l'utilisateur '{user.username}'.")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging
    from fastapi import FastAPI, Depends
    from fastapi.security import OAuth2PasswordBearer
    from src.auth.jwt_handler import jwt_handler
    from src.auth.models import User, UserRole, TokenData # Assurez-vous d'importer UserRole

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Cr√©e un fichier roles.yaml factice pour la d√©monstration.
    temp_roles_file = Path("configs/roles.yaml")
    temp_roles_file.write_text("""
roles:
  - name: admin
    permissions:
      - "user:read"
      - "user:write"
      - "admin:*"
  - name: user
    permissions:
      - "sfd:read"
      - "sfd:write"
  - name: viewer
    permissions:
      - "sfd:read"
""")

    # Re-initialise le rbac_manager avec le fichier factice.
    rbac_manager = RBACManager(temp_roles_file)

    # Simule un utilisateur authentifi√© (en temps normal, viendrait d'un token JWT).
    async def get_current_user_mock(username: str, roles: List[UserRole]) -> User:
        # Cr√©e un objet User factice pour la d√©monstration.
        return User(id=1, username=username, email=f"{username}@example.com", hashed_password="hashed", role=roles[0].value) # Utilise .value pour l'Enum

    # D√©pendance pour simuler l'utilisateur courant.
    oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

    async def get_user_admin() -> User:
        return await get_current_user_mock("admin_user", [UserRole.ADMIN])

    async def get_user_normal() -> User:
        return await get_current_user_mock("normal_user", [UserRole.USER])

    async def get_user_viewer() -> User:
        return await get_current_user_mock("viewer_user", [UserRole.VIEWER])

    app = FastAPI()

    @app.get("/admin_only")
    async def admin_only_endpoint(current_user: User = Depends(get_user_admin)):
        await verify_permission(current_user, "admin", "access")
        return {"message": f"Bienvenue, {current_user.username}! Vous avez acc√®s √† l'administration."

    @app.get("/sfd_read")
    async def sfd_read_endpoint(current_user: User = Depends(get_user_normal)):
        await verify_permission(current_user, "sfd", "read")
        return {"message": f"Bienvenue, {current_user.username}! Vous pouvez lire les SFD."

    @app.get("/sfd_write")
    async def sfd_write_endpoint(current_user: User = Depends(get_user_normal)):
        await verify_permission(current_user, "sfd", "write")
        return {"message": f"Bienvenue, {current_user.username}! Vous pouvez √©crire des SFD."

    async def run_demo_client():
        print("\n--- D√©monstration du RBAC Middleware ---")
        from fastapi.testclient import TestClient
        client = TestClient(app)

        print("\nTest 1: Admin acc√®de √† /admin_only (attendu: succ√®s)")
        response = client.get("/admin_only", headers={"Authorization": "Bearer fake_token_admin"})
        print(f"Statut: {response.status_code}, Message: {response.json()}")

        print("\nTest 2: Utilisateur normal acc√®de √† /admin_only (attendu: √©chec 403)")
        response = client.get("/admin_only", headers={"Authorization": "Bearer fake_token_user"})
        print(f"Statut: {response.status_code}, Message: {response.json()}")

        print("\nTest 3: Utilisateur normal acc√®de √† /sfd_read (attendu: succ√®s)")
        response = client.get("/sfd_read", headers={"Authorization": "Bearer fake_token_user"})
        print(f"Statut: {response.status_code}, Message: {response.json()}")

        print("\nTest 4: Viewer acc√®de √† /sfd_write (attendu: √©chec 403)")
        response = client.get("/sfd_write", headers={"Authorization": "Bearer fake_token_viewer"})
        print(f"Statut: {response.status_code}, Message: {response.json()}")

        print("D√©monstration du RBAC Middleware termin√©e.")

    # Lance le serveur Uvicorn en arri√®re-plan pour la d√©mo.
    async def run_server_and_client():
        config = uvicorn.Config(app, host="0.0.0.0", port=8000, log_level="warning")
        server = uvicorn.Server(config)
        server_task = asyncio.create_task(server.serve())
        await asyncio.sleep(1) # Donne le temps au serveur de d√©marrer.
        await run_demo_client()
        server_task.cancel()

    import uvicorn
    asyncio.run(run_server_and_client())

    # Nettoyage du fichier factice.
    temp_roles_file.unlink(missing_ok=True)

```

---

## Fichier : `backend\altiora\api\middleware\__init__.py`

```python
# backend/altiora/api/middleware/__init__.py
"""Initialise le package des middlewares de l'application Altiora.

Ce package contient les middlewares FastAPI qui interceptent les requ√™tes
et les r√©ponses HTTP pour appliquer des logiques transversales telles que
la limitation de d√©bit, la mise en cache et le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC).

Les modules suivants sont expos√©s pour faciliter les importations :
- `AdvancedRateLimiter`: Pour la gestion avanc√©e de la limitation de d√©bit.
- `cache_middleware`: Middleware pour la mise en cache des r√©ponses HTTP.
- `rbac_middleware`: Middleware pour la v√©rification des permissions RBAC.
"""
from .advanced_rate_limiter import AdvancedRateLimiter
from .cache_middleware import cache_middleware
from .rbac_middleware import rbac_middleware

__all__ = ['AdvancedRateLimiter', 'cache_middleware', 'rbac_middleware']

```

---

## Fichier : `backend\altiora\api\v1\analysis.py`

```python
# backend/altiora/api/v1/analysis.py
"""
Endpoints d‚Äôanalyse QA.
"""
# backend/altiora/api/v1/analysis.py
from __future__ import annotations

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List

from altiora.api.dependencies import CurrentUserDep, ModelSwapperDep
from altiora.core.orchestrator import AltioraOrchestrator

router = APIRouter()

class SpecificationInput(BaseModel):
    content: str
    project_id: str | None = None

class TestScenario(BaseModel):
    id: str
    title: str
    description: str
    type: str

class AnalysisResult(BaseModel):
    scenarios: List[TestScenario]
    summary: str

@router.post("/analyze", response_model=AnalysisResult)
async def analyze_specification(
    spec: SpecificationInput,
    user: CurrentUserDep,
    swapper: ModelSwapperDep,
) -> AnalysisResult:
    """Analyse compl√®te avec orchestrateur."""
    orchestrator = AltioraOrchestrator(swapper)
    return await orchestrator.process_specification(spec.content)
```

---

## Fichier : `backend\altiora\api\v1\batch.py`

```python
# backend/altiora/api/v1/batch.py
from fastapi import APIRouter, Depends, HTTPException
from backend.altiora.api.v1.schemas import BatchJobInput, BatchJobResponse
from backend.altiora.api.dependencies import CurrentUserDep
from backend.altiora.core.batch.scheduler import BatchScheduler
from pathlib import Path
from datetime import datetime, timedelta
import asyncio

router = APIRouter()


@router.post("/schedule", response_model=BatchJobResponse)
async def schedule_batch(
        job: BatchJobInput,
        user: CurrentUserDep,
) -> BatchJobResponse:
    """
    Planifie un traitement batch de sp√©cifications avec limites :
    - Max 100 fichiers
    - Timeout global de 2h
    """
    try:
        # Validation des chemins
        input_path = Path(job.input_dir)
        output_path = Path(job.output_dir)

        if not input_path.exists():
            raise HTTPException(400, f"Input dir not found: {job.input_dir}")

        output_path.mkdir(parents=True, exist_ok=True)

        # Validation du pattern de fichier
        if not job.file_pattern:
            raise HTTPException(400, "file_pattern cannot be empty")

        # Comptage et limitation
        files = list(input_path.glob(job.file_pattern))
        if len(files) > 100:
            raise HTTPException(
                400,
                f"Trop de fichiers : {len(files)} trouv√©s (max 100 autoris√©s)"
            )

        if not files:
            raise HTTPException(400, "Aucun fichier trouv√© avec le pattern sp√©cifi√©")

        # Calcul de l'ETA (estimation bas√©e sur 30s par fichier)
        estimated_duration = min(len(files) * 30, 7200)  # Max 2h (7200s)
        eta = datetime.utcnow() + timedelta(seconds=estimated_duration)

        # Cr√©ation du job
        scheduler = BatchScheduler()
        job_id = await scheduler.schedule(
            {
                "input_dir": str(input_path),
                "output_dir": str(output_path),
                "file_pattern": job.file_pattern,
                "resume": job.resume,
                "max_files": 100,
                "timeout": 7200  # 2h en secondes
            },
            delay=job.delay
        )

        return BatchJobResponse(
            job_id=job_id,
            estimated_files=len(files),
            eta=eta.isoformat() + "Z"
        )

    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
```

---

## Fichier : `backend\altiora\api\v1\health.py`

```python
# backend/altiora/api/v1/health.py
"""
Endpoints de sant√©.
"""

from __future__ import annotations

from fastapi import APIRouter, Depends

from altiora.api.dependencies import CacheDep, DBSessionDep

router = APIRouter()


@router.get("/")
async def health_check(cache: CacheDep, db: DBSessionDep) -> dict[str, str]:
    """Retourne l‚Äô√©tat g√©n√©ral des services."""
    await cache.redis.ping()
    await db.execute("SELECT 1")
    return {"status": "ok"}
```

---

## Fichier : `backend\altiora\api\v1\models.py`

```python
# backend/altiora/api/v1/models.py
"""
Endpoints de gestion des mod√®les IA.
"""

from __future__ import annotations

from typing import Any

from fastapi import APIRouter, Depends

from altiora.api.dependencies import ModelSwapperDep

router = APIRouter()


@router.post("/swap")
async def swap_model(
    model: str,
    swapper: ModelSwapperDep,
) -> dict[str, str]:
    """Force le swap vers un mod√®le donn√© (qwen3 ou starcoder2)."""
    await swapper.ensure_model_loaded(model)
    return {"active_model": model}
```

---

## Fichier : `backend\altiora\api\v1\schemas.py`

```python
# backend/altiora/api/v1/schemas.py
from pydantic import BaseModel
from typing import List, Optional, Any

class SpecificationInput(BaseModel):
    content: str
    project_id: Optional[str] = None
    language: Optional[str] = "fr"

class TestScenario(BaseModel):
    id: str
    title: str
    description: str
    type: str  # CP, CE, CL
    priority: int = 1

class AnalysisResult(BaseModel):
    scenarios: List[TestScenario]
    summary: str
    estimated_tests: int
    model_used: str

class BatchJobInput(BaseModel):
    input_dir: str
    output_dir: str
    file_pattern: str = "*.pdf"
    delay: int = 0  # secondes
    resume: bool = False

class BatchJobResponse(BaseModel):
    job_id: str
    status: str = "queued"
    estimated_files: int
```

---

## Fichier : `backend\altiora\api\v1\__init__.py`

```python
# backend/altiora/api/v1/__init__.py
"""
Routers API v1.
"""
```

---

## Fichier : `backend\altiora\config\settings.py`

```python
# backend/altiora/config/settings.py
"""
Param√®tres Pydantic (env, YAML, validation).
"""

from typing import List, Optional
from pathlib import Path
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field, validator


class QwenConfig(BaseSettings):
    model_path: Path = Field(default=Path("models/qwen3-32b-q4_K_M.gguf"))
    context_length: int = 32768
    threads: int = 8
    thinking_temp: float = 0.6
    non_thinking_temp: float = 0.7
    thinking_top_p: float = 0.95
    thinking_top_k: int = 20
    use_mmap: bool = True
    use_mlock: bool = False

    @validator("model_path")
    def validate_model_path(cls, v):
        if not v.exists():
            raise ValueError(f"Model file not found: {v}")
        return v


class StarcoderConfig(BaseSettings):
    model_path: Path = Field(default=Path("models/starcoder2-15b-q8_0.gguf"))
    context_length: int = 16384
    threads: int = 8
    temperature: float = 0.2
    max_tokens: int = 1200
    use_mmap: bool = True
    use_mlock: bool = False


class RedisConfig(BaseSettings):
    url: str = "redis://localhost:6379/0"
    decode_responses: bool = False
    max_connections: int = 50
    socket_keepalive: bool = True
    socket_keepalive_options: dict = Field(default_factory=dict)


class Settings(BaseSettings):
    # Application
    app_name: str = "Altiora"
    app_version: str = "2.0.0"
    debug: bool = False
    log_level: str = "INFO"

    # API
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_prefix: str = "/api/v1"
    cors_origins: List[str] = ["http://localhost:3000", "http://localhost:3001"]

    # Security
    jwt_secret_key: str = Field(..., env="JWT_SECRET_KEY")
    jwt_algorithm: str = "HS256"
    jwt_expiration_hours: int = 24
    encryption_key: str = Field(..., env="ENCRYPTION_KEY")

    # Models
    qwen: QwenConfig = QwenConfig()
    starcoder: StarcoderConfig = StarcoderConfig()
    ollama_host: str = "http://localhost:11434"

    # Infrastructure
    redis: RedisConfig = RedisConfig()
    database_url: str = "sqlite:///./altiora.db"

    # Performance
    memory_limit_gb: int = 32
    memory_warning_threshold: float = 0.85
    swap_enabled: bool = True
    model_swap_enabled: bool = True

    # Cache
    cache_ttl_default: int = 3600
    cache_compression_enabled: bool = True
    cache_semantic_enabled: bool = True

    # Batch Processing
    batch_max_parallel: int = 5
    batch_retry_max: int = 3
    batch_timeout_seconds: int = 300

    model_config = SettingsConfigDict(
        env_file=".env",
        env_nested_delimiter="__",
        case_sensitive=False
    )


settings = Settings()
```

---

## Fichier : `backend\altiora\config\validators.py`

```python
# backend/altiora/config/validators.py
"""
Validateurs personnalis√©s pour la configuration.
"""

from pathlib import Path
from typing import Any, Optional
from pydantic import validator, root_validator
import os


def validate_model_path(path: str | Path) -> Path:
    """V√©rifie que le fichier GGUF existe."""
    path = Path(path)
    if not path.is_file():
        raise ValueError(f"Model file not found: {path}")
    if not path.suffix == ".gguf":
        raise ValueError(f"Model must be GGUF format, got: {path.suffix}")
    return path


def validate_memory_limit(v: int) -> int:
    """V√©rifie que la limite m√©moire est raisonnable."""
    if v < 16:
        raise ValueError("Memory limit must be at least 16GB")
    if v > 128:
        raise ValueError("Memory limit above 128GB is unrealistic")
    return v


def validate_redis_url(v: str) -> str:
    """Valide l'URL Redis."""
    if not v.startswith(("redis://", "rediss://")):
        raise ValueError("Redis URL must start with redis:// or rediss://")
    return v


def validate_ollama_host(v: str) -> str:
    """Valide l'URL Ollama."""
    if not v.startswith(("http://", "https://")):
        raise ValueError("Ollama host must start with http:// or https://")
    return v


def validate_jwt_secret(v: str) -> str:
    """V√©rifie que le secret JWT est suffisamment s√©curis√©."""
    if len(v) < 32:
        raise ValueError("JWT secret must be at least 32 characters")
    return v


def validate_port(v: int) -> int:
    """Valide un num√©ro de port."""
    if v < 1 or v > 65535:
        raise ValueError(f"Invalid port number: {v}")
    return v


def validate_percentage(v: float) -> float:
    """Valide un pourcentage (0-1)."""
    if v < 0 or v > 1:
        raise ValueError(f"Percentage must be between 0 and 1, got: {v}")
    return v


class PathValidator:
    """Validateur pour les chemins avec cr√©ation automatique."""

    @staticmethod
    def validate_directory(path: str | Path, create: bool = True) -> Path:
        path = Path(path)
        if create and not path.exists():
            path.mkdir(parents=True, exist_ok=True)
        elif not path.is_dir():
            raise ValueError(f"Not a directory: {path}")
        return path

    @staticmethod
    def validate_file(path: str | Path, must_exist: bool = True) -> Path:
        path = Path(path)
        if must_exist and not path.is_file():
            raise ValueError(f"File not found: {path}")
        return path
```

---

## Fichier : `backend\altiora\config\__init__.py`

```python
# backend/altiora/config/__init__.py
"""
Configuration centralis√©e avec Pydantic.
"""
```

---

## Fichier : `backend\altiora\core\orchestrator.py`

```python
# backend/altiora/core/orchestrator.py (version optimis√©e)
"""
Orchestrateur principal d'Altiora - VERSION OPTIMIS√âE 32GB RAM.

CHANGEMENT MAJEUR : Utilise ModelSwapper pour ne jamais avoir
les deux mod√®les en m√©moire simultan√©ment.

Workflow:
1. Qwen3 analyse avec /think
2. Si besoin de code ‚Üí swap vers Starcoder2
3. Starcoder2 g√©n√®re ‚Üí swap retour vers Qwen3
4. Qwen3 finalise la r√©ponse
"""

from typing import List, Dict

from backend.altiora.core.models.model_swapper import ModelSwapper
from backend.altiora.core.models.qwen3.model_manager import Qwen3Manager
from backend.altiora.core.models.starcoder2.model_manager import Starcoder2Manager
from backend.altiora.infrastructure.cache.compressed_client import CompressedRedisCache


class AltioraOrchestrator:
    """Orchestre le flux de traitement des requ√™tes en g√©rant le chargement des mod√®les."""

    def __init__(self):
        """Initialise avec le swapper de mod√®les."""
        self.model_swapper = ModelSwapper()
        self.cache = CompressedRedisCache(settings.redis_url)  # Cache compress√©
        # Plus besoin d'instances s√©par√©es !

    async def process_request(self, request: AnalysisRequest) -> AnalysisResponse:
        """
        Traite une requ√™te avec swap m√©moire intelligent.
        """
        # 1. Charger Qwen3 pour l'analyse
        logger.info("Chargement de Qwen3 pour analyse...")
        qwen3 = await self.model_swapper.ensure_model_loaded("qwen3")

        # 2. Analyse avec Qwen3
        qwen_response = await self._analyze_with_qwen(qwen3, request)

        # 3. Si besoin de code, swap vers Starcoder2
        if self._needs_code_generation(qwen_response.content):
            logger.info("Swap Qwen3 ‚Üí Starcoder2 pour g√©n√©ration de code...")

            # Sauver l'√©tat de Qwen3 si n√©cessaire
            qwen_state = {"response": qwen_response, "context": request.context}

            # Swap vers Starcoder2
            starcoder = await self.model_swapper.swap_to_model("starcoder2", qwen_state)

            # G√©n√©rer le code
            code_sections = await self._generate_code_with_starcoder(starcoder, qwen_response)

            # Swap retour vers Qwen3
            logger.info("Swap Starcoder2 ‚Üí Qwen3 pour finalisation...")
            qwen3 = await self.model_swapper.swap_to_model("qwen3")

            # Qwen3 int√®gre le code dans sa r√©ponse
            final_response = await self._finalize_with_qwen(qwen3, qwen_response, code_sections)
        else:
            final_response = qwen_response
            code_sections = []

        # 4. Cleanup - CRUCIAL !
        await self.model_swapper.cleanup()

        return final_response


class AnalysisResult:
    def __init__(self, scenarios: List[Dict], summary: str):
        self.scenarios = scenarios
        self.summary = summary


class AltioraOrchestrator:
    async def process_specification(self, spec_content: str) -> AnalysisResult:
        """Workflow complet : analyse ‚Üí d√©cision ‚Üí g√©n√©ration."""

        # √âtape 1 : Charger Qwen3
        qwen3 = await self.swapper.ensure_model_loaded("qwen3")
        qwen_manager = Qwen3Manager(model=qwen3)

        # √âtape 2 : Analyser la sp√©cification
        scenarios = await qwen_manager.analyze(spec_content)

        # √âtape 3 : V√©rifier si g√©n√©ration de code n√©cessaire
        if any("pytest" in s.get('type', '').lower() for s in scenarios):
            starcoder = await self.swapper.swap_to_model("starcoder2")
            starcoder_manager = Starcoder2Manager(model=starcoder)
            await self._generate_code_if_needed(scenarios)
            await self.swapper.swap_to_model("qwen3")

        return AnalysisResult(
            scenarios=scenarios,
            summary=f"Analys√© {len(scenarios)} sc√©narios"
        )


async def _analyze_with_qwen(self, model: Llama, request: AnalysisRequest) -> AnalysisResponse:
    """Utilise Qwen3 pour analyser une sp√©cification."""
    # Construire le prompt
    prompt = f"""<|im_start|>system
Tu es un expert QA. Analyse cette sp√©cification et extrais :
1. Les objectifs de test
2. Les sc√©narios identifi√©s
3. Les pr√©conditions
4. Les √©tapes d√©taill√©es
<|im_end|>
<|im_start|>user
{request.content}
<|im_end|>
<|im_start|>assistant
/think"""

    # Appeler le mod√®le
    response = model(
        prompt,
        max_tokens=2048,
        temperature=0.7,
        stop=["<|im_end|>"]
    )

    # Parser la r√©ponse
    content = response["choices"][0]["text"]

    # Extraire les sc√©narios avec regex
    scenarios = []
    scenario_pattern = r"Sc√©nario (\d+):\s*(.+?)(?=Sc√©nario \d+:|$)"
    matches = re.finditer(scenario_pattern, content, re.DOTALL)

    for match in matches:
        scenarios.append({
            "id": f"SC-{match.group(1).zfill(3)}",
            "title": match.group(2).strip().split('\n')[0],
            "description": match.group(2).strip()
        })

    return AnalysisResponse(
        content=content,
        scenarios=scenarios,
        metadata={"model": "qwen3", "thinking_mode": True}
    )


async def _needs_code_generation(self, content: str) -> bool:
    """D√©termine si la r√©ponse n√©cessite de g√©n√©rer du code."""
    code_indicators = [
        "g√©n√©rer test",
        "cr√©er script",
        "impl√©menter",
        "code playwright",
        "automatiser",
        "def test_",
        "sc√©nario de test automatis√©"
    ]

    content_lower = content.lower()
    return any(indicator in content_lower for indicator in code_indicators)


async def _generate_code_with_starcoder(
        self,
        model: Llama,
        analysis: AnalysisResponse
) -> List[CodeSection]:
    """G√©n√®re du code de test avec Starcoder2."""
    code_sections = []

    for scenario in analysis.scenarios:
        prompt = f"""<fim_prefix>
# Test: {scenario['title']}
# Description: {scenario['description']}

import pytest
from playwright.sync_api import Page, expect

def test_{scenario['id'].lower().replace('-', '_')}(page: Page):
    \"\"\"
<fim_suffix>

    # Assertions
    expect(page).to_have_title(re.compile(".*"))
<fim_middle>"""

        response = model(
            prompt,
            max_tokens=800,
            temperature=0.2,
            stop=["<|endoftext|>", "<fim_prefix>"]
        )

        code = response["choices"][0]["text"]

        code_sections.append(CodeSection(
            scenario_id=scenario['id'],
            code=f"def test_{scenario['id'].lower().replace('-', '_')}(page: Page):\n    \"\"\"\n{code}",
            language="python",
            framework="playwright"
        ))

    return code_sections


async def _finalize_with_qwen(
        self,
        model: Llama,
        analysis: AnalysisResponse,
        code_sections: List[CodeSection]
) -> AnalysisResponse:
    """Qwen3 int√®gre le code dans sa r√©ponse finale."""
    # Construire un r√©sum√© avec le code
    code_summary = "\n\n".join([
        f"### {cs.scenario_id}\n```python\n{cs.code}\n```"
        for cs in code_sections
    ])

    final_prompt = f"""<|im_start|>system
Int√®gre les tests g√©n√©r√©s dans un rapport final structur√©.
<|im_end|>
<|im_start|>user
Analyse initiale:
{analysis.content}

Tests g√©n√©r√©s:
{code_summary}
<|im_end|>
<|im_start|>assistant"""

    response = model(
        final_prompt,
        max_tokens=1024,
        temperature=0.5
    )

    analysis.content = response["choices"][0]["text"]
    analysis.code_sections = code_sections
    return analysis

```

---

## Fichier : `backend\altiora\core\__init__.py`

```python
# backend/altiora/core/__init__.py
"""
Contient la logique m√©tier principale et les modules fondamentaux d'Altiora.
"""

```

---

## Fichier : `backend\altiora\core\batch\batch_processor.py`

```python
"""
# backend/altiora/core/batch/processor.py

Asynchronous batch processor for *Sp√©cifications Fonctionnelles D√©taill√©es* (SFD).

Features:
- 100 % async/await
- Redis persistence with compression
- Prometheus metrics
- Configurable concurrency
- Automatic retry & resume
"""

from __future__ import annotations

import asyncio
import gc
import json
import logging
import uuid
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, List, Optional

import aiofiles
import zstandard as zstd
from prometheus_client import Counter, Gauge
from redis.asyncio import Redis

from altiora.core.models.model_swapper import ModelSwapper
from altiora.services.ocr.ocr_wrapper import OCRRequest, extract_text

# ------------------------------------------------------------------
# Prometheus metrics
# ------------------------------------------------------------------
BATCH_DOCS_TOTAL = Gauge("altiora_batch_docs_total", "Total documents in batch")
BATCH_SUCCESS_TOTAL = Counter("altiora_batch_success_total", "Successfully processed docs")
BATCH_CHUNK_TIME = Gauge("altiora_batch_chunk_seconds", "Processing time per chunk")

# ------------------------------------------------------------------
# Configuration
# ------------------------------------------------------------------
MAX_WORKERS = 20
LLM_CONCURRENCY = 6
CHUNK_SIZE = 16

# ------------------------------------------------------------------
# Data model
# ------------------------------------------------------------------
@dataclass
class Job:
    """Single SFD job state."""
    path: Path
    ocr_text: str = ""
    status: str = "pending"
    error: str = ""
    result: Optional[dict[str, Any]] = None


class BatchProcessor:
    """Async batch processor for SFD files."""

    def __init__(self, redis_url: str, swapper: ModelSwapper) -> None:
        self.redis = Redis.from_url(redis_url, decode_responses=False)
        self.swapper = swapper
        self.limiter = asyncio.Semaphore(LLM_CONCURRENCY)

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def run(
        self,
        input_dir: Path,
        output_dir: Path,
        resume: bool = False,
    ) -> None:
        """Process all SFD files asynchronously."""
        output_dir.mkdir(parents=True, exist_ok=True)
        job_key = f"batch:{input_dir.name}"

        jobs = await self._load_or_create_jobs(job_key, input_dir, resume)
        BATCH_DOCS_TOTAL.set(len(jobs))

        await self._pipeline(jobs)
        await self._dump_results(output_dir, jobs)

    # ------------------------------------------------------------------
    # Pipeline (OCR ‚Üí LLM)
    # ------------------------------------------------------------------
    async def _pipeline(self, jobs: list[Job]) -> None:
        """Run OCR + LLM asynchronously."""
        ocr_tasks = [self._ocr_one(job) for job in jobs if job.status == "pending"]
        await asyncio.gather(*ocr_tasks)

        llm_tasks = [self._llm_one(job) for job in jobs if job.status == "ocr_ok"]
        await asyncio.gather(*llm_tasks)

    async def _ocr_one(self, job: Job) -> None:
        """OCR single file."""
        try:
            req = OCRRequest(file_path=str(job.path), language="fra", preprocess=True)
            job.ocr_text = await asyncio.to_thread(extract_text, req)
            job.status = "ocr_ok"
        except Exception as e:
            job.status, job.error = "ocr_failed", str(e)
            logging.exception("OCR failed for %s", job.path.name)

    async def _llm_one(self, job: Job) -> None:
        """LLM analysis single file."""
        async with self.limiter:
            try:
                qwen3 = await self.swapper.ensure_model_loaded("qwen3")
                job.result = await qwen3.analyze(job.ocr_text)
                job.status = "done"
                BATCH_SUCCESS_TOTAL.inc()
            except Exception as e:
                job.status, job.error = "llm_failed", str(e)
                logging.exception("LLM failed for %s", job.path.name)

    # ------------------------------------------------------------------
    # Persistence
    # ------------------------------------------------------------------
    async def _load_or_create_jobs(
        self,
        key: str,
        input_dir: Path,
        resume: bool,
    ) -> list[Job]:
        """Load or create job list."""
        if resume and await self.redis.exists(key):
            raw = await self.redis.get(key)
            return [Job(**j) for j in json.loads(zstd.decompress(raw).decode())]

        files = [p for p in input_dir.iterdir() if p.suffix.lower() in {".pdf", ".txt", ".docx"}]
        jobs = [Job(p) for p in files]
        await self._save_jobs(key, jobs)
        return jobs

    async def _save_jobs(self, key: str, jobs: list[Job]) -> None:
        """Save current state to Redis."""
        await self.redis.set(
            key,
            zstd.compress(json.dumps([asdict(j) for j in jobs]).encode()),
            ex=3600,
        )

    async def _dump_results(self, output_dir: Path, jobs: list[Job]) -> None:
        """Write final results to disk."""
        summary = {
            "total": len(jobs),
            "success": sum(1 for j in jobs if j.status == "done"),
            "failed": sum(1 for j in jobs if j.status.endswith("failed")),
        }

        async with aiofiles.open(output_dir / "summary.json", "w") as f:
            await f.write(json.dumps(summary, indent=2))

        for job in jobs:
            if job.result:
                out_file = output_dir / f"{job.path.stem}.json"
                async with aiofiles.open(out_file, "w") as f:
                    await f.write(json.dumps(job.result, indent=2))

        gc.collect()  # Free memory
```

---

## Fichier : `backend\altiora\core\batch\scheduler.py`

```python
# backend/altiora/core/batch/scheduler.py
"""
Ordonnanceur de t√¢ches batch.

G√®re la file d‚Äôattente, la planification et le suivi des jobs.
"""

from __future__ import annotations

import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Any

from altiora.infrastructure.queue.redis_queue import RedisTaskQueue


class BatchScheduler:
    """Planificateur simple bas√© sur Redis."""

    def __init__(self) -> None:
        self.queue = RedisTaskQueue()

    async def schedule(
        self,
        payload: dict[str, Any],
        delay: int = 0,
    ) -> str:
        """Ajoute un job dans la file avec un d√©lai optionnel."""
        job_id = str(uuid.uuid4())
        eta = datetime.utcnow() + timedelta(seconds=delay)
        await self.queue.enqueue(
            "batch_process",
            {"job_id": job_id, **payload},
            eta=eta,
        )
        return job_id
```

---

## Fichier : `backend\altiora\core\batch\strategies.py`

```python
# backend/altiora/core/batch/strategies.py
"""
Strat√©gies de traitement batch.

D√©finit les patterns de retry, de parall√©lisation et de priorisation.
"""

from __future__ import annotations

import asyncio
import logging
from abc import ABC, abstractmethod
from typing import Any, Protocol

from tenacity import AsyncRetrying, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)


class BatchStrategy(Protocol):
    """Interface pour une strat√©gie batch."""

    async def execute(self, items: list[Any]) -> list[Any]:
        """Ex√©cute la strat√©gie sur la liste d‚Äôitems."""
        ...


class LinearStrategy:
    """Traitement s√©quentiel, simple et pr√©visible."""

    async def execute(self, items: list[Any]) -> list[Any]:
        """Traite les items un par un."""
        results = []
        for item in items:
            await asyncio.sleep(0)  # yield control
            results.append(await self._process(item))
        return results

    async def _process(self, item: Any) -> Any:
        """Logique m√©tier √† surcharger ou injecter."""
        return item


class ParallelStrategy:
    """Traitement parall√®le limit√© par un pool de workers."""

    def __init__(self, max_workers: int = 4) -> None:
        self.max_workers = max_workers

    async def execute(self, items: list[Any]) -> list[Any]:
        """Traite les items en parall√®le."""
        semaphore = asyncio.Semaphore(self.max_workers)

        async def _with_semaphore(item: Any) -> Any:
            async with semaphore:
                return await self._process(item)

        tasks = [_with_semaphore(item) for item in items]
        return await asyncio.gather(*tasks)

    async def _process(self, item: Any) -> Any:
        return item


class RetryStrategy:
    """Enrobe n‚Äôimporte quelle strat√©gie avec retry exponentiel."""

    def __init__(
        self,
        strategy: BatchStrategy,
        max_attempts: int = 3,
    ) -> None:
        self.strategy = strategy
        self.retry = AsyncRetrying(
            stop=stop_after_attempt(max_attempts),
            wait=wait_exponential(multiplier=1, min=1, max=10),
            reraise=True,
        )

    async def execute(self, items: list[Any]) -> list[Any]:
        """Ex√©cute la strat√©gie sous-jacente avec retry."""
        return await self.retry(self.strategy.execute, items)
```

---

## Fichier : `backend\altiora\core\batch\__init__.py`

```python
# backend/altiora/core/batch/__init__.py
"""
Batch processing ‚Äì orchestration et scheduling.
"""
```

---

## Fichier : `backend\altiora\core\factories\model_factory.py`

```python
from enum import Enum
from typing import Union
import logging

# backend/altiora/core/factories/model_factory.py
from altiora.core.models.qwen3.model_manager import Qwen3Manager
from altiora.core.models.starcoder2.model_manager import Starcoder2Manager

logger = logging.getLogger(__name__)


class ModelType(Enum):
    """√ânum√©ration des types de mod√®les d'IA support√©s par la fabrique."""
    QWEN3 = "qwen3"
    STARCODER2 = "starcoder2"


class ModelFactory:
    """Fabrique pour la cr√©ation et la gestion des instances de mod√®les d'IA.

    Cette fabrique impl√©mente le pattern Singleton pour chaque type de mod√®le,
    assurant qu'une seule instance d'un mod√®le donn√© est cr√©√©e et r√©utilis√©e
    tout au long de l'application. Cela permet d'optimiser l'utilisation des
    ressources en √©vitant de charger plusieurs fois le m√™me mod√®le en m√©moire.
    """
    _instances: Dict[ModelType, Union[Qwen3OllamaInterface, StarCoder2OllamaInterface]] = {}
    
    @classmethod
    async def create(cls, model_type: ModelType) -> Union[Qwen3OllamaInterface, StarCoder2OllamaInterface]:
        """Cr√©e ou r√©cup√®re une instance de mod√®le d'IA."

        Si une instance du mod√®le demand√© existe d√©j√†, elle est retourn√©e.
        Sinon, une nouvelle instance est cr√©√©e, initialis√©e et stock√©e.

        Args:
            model_type: Le type de mod√®le √† cr√©er ou √† r√©cup√©rer (ex: `ModelType.QWEN3`).

        Returns:
            Une instance du mod√®le d'IA sp√©cifi√©.

        Raises:
            ValueError: Si le type de mod√®le demand√© n'est pas reconnu.
        """
        if model_type not in cls._instances:
            logger.info(f"Cr√©ation d'une nouvelle instance pour le mod√®le : {model_type.value}")
            if model_type == ModelType.QWEN3:
                instance = Qwen3OllamaInterface()
            elif model_type == ModelType.STARCODER2:
                instance = StarCoder2OllamaInterface()
            else:
                raise ValueError(f"Type de mod√®le inconnu : {model_type}")
            
            await instance.initialize()
            cls._instances[model_type] = instance
        else:
            logger.info(f"R√©utilisation de l'instance existante pour le mod√®le : {model_type.value}")
        
        return cls._instances[model_type]


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        print("\n--- D√©monstration de ModelFactory ---")

        # Cr√©ation/r√©cup√©ration d'une instance de Qwen3.
        print("Cr√©ation de Qwen3...")
        qwen3_instance_1 = await ModelFactory.create(ModelType.QWEN3)
        print(f"Instance Qwen3 1 : {qwen3_instance_1}")

        print("Cr√©ation de Qwen3 (devrait r√©utiliser l'instance existante)...")
        qwen3_instance_2 = await ModelFactory.create(ModelType.QWEN3)
        print(f"Instance Qwen3 2 : {qwen3_instance_2}")
        assert qwen3_instance_1 is qwen3_instance_2, "Les instances de Qwen3 devraient √™tre les m√™mes."

        # Cr√©ation/r√©cup√©ration d'une instance de StarCoder2.
        print("\nCr√©ation de StarCoder2...")
        starcoder2_instance_1 = await ModelFactory.create(ModelType.STARCODER2)
        print(f"Instance StarCoder2 1 : {starcoder2_instance_1}")

        print("Cr√©ation de StarCoder2 (devrait r√©utiliser l'instance existante)...")
        starcoder2_instance_2 = await ModelFactory.create(ModelFactory.STARCODER2)
        print(f"Instance StarCoder2 2 : {starcoder2_instance_2}")
        assert starcoder2_instance_1 is starcoder2_instance_2, "Les instances de StarCoder2 devraient √™tre les m√™mes."

        # Nettoyage des instances (si les interfaces ont une m√©thode close).
        if hasattr(qwen3_instance_1, 'close'):
            await qwen3_instance_1.close()
        if hasattr(starcoder2_instance_1, 'close'):
            await starcoder2_instance_1.close()

        print("D√©monstration de ModelFactory termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\core\feedback\feedback_collector.py`

```python
# backend/altiora/core/feedback/feedback_collector.py
import json

from pydantic import BaseModel
from datetime import datetime
import redis.asyncio as redis


class Feedback(BaseModel):
    user_id: str
    query: str
    response: str
    rating: int
    correction: str | None = None
    timestamp: datetime


class FeedbackCollector:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url, decode_responses=True)

    async def add(self, fb: Feedback):
        key = f"feedback:{datetime.utcnow().isoformat()}"
        await self.redis.set(key, fb.model_dump_json())

    async def get_batch(self, min_rating: int = 3) -> list[Feedback]:
        keys = await self.redis.keys("feedback:*")
        items = [await self.redis.get(k) for k in keys]
        return [
            Feedback(**json.loads(i))
            for i in items
            if i and json.loads(i)["rating"] >= min_rating
        ]
```

---

## Fichier : `backend\altiora\core\models\model_swapper.py`

```python
# backend/altiora/core/models/model_swapper.py
"""
Gestionnaire de swap m√©moire pour les mod√®les.

Ce module CRITIQUE g√®re le chargement/d√©chargement des mod√®les
pour respecter la limite de 32GB RAM. UN SEUL mod√®le actif √† la fois.

Architecture:
1. Qwen3 analyse et d√©cide
2. Si besoin de code : sauver √©tat Qwen3, lib√©rer m√©moire
3. Charger Starcoder2, g√©n√©rer code
4. Lib√©rer Starcoder2, recharger Qwen3
5. Qwen3 finalise la r√©ponse

IMPORTANT: Toujours appeler cleanup() apr√®s usage d'un mod√®le!
"""

import gc
import os
import psutil
from typing import Optional, Dict, Any
import logging
from pathlib import Path
import pickle

from llama_cpp import Llama

from altiora.config.settings import settings

logger = logging.getLogger(__name__)


class ModelSwapper:
    """
    Gestionnaire de swap m√©moire pour mod√®les IA.

    Assure qu'UN SEUL mod√®le est en m√©moire √† la fois.
    Sauvegarde l'√©tat si n√©cessaire pour reprendre apr√®s swap.
    """

    def __init__(self):
        """Initialise le gestionnaire de swap."""
        self.current_model: Optional[Llama] = None
        self.current_model_name: Optional[str] = None
        self.state_cache_dir = Path("/tmp/altiora/model_states")
        self.state_cache_dir.mkdir(parents=True, exist_ok=True)

        # Configuration mmap pour √©conomiser la m√©moire
        self.model_configs = {
            "qwen3": {
                "path": settings.qwen.model_path,
                "n_ctx": settings.qwen.context_length,
                "n_threads": settings.qwen.threads,
                "use_mmap": True,  # CRUCIAL : utilise memory mapping
                "use_mlock": False,  # Ne pas verrouiller toute la RAM
                "n_batch": 512,
                "f16_kv": False,
                "verbose": False
            },
            "starcoder2": {
                "path": settings.starcoder.model_path,
                "n_ctx": settings.starcoder.context_length,
                "n_threads": settings.starcoder.threads,
                "use_mmap": True,  # CRUCIAL : utilise memory mapping
                "use_mlock": False,
                "n_batch": 256,
                "f16_kv": False,
                "verbose": False
            }
        }

    def get_memory_usage(self) -> Dict[str, float]:
        """
        Retourne l'utilisation m√©moire actuelle.

        Returns:
            Dict avec used_gb, available_gb, percent
        """
        memory = psutil.virtual_memory()
        return {
            "used_gb": memory.used / (1024 ** 3),
            "available_gb": memory.available / (1024 ** 3),
            "percent": memory.percent
        }

    async def ensure_model_loaded(self, model_name: str) -> Llama:
        """
        S'assure que le mod√®le demand√© est charg√©.

        Si un autre mod√®le est en m√©moire, le d√©charge d'abord.

        Args:
            model_name: "qwen3" ou "starcoder2"

        Returns:
            Instance du mod√®le Llama charg√©

        Raises:
            MemoryError: Si pas assez de RAM disponible
        """
        # V√©rifier la m√©moire disponible
        memory = self.get_memory_usage()
        logger.info(f"M√©moire avant chargement: {memory['used_gb']:.1f}GB utilis√©s, "
                    f"{memory['available_gb']:.1f}GB disponibles")

        # Si le bon mod√®le est d√©j√† charg√©, le retourner
        if self.current_model and self.current_model_name == model_name:
            logger.debug(f"Mod√®le {model_name} d√©j√† en m√©moire")
            return self.current_model

        # Si un autre mod√®le est charg√©, le d√©charger
        if self.current_model:
            logger.info(f"D√©chargement du mod√®le {self.current_model_name}")
            await self._unload_current_model()

        # V√©rifier qu'on a assez de m√©moire
        required_gb = 20 if model_name == "qwen3" else 15
        if memory['available_gb'] < required_gb + 2:  # +2GB de marge
            raise MemoryError(
                f"Pas assez de m√©moire pour charger {model_name}. "
                f"Requis: {required_gb}GB, Disponible: {memory['available_gb']:.1f}GB"
            )

        # Charger le nouveau mod√®le
        logger.info(f"Chargement du mod√®le {model_name}...")
        config = self.model_configs[model_name]

        try:
            self.current_model = Llama(
                model_path=str(config["path"]),
                **{k: v for k, v in config.items() if k != "path"}
            )
            self.current_model_name = model_name

            # V√©rifier la m√©moire apr√®s chargement
            memory_after = self.get_memory_usage()
            logger.info(f"M√©moire apr√®s chargement: {memory_after['used_gb']:.1f}GB utilis√©s")

            return self.current_model

        except Exception as e:
            logger.error(f"Erreur chargement {model_name}: {e}")
            raise

    async def _unload_current_model(self) -> None:
        """
        D√©charge le mod√®le actuel de la m√©moire.

        Force la lib√©ration m√©moire avec garbage collection.
        """
        if not self.current_model:
            return

        model_name = self.current_model_name

        # Supprimer toutes les r√©f√©rences
        del self.current_model
        self.current_model = None
        self.current_model_name = None

        # Forcer le garbage collection - CRUCIAL!
        gc.collect()

        # Sur Linux, on peut aussi lib√©rer la m√©moire au niveau OS
        if hasattr(os, 'system'):
            os.system('sync && echo 3 > /proc/sys/vm/drop_caches 2>/dev/null')

        logger.info(f"Mod√®le {model_name} d√©charg√© et m√©moire lib√©r√©e")

        # Attendre un peu pour que l'OS r√©cup√®re la m√©moire
        import asyncio
        await asyncio.sleep(0.5)

    async def swap_to_model(self, target_model: str, state: Optional[Dict] = None) -> Llama:
        """
        Swap vers un mod√®le sp√©cifique avec √©tat optionnel.

        Args:
            target_model: Mod√®le cible ("qwen3" ou "starcoder2")
            state: √âtat √† restaurer apr√®s chargement

        Returns:
            Mod√®le charg√©
        """
        # Sauver l'√©tat du mod√®le actuel si demand√©
        if state and self.current_model_name:
            await self._save_state(self.current_model_name, state)

        # Charger le nouveau mod√®le
        model = await self.ensure_model_loaded(target_model)

        # Restaurer l'√©tat si disponible
        saved_state = await self._load_state(target_model)
        if saved_state:
            logger.info(f"√âtat restaur√© pour {target_model}")

        return model

    async def _save_state(self, model_name: str, state: Dict) -> None:
        """Sauvegarde l'√©tat d'un mod√®le."""
        state_file = self.state_cache_dir / f"{model_name}_state.pkl"
        with open(state_file, 'wb') as f:
            pickle.dump(state, f)

    async def _load_state(self, model_name: str) -> Optional[Dict]:
        """Charge l'√©tat sauvegard√© d'un mod√®le."""
        state_file = self.state_cache_dir / f"{model_name}_state.pkl"
        if state_file.exists():
            with open(state_file, 'rb') as f:
                return pickle.load(f)
        return None

    async def cleanup(self) -> None:
        """
        Nettoie toutes les ressources.

        TOUJOURS appeler cette m√©thode √† la fin!
        """
        await self._unload_current_model()

        # Nettoyer les √©tats sauvegard√©s
        for state_file in self.state_cache_dir.glob("*_state.pkl"):
            state_file.unlink()
```

---

## Fichier : `backend\altiora\core\models\__init__.py`

```python
# backend/altiora/core/models/__init__.py
"""
Contient les d√©finitions et les gestionnaires des mod√®les d'IA.
"""
```

---

## Fichier : `backend\altiora\core\models\ensemble\multi_model.py`

```python
from typing import List
import asyncio
from src.ensemble.voting_strategies import WeightedVoting

# backend/altiora/core/models/ensemble/multi_model.py
class MultiModelEnsemble:
    def __init__(self, models: List[str]):
        self.models = self.load_models(models)
        self.voting_strategy = WeightedVoting()

    async def generate(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse en utilisant plusieurs mod√®les"""
        responses = await asyncio.gather(*[
            model.generate(prompt) for model in self.models
        ])

        # Vote pond√©r√© bas√© sur la confiance
        best_response = self.voting_strategy.select(
            responses,
            weights=self.calculate_confidence_weights(responses)
        )

        return best_response
```

---

## Fichier : `backend\altiora\core\models\ensemble\router.py`

```python
# backend/altiora/core/models/ensemble/router.py
"""
Router qui choisit dynamiquement le mod√®le adapt√©.
"""

from __future__ import annotations

from typing import Any

from altiora.core.models.model_swapper import ModelSwapper


class ModelRouter:
    """D√©cide quel mod√®le activer selon la t√¢che demand√©e."""

    async def route(self, task: str, swapper: ModelSwapper) -> str:
        """Retourne le nom du mod√®le √† utiliser."""
        if "code" in task.lower() or "script" in task.lower():
            await swapper.ensure_model_loaded("starcoder2")
            return "starcoder2"
        await swapper.ensure_model_loaded("qwen3")
        return "qwen3"
```

---

## Fichier : `backend\altiora\core\models\ensemble\__init__.py`

```python
# backend/altiora/core/models/ensemble/__init__.py
"""
Routage intelligent entre Qwen3 et Starcoder2.
"""
```

---

## Fichier : `backend\altiora\core\models\qwen3\cache_optimizer.py`

```python
# backend/altiora/core/models/qwen3/cache_optimizer.py
"""
Optimisation du cache conversation Qwen3 (cl√©s, TTL, compression).
"""

from __future__ import annotations

import hashlib
from typing import Any

from altiora.infrastructure.cache.unified_cache import UnifiedCache


class QwenCacheOptimizer:
    """Utilitaire de cache sp√©cifique aux prompts Qwen3."""

    def __init__(self, cache: UnifiedCache) -> None:
        self.cache = cache

    def _cache_key(self, prompt: str) -> str:
        """G√©n√®re une cl√© stable √† partir du prompt."""
        return f"qwen:{hashlib.sha256(prompt.encode()).hexdigest()}"

    async def get(self, prompt: str) -> str | None:
        """R√©cup√®re la r√©ponse en cache."""
        key = self._cache_key(prompt)
        return await self.cache.get(key)

    async def set(self, prompt: str, response: str, ttl: int = 3600) -> None:
        """Stocke la r√©ponse avec TTL par d√©faut 1 h."""
        key = self._cache_key(prompt)
        await self.cache.set(key, response, ttl=ttl)
```

---

## Fichier : `backend\altiora\core\models\qwen3\config.py`

```python
# backend/altiora/core/models/qwen3/config.py
"""
Configuration sp√©cifique pour le mod√®le Qwen3.
"""
```

---

## Fichier : `backend\altiora\core\models\qwen3\model_manager.py`

```python
# backend/altiora/core/models/qwen3/model_manager.py
"""
Manager Qwen3-32B avec modes thinking et direct.
"""

from __future__ import annotations

import logging
from typing import Any

from llama_cpp import Llama

logger = logging.getLogger(__name__)


class Qwen3Manager:
    """Wrapper autour de l‚Äôinstance Llama pour Qwen3."""

    def __init__(self, model: Llama) -> None:
        self.model = model

    async def analyze(self, spec: str) -> str:
        """Analyse une sp√©cification."""
        prompt = (
            f"<|im_start|>system\n"
            f"You are a QA expert. Analyze the following spec step by step.\n"
            f"<|im_end|>\n"
            f"<|im_start|>user\n{spec}<|im_end|>\n"
            f"<|im_start|>assistant\n<thinking>"
        )
        output = self.model(prompt, max_tokens=1500, stop=["</thinking>"])
        return output["choices"][0]["text"]

    async def direct(self, prompt: str) -> str:
        """R√©ponse directe sans thinking."""
        prompt = (
            f"<|im_start|>system\n"
            f"Answer concisely without showing reasoning.\n"
            f"<|im_end|>\n"
            f"<|im_start|>user\n{prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        output = self.model(prompt, max_tokens=800, temperature=0.7)
        return output["choices"][0]["text"]
```

---

## Fichier : `backend\altiora\core\models\qwen3\prompt_builder.py`

```python
# backend/altiora/core/models/qwen3/prompt_builder.py
"""
Constructeur de prompts sp√©cialis√©s Qwen3.
"""

from __future__ import annotations

from typing import Any


class QwenPromptBuilder:
    """Fabrique de prompts conformes au format Qwen3."""

    @staticmethod
    def build_thinking_prompt(context: str, task: str) -> str:
        """Prompt avec bloc <thinking>."""
        return (
            f"<|im_start|>system\n"
            f"You are Altiora, QA specialist. Think step by step.\n"
            f"<|im_end|>\n"
            f"<|im_start|>user\nContext: {context}\n\nTask: {task}<|im_end|>\n"
            f"<|im_start|>assistant\n<thinking>"
        )

    @staticmethod
    def build_direct_prompt(context: str, task: str) -> str:
        """Prompt sans bloc <thinking>."""
        return (
            f"<|im_start|>system\n"
            f"Answer directly and concisely.\n"
            f"<|im_end|>\n"
            f"<|im_start|>user\nContext: {context}\n\nTask: {task}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
```

---

## Fichier : `backend\altiora\core\models\qwen3\quantizer.py`

```python
# backend/altiora/core/models/qwen3/quantizer.py
"""
Utilitaires de quantification (placeholder pour futur support GPU).
"""

from __future__ import annotations


class QwenQuantizer:
    """Classe factice ‚Äì sera impl√©ment√©e si besoin de quantifier le mod√®le."""
```

---

## Fichier : `backend\altiora\core\models\qwen3\__init__.py`

```python
# backend/altiora/core/models/qwen3/__init__.py
"""
Gestion du mod√®le Qwen3-32B (mode thinking / direct).
"""
```

---

## Fichier : `backend\altiora\core\models\starcoder2\code_generator.py`

```python
# backend/altiora/core/models/starcoder2/code_generator.py
"""
G√©n√©ration de code de test avec templates Starcoder2.
"""

from __future__ import annotations

from pathlib import Path

from altiora.core.models.starcoder2.model_manager import Starcoder2Manager


class TestCodeGenerator:
    """G√©n√®re des templates de test (pytest, playwright‚Ä¶)."""

    def __init__(self, manager: Starcoder2Manager) -> None:
        self.manager = manager

    async def generate_pytest(self, spec_path: Path) -> str:
        """G√©n√®re un fichier pytest √† partir d‚Äôune spec."""
        spec_text = spec_path.read_text(encoding="utf-8")
        description = f"pytest tests for {spec_path.name}: {spec_text[:300]}..."
        return await self.manager.generate_code(
            language="python",
            description=description,
            framework="pytest",
        )
```

---

## Fichier : `backend\altiora\core\models\starcoder2\config.py`

```python
# backend/altiora/core/models/starcoder2/config.py
"""
Configuration sp√©cifique Starcoder2.
"""

from __future__ import annotations

from altiora.config.settings import settings

STARCODER2_DEFAULT_CONFIG = {
    "n_ctx": settings.starcoder.context_length,
    "n_threads": settings.starcoder.threads,
    "temperature": settings.starcoder.temperature,
    "max_tokens": settings.starcoder.max_tokens,
    "use_mmap": True,
    "use_mlock": False,
}
```

---

## Fichier : `backend\altiora\core\models\starcoder2\model_manager.py`

```python
# backend/altiora/core/models/starcoder2/model_manager.py
"""
Manager Starcoder2-15B (g√©n√©ration de code).
"""

from __future__ import annotations

import logging
from typing import Any

from llama_cpp import Llama

logger = logging.getLogger(__name__)


class Starcoder2Manager:
    """Wrapper Starcoder2 sp√©cialis√© code."""

    def __init__(self, model: Llama) -> None:
        self.model = model

    async def generate_code(
        self,
        language: str,
        description: str,
        framework: str | None = None,
    ) -> str:
        """G√©n√®re du code √† partir d‚Äôune description."""
        prompt = (
            f"<fim_prefix>Generate {language} code for: {description}"
            f"{f' using {framework}' if framework else ''}\n\n"
            f"<fim_suffix>\n\n<fim_middle>"
        )
        output = self.model(
            prompt,
            max_tokens=1200,
            temperature=0.2,
            stop=["<|endoftext|>"],
        )
        return output["choices"][0]["text"]
```

---

## Fichier : `backend\altiora\core\models\starcoder2\__init__.py`

```python
# backend/altiora/core/models/starcoder2/__init__.py
"""
Gestion du mod√®le Starcoder2-15B pour g√©n√©ration de code.
"""
```

---

## Fichier : `backend\altiora\core\modules\voice_anythingllm.py`

```python
import asyncio
import aiohttp
import speech_recognition as sr
import pyttsx3
import logging
from typing import Dict

logger = logging.getLogger(__name__)


class VoiceAnythingLLM:
    """Assistant vocal pilotant AnythingLLM via API"""

    def __init__(self, workspace_slug: str = "Altiora Knowledge"):
        self.workspace_slug = workspace_slug
        self.recognizer = sr.Recognizer()
        self.engine = pyttsx3.init()
        self.engine.setProperty('rate', 180)
        self.anythingllm_url = "http://localhost:3001"

    async def recognize_voice(self) -> str:
        """Reconnaissance vocale -> texte"""
        with sr.Microphone() as source:
            self.recognizer.adjust_for_ambient_noise(source)
            logger.info("üé§ Parlez...")

            audio = await asyncio.to_thread(
                self.recognizer.listen, source, timeout=3, phrase_time_limit=10
            )
            return await asyncio.to_thread(
                self.recognizer.recognize_google, audio, language="fr-FR"
            )

    async def send_to_anythingllm(self, text: str) -> Dict:
        """Envoie le texte au workspace AnythingLLM"""
        payload = {
            "message": text,
            "mode": "chat"
        }

        async with aiohttp.ClientSession() as session:
            url = f"{self.anythingllm_url}/api/v1/workspace/{self.workspace_slug}/chat"
            async with session.post(url, json=payload) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return data.get("textResponse", "")
                else:
                    logger.error(f"‚ùå Erreur AnythingLLM : {resp.status}")
                    return "Erreur de communication avec AnythingLLM."

    async def speak(self, text: str):
        """Synth√®se vocale"""
        await asyncio.to_thread(self.engine.say, text)
        await asyncio.to_thread(self.engine.runAndWait)

    async def start_session(self):
        """Boucle √©coute ‚Üí AnythingLLM ‚Üí voix"""
        logger.info("üé§ Mode vocal AnythingLLM activ√©")

        while True:
            try:
                # 1. Reconnaissance
                query = await self.recognize_voice()
                logger.info(f"üó£Ô∏è Re√ßu : {query}")

                # 2. Envoi √† AnythingLLM
                response = await self.send_to_anythingllm(query)
                logger.info(f"üí¨ R√©ponse : {response}")

                # 3. Vocalisation
                await self.speak(response)

            except sr.UnknownValueError:
                logger.debug("üîá Bruit ignor√©")
            except KeyboardInterrupt:
                logger.info("üëã Session vocale arr√™t√©e")
                break
            except Exception as e:
                logger.error(f"üí• Erreur : {e}")
                await self.speak("Erreur technique. Veuillez r√©essayer.")
```

---

## Fichier : `backend\altiora\core\modules\voice_assistant.py`

```python
import asyncio
import logging
import speech_recognition as sr
import pyttsx3
from typing import Optional

logger = logging.getLogger(__name__)


class VoiceAssistant:
    """Assistant vocal pour Altiora"""

    def __init__(self, altiora_core):
        self.recognizer = sr.Recognizer()
        self.engine = pyttsx3.init()
        self.engine.setProperty('rate', 180)  # Vitesse de parole
        self.engine.setProperty('voice', 'fr')  # Langue fran√ßaise
        self.altiora = altiora_core
        self.is_listening = False

    async def start_listening(self):
        """Lance l'√©coute continue"""
        self.is_listening = True
        with sr.Microphone() as source:
            self.recognizer.adjust_for_ambient_noise(source)
            logger.info("üé§ Assistant vocal pr√™t...")

            while self.is_listening:
                try:
                    audio = await asyncio.to_thread(
                        self.recognizer.listen,
                        source,
                        timeout=1,
                        phrase_time_limit=5
                    )
                    text = await asyncio.to_thread(
                        self.recognizer.recognize_google,
                        audio,
                        language="fr-FR"
                    )

                    if text and any(kw in text.lower() for kw in ["altiora", "analyse", "test"]):
                        logger.info(f"üó£Ô∏è Entendu : {text}")
                        response = await self.altiora.process_request(text)
                        await self.speak(response)

                except sr.UnknownValueError:
                    pass  # Bruit ignor√©
                except sr.RequestError as e:
                    logger.error(f"üé§ Erreur reconnaissance : {e}")

    async def speak(self, text: str):
        """Synth√®se vocale"""
        logger.info(f"üîä R√©ponse : {text}")
        await asyncio.to_thread(self.engine.say, text)
        await asyncio.to_thread(self.engine.runAndWait)

    def stop(self):
        """Arr√™te l'√©coute"""
        self.is_listening = False
```

---

## Fichier : `backend\altiora\core\modules\__init__.py`

```python
# backend/altiora/core/modules/__init__.py
"""Initialise le package `modules` de l'application Altiora.

Ce package contient des modules fonctionnels sp√©cifiques qui peuvent √™tre
r√©utilis√©s √† travers diff√©rentes parties de l'application. Chaque sous-package
dans `modules` est d√©di√© √† une fonctionnalit√© ou un domaine particulier.
"""

```

---

## Fichier : `backend\altiora\core\modules\psychodesign\altiora_core.py`

```python
# backend/altiora/core/modules/psychodesign/altiora_core.py
"""Noyau de personnalit√© et d'apprentissage supervis√© pour l'IA Altiora.

Ce module g√®re les traits de personnalit√© de l'IA, son √©volution via le
feedback utilisateur et l'apprentissage supervis√©. Il int√®gre des m√©canismes
de validation administrative pour assurer une √©volution contr√¥l√©e et s√©curis√©e.
"""

import json
import logging
from collections import defaultdict
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import numpy as np

from guardrails.admin_control_system import AdminControlSystem, AdminCommand
from guardrails.ethical_safeguards import EthicalSafeguards
from src.modules.psychodesign.personality_quiz import PersonalityProfile

logger = logging.getLogger(__name__)


@dataclass
class PersonalityEvolution:
    """Enregistrement d'un changement dans les traits de personnalit√© de l'IA."""
    timestamp: datetime
    change_type: str # Type de changement (ex: 'trait_formalite', 'preference_update').
    old_value: float # Ancienne valeur du trait ou de la pr√©f√©rence.
    new_value: float # Nouvelle valeur du trait ou de la pr√©f√©rence.
    reason: str      # Raison du changement (ex: 'feedback utilisateur', 'ajustement automatique').
    source: str      # Source du changement (ex: 'auto', 'user_feedback', 'admin_override').
    approved: bool = False # Indique si le changement a √©t√© approuv√© par un administrateur.
    admin_review: Optional[str] = None # Commentaires de l'administrateur.


@dataclass
class LearningProposal:
    """Repr√©sente une proposition de modification de la personnalit√© issue de l'apprentissage supervis√©."""
    proposal_id: str
    user_id: str
    suggested_changes: Dict[str, Any] # Changements propos√©s (ex: {"formalite": 0.7}).
    confidence_score: float # Score de confiance de la proposition (entre 0 et 1).
    evidence: List[Dict[str, Any]] # Preuves ou donn√©es brutes ayant men√© √† la proposition.
    timestamp: datetime
    status: str = "pending" # Statut de la proposition ('pending', 'approved', 'rejected').
    admin_decision: Optional[str] = None # D√©cision de l'administrateur.


class AltioraCore:
    """Moteur de personnalit√© de l'IA avec capacit√©s d'apprentissage supervis√©."""

    def __init__(self, user_id: str, admin_system: AdminControlSystem):
        """Initialise le noyau Altiora."

        Args:
            user_id: L'identifiant de l'utilisateur associ√© √† cette instance du noyau.
            admin_system: Une instance de `AdminControlSystem` pour la gestion des commandes administratives.
        """
        self.user_id = user_id
        self.admin_system = admin_system
        self.ethical_safeguards = EthicalSafeguards() # Syst√®me de garde-fous √©thiques.

        self.core_path = Path("altiora_core") # R√©pertoire pour la persistance des donn√©es du noyau.
        self.core_path.mkdir(exist_ok=True)

        self.personality = self._load_default_personality()
        self.evolution_history: List[PersonalityEvolution] = []
        self.learning_proposals: List[LearningProposal] = []

        self.supervised_mode = False # Mode d'apprentissage supervis√© (True/False).
        self.learning_mode = "conservative" # Strat√©gie d'apprentissage ('conservative', 'adaptive').
        self.logger = self._setup_logging()

    # ------------------------------------------------------------------
    # Initialisation et Persistance
    # ------------------------------------------------------------------

    def _setup_logging(self) -> logging.Logger:
        """Configure un logger sp√©cifique pour cette instance du noyau Altiora."""
        logger = logging.getLogger(f"altiora_core_{self.user_id}")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(self.core_path / f"{self.user_id}.log")
        formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger

    def _load_default_personality(self) -> PersonalityProfile:
        """Charge le profil de personnalit√© de l'utilisateur depuis le disque, ou cr√©e un profil par d√©faut."

        Returns:
            L'objet `PersonalityProfile` charg√© ou par d√©faut.
        """
        profile_file = self.core_path / f"{self.user_id}_profile.json"
        if profile_file.exists():
            try:
                with open(profile_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return PersonalityProfile(**data)
            except (IOError, OSError, json.JSONDecodeError) as e:
                self.logger.error(f"Erreur lors du chargement du profil pour {self.user_id}: {e}")

        # Retourne un profil par d√©faut si aucun n'est trouv√© ou si le chargement √©choue.
        return PersonalityProfile(
            user_id=self.user_id,
            traits={
                "formalite": 0.6,
                "empathie": 0.7,
                "humor": 0.2,
                "proactivite": 0.5,
                "verbosite": 0.5,
                "confirmation": 0.3,
                "technical_level": 0.7,
            },
            preferences={
                "vouvoiement": True,
                "expressions": ["Parfait!", "Int√©ressant", "Voyons voir..."],
                "voice_settings": {"pitch": 1.0, "speed": 1.1, "intonation": "dynamique"},
            },
            vocal_profile={},
            behavioral_patterns={},
            quiz_metadata={"created_at": datetime.now().isoformat()},
        )

    # ------------------------------------------------------------------
    # API Publique ‚Äì Interaction et Apprentissage
    # ------------------------------------------------------------------

    async def process_learning_feedback(self, feedback: Dict[str, Any]) -> Optional[LearningProposal]:
        """Traite un feedback utilisateur et g√©n√®re une proposition d'apprentissage."

        Args:
            feedback: Un dictionnaire contenant le feedback de l'utilisateur.

        Returns:
            Une `LearningProposal` si une proposition est g√©n√©r√©e, sinon None.
        """
        feedback_type = feedback.get("type")
        if feedback_type == "correction":
            return await self._handle_correction_feedback(feedback)
        if feedback_type == "adjustment":
            return await self._handle_adjustment_feedback(feedback)
        if feedback_type == "explicit_preference":
            return await self._handle_preference_feedback(feedback) # M√©thode √† impl√©menter.
        return None

    async def handle_user_interaction(self, interaction: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse une interaction utilisateur, applique les garde-fous √©thiques et g√©n√®re une r√©ponse personnalis√©e."

        Args:
            interaction: Un dictionnaire d√©crivant l'interaction de l'utilisateur.

        Returns:
            Un dictionnaire contenant la r√©ponse de l'IA et potentiellement des informations sur les propositions d'apprentissage.
        """
        alert = await self.ethical_safeguards.analyze_interaction(self.user_id, interaction)
        if alert and alert.severity == "critical":
            return {
                "status": "blocked",
                "message": "Interaction bloqu√©e pour raisons √©thiques",
                "alert_id": alert.alert_id,
            }

        response = await self._generate_response()
        if interaction.get("type") == "feedback":
            proposal = await self.process_learning_feedback(interaction)
            if proposal:
                response["learning_proposal"] = proposal.proposal_id

        self.logger.info("Interaction trait√©e : %s", interaction.get("type"))
        return response

    # ------------------------------------------------------------------
    # Gestion des Propositions d'Apprentissage
    # ------------------------------------------------------------------

    async def _handle_correction_feedback(self, feedback: Dict[str, Any]) -> Optional[LearningProposal]:
        """Traite le feedback de correction et g√©n√®re une proposition d'apprentissage."""
        original = feedback.get("original")
        corrected = feedback.get("corrected")
        changes = self._analyze_correction_impact(original, corrected)
        if not changes:
            return None

        proposal = LearningProposal(
            proposal_id=f"corr_{self.user_id}_{datetime.now().isoformat()}",
            user_id=self.user_id,
            suggested_changes=changes,
            confidence_score=0.9,
            evidence=[{"type": "correction", "data": feedback}],
            timestamp=datetime.now(),
        )
        self.learning_proposals.append(proposal)
        await self._submit_for_admin_review(proposal)
        return proposal

    async def _handle_adjustment_feedback(self, feedback: Dict[str, Any]) -> Optional[LearningProposal]:
        """Traite le feedback d'ajustement et g√©n√®re une proposition d'apprentissage."""
        adjustment_type = feedback.get("adjustment_type")
        trait_map = {
            "shorter": ("verbosite", -0.2),
            "longer": ("verbosite", 0.2),
            "more_formal": ("formalite", 0.1),
            "less_formal": ("formalite", -0.1),
            "more_empathetic": ("empathie", 0.1),
            "less_empathetic": ("empathie", -0.1),
        }
        if adjustment_type not in trait_map:
            return None

        trait, delta = trait_map[adjustment_type]
        new_value = max(0.0, min(1.0, self.personality.traits[trait] + delta)) # Assure que la valeur reste entre 0 et 1.

        proposal = LearningProposal(
            proposal_id=f"adj_{self.user_id}_{datetime.now().isoformat()}",
            user_id=self.user_id,
            suggested_changes={trait: new_value},
            confidence_score=0.7,
            evidence=[{"type": "adjustment", "data": feedback}],
            timestamp=datetime.now(),
        )
        self.learning_proposals.append(proposal)
        await self._submit_for_admin_review(proposal)
        return proposal

    async def _handle_preference_feedback(self, feedback: Dict[str, Any]) -> Optional[LearningProposal]:
        """Traite le feedback de pr√©f√©rence explicite et g√©n√®re une proposition d'apprentissage."""
        # TODO: Impl√©menter la logique pour analyser le feedback de pr√©f√©rence.
        self.logger.warning(f"Feedback de pr√©f√©rence explicite non impl√©ment√© : {feedback}")
        return None

    # ------------------------------------------------------------------
    # Fonctions internes
    # ------------------------------------------------------------------

    def _analyze_correction_impact(self, original: Optional[str], corrected: Optional[str]) -> Dict[str, float]:
        """Analyse l'impact d'une correction sur les traits de personnalit√© (ex: verbosit√©, formalit√©)."""
        changes: Dict[str, float] = {}
        if not original or not corrected:
            return changes

        # Exemple: Ajustement de la verbosit√© si la correction est significativement plus courte.
        if len(corrected) < len(original) * 0.8:
            changes["verbosite"] = max(0.0, self.personality.traits["verbosite"] - 0.1)

        # Exemple: Ajustement de la formalit√© bas√© sur la pr√©sence de certains indicateurs.
        formal_indicators = ["vous", "monsieur", "madame"]
        orig_formal = any(w in str(original).lower() for w in formal_indicators)
        corr_formal = any(w in str(corrected).lower() for w in formal_indicators)
        if orig_formal != corr_formal:
            delta = 0.1 if corr_formal else -0.1
            changes["formalite"] = max(0.0, min(1.0, self.personality.traits["formalite"] + delta))
        return changes

    async def _submit_for_admin_review(self, proposal: LearningProposal) -> None:
        """Soumet une proposition d'apprentissage √† l'administrateur pour examen et approbation."""
        command = AdminCommand(
            command_id=proposal.proposal_id,
            timestamp=datetime.now(),
            action="review_learning_proposal",
            target_user=self.user_id,
            parameters={
                "proposal": asdict(proposal),
                "user_id": self.user_id # Ajout de l'user_id pour le contexte admin.
            },
        )
        await self.admin_system.execute_admin_command(command)
        self.logger.info("Proposition d'apprentissage soumise pour examen : %s", proposal.proposal_id)

    async def _generate_response(self) -> Dict[str, Any]:
        """G√©n√®re une r√©ponse de l'IA bas√©e sur la personnalit√© actuelle (stub)."""
        # TODO: Int√©grer un mod√®le de g√©n√©ration de texte qui utilise les traits de personnalit√©.
        return {
            "status": "success",
            "response": "R√©ponse g√©n√©r√©e selon la personnalit√© actuelle (impl√©mentation √† venir).",
            "personality_snapshot": self.personality.traits,
        }

    async def apply_approved_changes(self, proposal_id: str) -> bool:
        """Applique les changements de personnalit√© valid√©s par un administrateur."

        Args:
            proposal_id: L'ID de la proposition d'apprentissage approuv√©e.

        Returns:
            True si les changements ont √©t√© appliqu√©s, False sinon.
        """
        proposal = next((p for p in self.learning_proposals if p.proposal_id == proposal_id), None)
        if not proposal or proposal.status != "approved":
            self.logger.warning(f"Proposition {proposal_id} non trouv√©e ou non approuv√©e. Changements non appliqu√©s.")
            return False

        for trait, new_value in proposal.suggested_changes.items():
            old_value = self.personality.traits.get(trait, 0.5) # R√©cup√®re l'ancienne valeur.
            evolution = PersonalityEvolution(
                timestamp=datetime.now(),
                change_type=f"trait_{trait}",
                old_value=old_value,
                new_value=float(new_value),
                reason=f"Approbation de la proposition d'apprentissage {proposal_id}",
                source="learning",
                approved=True,
            )
            self.evolution_history.append(evolution)
            self.personality.traits[trait] = float(new_value) # Applique le nouveau trait.
            self.logger.info(f"Trait '{trait}' mis √† jour de {old_value:.2f} √† {new_value:.2f}.")

        await self._save_state()
        self.logger.info("Changements de personnalit√© appliqu√©s : %s", proposal.suggested_changes)
        return True

    async def _save_state(self) -> None:
        """Sauvegarde l'√©tat complet du noyau Altiora (personnalit√©, historique, propositions)."""
        try:
            # Sauvegarde le profil de personnalit√©.
            with open(self.core_path / f"{self.user_id}_profile.json", "w", encoding='utf-8') as f:
                json.dump(asdict(self.personality), f, indent=2, default=str)
            # Sauvegarde l'historique d'√©volution.
            with open(self.core_path / f"{self.user_id}_evolution.json", "w", encoding='utf-8') as f:
                json.dump([asdict(e) for e in self.evolution_history], f, indent=2, default=str)
            # Sauvegarde les propositions d'apprentissage.
            with open(self.core_path / f"{self.user_id}_proposals.json", "w", encoding='utf-8') as f:
                json.dump([asdict(p) for p in self.learning_proposals], f, indent=2, default=str)
            self.logger.info(f"√âtat du noyau Altiora sauvegard√© pour l'utilisateur {self.user_id}.")
        except (IOError, OSError) as e:
            self.logger.error(f"Erreur lors de la sauvegarde de l'√©tat pour {self.user_id}: {e}")

    # ------------------------------------------------------------------
    # Acc√®s en lecture (pour le reporting ou le d√©bogage)
    # ------------------------------------------------------------------

    def get_personality_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des traits de personnalit√© actuels et des statistiques d'apprentissage."""
        return {
            "user_id": self.user_id,
            "current_traits": self.personality.traits,
            "evolution_count": len(self.evolution_history),
            "pending_proposals": len([p for p in self.learning_proposals if p.status == "pending"]),
            "last_change": self.evolution_history[-1].timestamp.isoformat() if self.evolution_history else None,
            "supervised_mode": self.supervised_mode,
            "learning_mode": self.learning_mode,
        }

    def get_evolution_report(self) -> str:
        """G√©n√®re un rapport textuel de l'√©volution de la personnalit√©."""
        lines = [f"üìà **Rapport d'√âvolution - {self.user_id}**", "", "**Traits actuels :**"]
        lines.extend(f"‚Ä¢ {k} : {v:.1%}" for k, v in self.personality.traits.items())
        lines += [
            "",
            f"**Historique :** {len(self.evolution_history)} changements",
            f"**Propositions en attente :** {len([p for p in self.learning_proposals if p.status == 'pending'])} ",
            f"**Mode apprentissage :** {self.learning_mode}",
        ]
        return "\n".join(lines)


class EvolutionAnalyzer:
    """Outils d'analyse des tendances de personnalit√© √† partir de l'historique d'√©volution."""

    @staticmethod
    def analyze_trends(evolution_history: List[PersonalityEvolution]) -> Dict[str, Any]:
        """Analyse les tendances d'√©volution des traits de personnalit√©."

        Args:
            evolution_history: La liste des objets `PersonalityEvolution`.

        Returns:
            Un dictionnaire contenant des analyses de tendances pour chaque trait.
        """
        if not evolution_history:
            return {}

        trends = defaultdict(list)
        for evo in evolution_history:
            if evo.change_type.startswith("trait_"):
                trait = evo.change_type[6:]
                trends[trait].append({"value": evo.new_value, "timestamp": evo.timestamp.isoformat()})

        analysis: Dict[str, Any] = {}
        for trait, changes in trends.items():
            if len(changes) >= 2:
                values = [float(c["value"]) for c in changes]
                analysis[trait] = {
                    "direction": "increasing" if values[-1] > values[0] else "decreasing",
                    "volatility": float(np.std(values)),
                    "total_change": float(values[-1] - values[0]),
                }
        return analysis


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Classe factice pour AdminControlSystem pour la d√©monstration.
    class MockAdminControlSystem:
        async def execute_admin_command(self, command: AdminCommand):
            logging.info(f"[MockAdmin] Commande re√ßue : {command.action} pour {command.target_user}")
            if command.action == "review_learning_proposal":
                # Simule l'approbation automatique pour la d√©mo.
                proposal_id = command.command_id
                for prop in altiora_core_instance.learning_proposals:
                    if prop.proposal_id == proposal_id:
                        prop.status = "approved"
                        prop.admin_decision = "Approuv√© automatiquement pour la d√©mo."
                        logging.info(f"[MockAdmin] Proposition {proposal_id} approuv√©e.")
                        break

    async def demo():
        user_id = "demo_user_1"
        mock_admin = MockAdminControlSystem()
        global altiora_core_instance # Rendre l'instance accessible au mock_admin.
        altiora_core_instance = AltioraCore(user_id, mock_admin)

        print("\n--- Profil de personnalit√© initial ---")
        print(altiora_core_instance.get_personality_summary())

        print("\n--- Simulation d'interaction utilisateur et feedback ---")
        # Simule un feedback de correction.
        feedback_correction = {
            "type": "correction",
            "original": "Le rapport est trop long et formel.",
            "corrected": "Rapport concis."
        }
        await altiora_core_instance.handle_user_interaction(feedback_correction)

        # Simule un feedback d'ajustement.
        feedback_adjustment = {
            "type": "adjustment",
            "adjustment_type": "shorter"
        }
        await altiora_core_instance.handle_user_interaction(feedback_adjustment)

        print("\n--- Propositions d'apprentissage en attente ---")
        for prop in altiora_core_instance.learning_proposals:
            print(f"Proposition ID: {prop.proposal_id}, Statut: {prop.status}, Changements: {prop.suggested_changes}")

        # Simule l'approbation des propositions par l'admin.
        print("\n--- Application des changements approuv√©s (simul√©e par l'admin) ---")
        for prop in altiora_core_instance.learning_proposals:
            if prop.status == "pending":
                # L'admin mock va approuver la proposition.
                await mock_admin.execute_admin_command(AdminCommand(
                    command_id=prop.proposal_id,
                    timestamp=datetime.now(),
                    action="review_learning_proposal",
                    target_user=user_id,
                    parameters={'proposal': asdict(prop)}
                ))
                await altiora_core_instance.apply_approved_changes(prop.proposal_id)

        print("\n--- Profil de personnalit√© apr√®s apprentissage ---")
        print(altiora_core_instance.get_personality_summary())

        print("\n--- Rapport d'√©volution ---")
        print(altiora_core_instance.get_evolution_report())

        print("\n--- Analyse des tendances ---")
        trends = EvolutionAnalyzer.analyze_trends(altiora_core_instance.evolution_history)
        print(f"Tendances : {trends}")

        print("D√©monstration de AltioraCore termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\core\modules\psychodesign\personality_evolution.py`

```python
# backend/altiora/core/modules/psychodesign/personality_evolution.py
"""Module pour g√©rer l'√©volution de la personnalit√© de l'IA via le fine-tuning supervis√©.

Ce module orchestre le processus d'entra√Ænement LoRA en arri√®re-plan en utilisant
des exemples d'interactions de haute qualit√©. Il permet d'ajouter de nouveaux
exemples d'entra√Ænement, de d√©clencher des cycles de fine-tuning, de v√©rifier
leur statut et de r√©cup√©rer le chemin du dernier adaptateur entra√Æn√©.
"""

import asyncio
import json
import logging
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

# Chemins bas√©s sur la structure du projet pour les donn√©es et scripts d'entra√Ænement.
TRAINING_DATA_PATH = Path("data/training/data/train_dataset.jsonl")
TRAINING_SCRIPT_PATH = Path("data/training/src/train_qwen3_thinkpad.py")
ADAPTERS_OUTPUT_DIR = Path("data/models/lora_adapters")


class PersonalityEvolution:
    """G√®re le cycle de vie du fine-tuning de la personnalit√© de l'IA."""

    def __init__(self):
        """Initialise le gestionnaire d'√©volution de la personnalit√©."

        S'assure que le r√©pertoire de sortie des adaptateurs existe.
        """
        ADAPTERS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        self.training_process: Optional[asyncio.subprocess.Process] = None

    async def add_training_example(self, example: Dict[str, str]) -> bool:
        """Ajoute un nouvel exemple de haute qualit√© au dataset d'entra√Ænement."

        L'exemple doit √™tre un dictionnaire avec les cl√©s "instruction", "input", "output",
        conform√©ment au format JSON Lines attendu par le script d'entra√Ænement.

        Args:
            example: Le dictionnaire contenant l'exemple d'entra√Ænement.

        Returns:
            True si l'ajout a r√©ussi, False sinon.
        """
        required_keys = {"instruction", "input", "output"}
        if not required_keys.issubset(example.keys()):
            logger.error(f"Exemple d'entra√Ænement invalide. Cl√©s requises : {required_keys}. Cl√©s fournies : {example.keys()}")
            return False

        try:
            # Cr√©e le r√©pertoire parent si n√©cessaire.
            TRAINING_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(TRAINING_DATA_PATH, "a", encoding="utf-8") as f:
                f.write(json.dumps(example, ensure_ascii=False) + "\n")
            logger.info(f"Nouvel exemple d'entra√Ænement ajout√© √† {TRAINING_DATA_PATH}.")
            return True
        except IOError as e:
            logger.error(f"Impossible d'√©crire dans le fichier d'entra√Ænement {TRAINING_DATA_PATH}: {e}")
            return False

    async def trigger_finetuning_cycle(self, min_new_examples: int = 10) -> Dict[str, Any]:
        """D√©clenche un nouveau cycle de fine-tuning si les conditions sont remplies."

        Args:
            min_new_examples: Nombre minimum de nouveaux exemples requis pour lancer un cycle.
                              (La logique de v√©rification du nombre d'exemples n'est pas impl√©ment√©e ici).

        Returns:
            Un dictionnaire indiquant le statut du d√©clenchement (started, already_running, error).
        """
        if self.training_process and self.training_process.returncode is None:
            logger.info(f"Un processus d'entra√Ænement est d√©j√† en cours (PID: {self.training_process.pid}).")
            return {"status": "already_running", "pid": self.training_process.pid}

        if not TRAINING_SCRIPT_PATH.exists():
            logger.error(f"Script d'entra√Ænement non trouv√© : {TRAINING_SCRIPT_PATH}.")
            return {"status": "error", "reason": "Training script not found"}

        logger.info("D√©clenchement d'un nouveau cycle de fine-tuning de la personnalit√©...")

        try:
            # Lance le script d'entra√Ænement en arri√®re-plan.
            # `asyncio.create_subprocess_exec` est utilis√© pour ex√©cuter un processus externe.
            self.training_process = await asyncio.create_subprocess_exec(
                "python",
                str(TRAINING_SCRIPT_PATH),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            logger.info(f"Processus d'entra√Ænement d√©marr√© avec le PID : {self.training_process.pid}.")
            return {"status": "started", "pid": self.training_process.pid}
        except FileNotFoundError:
            logger.error("La commande 'python' ou le script d'entra√Ænement n'a pas √©t√© trouv√©. Assurez-vous que Python est dans le PATH et que le script existe.")
            return {"status": "error", "reason": "Python interpreter or training script not found"}
        except Exception as e:
            logger.error(f"Erreur lors du lancement du script d'entra√Ænement : {e}")
            return {"status": "error", "reason": str(e)}

    async def get_training_status(self) -> Dict[str, Any]:
        """V√©rifie le statut du processus d'entra√Ænement en cours."

        Returns:
            Un dictionnaire d√©crivant le statut de l'entra√Ænement (not_running, running, completed_successfully, failed).
        """
        if not self.training_process:
            return {"status": "not_running"}

        if self.training_process.returncode is None:
            return {"status": "running", "pid": self.training_process.pid}
        else:
            # Le processus est termin√©, r√©cup√®re la sortie standard et d'erreur.
            stdout, stderr = await self.training_process.communicate()
            if self.training_process.returncode == 0:
                return {
                    "status": "completed_successfully",
                    "pid": self.training_process.pid,
                    "stdout": stdout.decode('utf-8'),
                }
            else:
                return {
                    "status": "failed",
                    "pid": self.training_process.pid,
                    "returncode": self.training_process.returncode,
                    "stderr": stderr.decode('utf-8'),
                }

    def get_latest_adapter(self) -> Optional[Path]:
        """Trouve le chemin du dernier adaptateur LoRA entra√Æn√©."

        Les adaptateurs sont suppos√©s √™tre stock√©s dans des sous-r√©pertoires
        nomm√©s 'checkpoint-N' o√π N est un num√©ro d'√©tape.

        Returns:
            Le chemin `Path` vers le r√©pertoire du dernier adaptateur,
            ou None si aucun adaptateur n'est trouv√©.
        """
        try:
            # Liste tous les sous-r√©pertoires qui ressemblent √† des checkpoints.
            adapters = [p for p in ADAPTERS_OUTPUT_DIR.iterdir() if p.is_dir() and p.name.startswith("checkpoint-")]
            if not adapters:
                logger.info(f"Aucun adaptateur trouv√© dans {ADAPTERS_OUTPUT_DIR}.")
                return None
            # Trie les adaptateurs par num√©ro de checkpoint pour trouver le plus r√©cent.
            latest_adapter = max(adapters, key=lambda p: int(p.name.split('-')[-1]))
            logger.info(f"Dernier adaptateur trouv√© : {latest_adapter}.")
            return latest_adapter
        except (FileNotFoundError, ValueError) as e:
            logger.error(f"Erreur lors de la recherche du dernier adaptateur : {e}")
            return None


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def main():
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        evolution = PersonalityEvolution()

        # 1. Ajouter un nouvel exemple d'entra√Ænement.
        new_example = {
            "instruction": "Reformule cette phrase de mani√®re plus empathique.",
            "input": "Le ticket est ferm√© car le probl√®me n'est pas reproductible.",
            "output": "Je comprends votre frustration. Pour l'instant, nous n'avons pas pu reproduire le probl√®me pour le corriger, mais nous restons attentifs si de nouvelles informations apparaissent."
        }
        print("\n--- Ajout d'un exemple d'entra√Ænement ---")
        await evolution.add_training_example(new_example)

        # 2. D√©clencher un cycle de fine-tuning (simulation).
        # Pour la d√©mo, nous allons cr√©er un script factice si n√©cessaire.
        if not TRAINING_SCRIPT_PATH.exists():
            TRAINING_SCRIPT_PATH.parent.mkdir(parents=True, exist_ok=True)
            TRAINING_SCRIPT_PATH.write_text("""
import time
import sys
print("Simulating a successful training run...")
time.sleep(1)
print("Training complete.")
sys.exit(0)
""")
            print(f"Script d'entra√Ænement factice cr√©√© √† : {TRAINING_SCRIPT_PATH}")
        
        print("\n--- D√©clenchement du Fine-Tuning ---")
        start_result = await evolution.trigger_finetuning_cycle(min_new_examples=1)
        logging.info(f"R√©sultat du d√©clenchement : {start_result}")

        if start_result["status"] == "started":
            print("Attente de la fin du processus d'entra√Ænement (simulation)...")
            # Attendre que le processus se termine.
            while True:
                status_result = await evolution.get_training_status()
                logging.info(f"Statut actuel : {status_result['status']}")
                if status_result['status'] not in ["running", "not_running"]:
                    break
                await asyncio.sleep(1)
            logging.info(f"Statut final de l'entra√Ænement : {status_result}")

        # 3. Trouver le dernier adaptateur (simulation).
        print("\n--- Recherche du dernier adaptateur ---")
        # Simuler la cr√©ation de r√©pertoires d'adaptateurs.
        (ADAPTERS_OUTPUT_DIR / "checkpoint-100").mkdir(parents=True, exist_ok=True)
        (ADAPTERS_OUTPUT_DIR / "checkpoint-200").mkdir(parents=True, exist_ok=True)
        latest = evolution.get_latest_adapter()
        logging.info(f"Dernier adaptateur trouv√© : {latest}")
        assert latest and latest.name == "checkpoint-200"

        print("D√©monstration de PersonalityEvolution termin√©e.")

        # Nettoyage des fichiers et r√©pertoires temporaires.
        if TRAINING_DATA_PATH.exists():
            TRAINING_DATA_PATH.unlink()
        if TRAINING_SCRIPT_PATH.exists():
            TRAINING_SCRIPT_PATH.unlink()
        if ADAPTERS_OUTPUT_DIR.exists():
            import shutil
            shutil.rmtree(ADAPTERS_OUTPUT_DIR)

    asyncio.run(main())
```

---

## Fichier : `backend\altiora\core\modules\psychodesign\personality_quiz.py`

```python
# backend/altiora/core/modules/psychodesign/personality_quiz.py
"""Module pour le quiz de personnalisation de l'IA Altiora.

Ce module permet de d√©finir le profil initial de la personnalit√© de l'IA
en posant une s√©rie de questions √† l'utilisateur. Il collecte des r√©ponses
textuelles et peut potentiellement analyser des caract√©ristiques vocales
pour affiner les traits de personnalit√© de l'assistant QA.
"""

import json
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Importation conditionnelle de speech_recognition.
try:
    import speech_recognition as sr
    HAS_SPEECH_RECOGNITION = True
except ImportError:
    sr = None
    HAS_SPEECH_RECOGNITION = False
    logging.warning("La biblioth√®que 'speech_recognition' n'est pas install√©e. La calibration vocale sera d√©sactiv√©e.")

logger = logging.getLogger(__name__)


@dataclass
class QuizResponse:
    """Repr√©sente une r√©ponse individuelle √† une question du quiz."""
    question_id: str
    response: Any
    confidence: float
    response_time: float
    vocal_features: Dict[str, float]


@dataclass
class PersonalityProfile:
    """Repr√©sente le profil de personnalit√© complet de l'IA pour un utilisateur donn√©."""
    user_id: str
    traits: Dict[str, float] # Traits de personnalit√© (ex: formalit√©, empathie).
    preferences: Dict[str, Any] # Pr√©f√©rences de communication (ex: vouvoiement, expressions).
    vocal_profile: Dict[str, Any] # Caract√©ristiques vocales (si calibration effectu√©e).
    behavioral_patterns: Dict[str, Any] # Mod√®les comportementaux identifi√©s.
    quiz_metadata: Dict[str, Any] # M√©tadonn√©es du quiz (date de compl√©tion, etc.).


class PersonalityQuiz:
    """Syst√®me de quiz de personnalisation avanc√© pour d√©finir le profil de l'IA."""

    def __init__(self, user_id: str):
        """Initialise le quiz de personnalit√©."

        Args:
            user_id: L'identifiant de l'utilisateur qui passe le quiz.
        """
        self.user_id = user_id
        self.responses: List[QuizResponse] = []
        self.vocal_samples: List[Dict[str, Any]] = []

        self.quiz_path = Path("quiz_data") # R√©pertoire pour sauvegarder les donn√©es du quiz.
        self.quiz_path.mkdir(exist_ok=True)

        # Initialisation conditionnelle de speech recognition.
        self.recognizer: Optional[sr.Recognizer] = None
        self.microphone: Optional[sr.Microphone] = None
        if HAS_SPEECH_RECOGNITION:
            self.recognizer = sr.Recognizer()
            self.microphone = sr.Microphone()

        self.questions = self._load_questions()

    # ------------------------------------------------------------------
    # Questionnaire (d√©finition des questions)
    # ------------------------------------------------------------------

    @staticmethod
    def _load_questions() -> List[Dict[str, Any]:
        """Charge la liste des questions du quiz."

        Returns:
            Une liste de dictionnaires, chaque dictionnaire repr√©sentant une question.
        """
        return [
            {
                "id": "comm_1",
                "type": "choice",
                "question": "Comment pr√©f√©rez-vous qu'on s'adresse √† vous ?",
                "options": [
                    {"text": "Salut ! (familier)", "value": "tu", "weight": 0.2},
                    {"text": "Bonjour (professionnel)", "value": "vous", "weight": 0.8},
                    {"text": "S'adapte selon le contexte", "value": "adaptive", "weight": 0.5}
                ],
                "trait": "formalite"
            },
            {
                "id": "comm_2",
                "type": "scale",
                "question": "Quand j'explique quelque chose, pr√©f√©rez-vous :",
                "scale": {
                    "min": "Aller directement au r√©sultat",
                    "max": "Avoir tous les d√©tails et le contexte"
                },
                "trait": "verbosite"
            },
            {
                "id": "comm_3",
                "type": "scenario",
                "question": "Je viens de terminer une analyse complexe. Votre r√©action pr√©f√©r√©e :",
                "options": [
                    {"text": "Parfait, donne-moi juste le r√©sum√©", "weight": 0.1},
                    {"text": "Super! Peux-tu m'expliquer les points cl√©s ?", "weight": 0.5},
                    {"text": "G√©nial! J'aimerais comprendre tout le processus", "weight": 0.9}
                ],
                "trait": "verbosite"
            },
            {
                "id": "work_1",
                "type": "choice",
                "question": "Face √† une erreur dans votre code, pr√©f√©rez-vous que je :",
                "options": [
                    {"text": "Corrige directement sans vous d√©ranger", "weight": 0.0},
                    {"text": "Vous montre la correction avec explication rapide", "weight": 0.3},
                    {"text": "Explique le probl√®me et vous guide vers la solution", "weight": 0.7},
                    {"text": "Fais une session compl√®te de debugging ensemble", "weight": 1.0}
                ],
                "trait": "empathie"
            },
            {
                "id": "vocal_1",
                "type": "calibration",
                "question": "Lisez cette phrase : 'Altiora, analyse le document de sp√©cification et cr√©e les tests'",
                "purpose": "baseline"
            },
            {
                "id": "vocal_2",
                "type": "calibration",
                "question": "Lisez : 'Non, je voulais dire le module de paiement, pas le module utilisateur'",
                "purpose": "correction"
            },
            {
                "id": "vocal_3",
                "type": "calibration",
                "question": "Lisez : 'Parfait! Exactement ce que je voulais'",
                "purpose": "satisfaction"
            }
        ]

    async def start_quiz(self) -> PersonalityProfile:
        """D√©marre le processus du quiz de personnalisation."

        Parcourt toutes les questions, collecte les r√©ponses et g√©n√®re le profil.

        Returns:
            L'objet `PersonalityProfile` g√©n√©r√©.
        """
        logger.info(f"\nQuiz de personnalisation Altiora pour {self.user_id}")
        print("=" * 60)

        for question in self.questions:
            await self._ask_question(question)

        await self._analyze_vocal_patterns() # Analyse les patterns vocaux si des √©chantillons ont √©t√© collect√©s.
        profile = self._generate_profile()
        await self._save_profile(profile)
        return profile

    # ------------------------------------------------------------------
    # Gestionnaires de questions
    # ------------------------------------------------------------------

    async def _ask_question(self, question: Dict[str, Any]) -> None:
        """Pose une question √† l'utilisateur et collecte sa r√©ponse."

        Args:
            question: Le dictionnaire repr√©sentant la question √† poser.
        """
        logger.info(f"\n{question['question']}")

        response: Dict[str, Any]
        if question["type"] == "choice":
            response = await self._handle_choice_question(question)
        elif question["type"] == "scale":
            response = await self._handle_scale_question(question)
        elif question["type"] == "calibration":
            response = await self._handle_calibration_question(question)
        else:
            response = await self._handle_text_question(question)

        self.responses.append(
            QuizResponse(
                question_id=question["id"],
                response=response["value"],
                confidence=response.get("confidence", 1.0),
                response_time=response.get("time", 0.0),
                vocal_features=response.get("vocal_features", {})
            )
        )

    @staticmethod
    async def _handle_choice_question(question: Dict[str, Any]) -> Dict[str, Any]:
        """G√®re les questions √† choix multiples."""
        for i, opt in enumerate(question["options"], 1):
            logger.info(f"  {i}. {opt['text']}")
        while True:
            try:
                choice = int(input("Votre choix (1-{}): ".format(len(question["options"]))).strip())
                if 1 <= choice <= len(question["options"]):
                    selected = question["options"][choice - 1]
                    return {"value": selected.get("value", selected["weight"])} # Retourne la valeur ou le poids.
                else:
                    logger.warning("Choix invalide. Veuillez entrer un nombre dans la plage indiqu√©e.")
            except ValueError:
                logger.warning("Entr√©e invalide. Veuillez entrer un nombre.")

    @staticmethod
    async def _handle_scale_question(_question: Dict[str, Any]) -> Dict[str, Any]:
        """G√®re les questions avec une √©chelle de valeur (ex: 0 √† 1)."""
        while True:
            try:
                val_str = input("Entrez une valeur entre 0 et 1 : ").strip()
                val = float(val_str)
                if 0.0 <= val <= 1.0:
                    return {"value": val}
                else:
                    logger.warning("Valeur hors de la plage. Veuillez entrer un nombre entre 0 et 1.")
            except ValueError:
                logger.warning("Entr√©e invalide. Veuillez entrer un nombre.")

    @staticmethod
    async def _handle_text_question(_question: Dict[str, Any]) -> Dict[str, Any]:
        """G√®re les questions n√©cessitant une r√©ponse textuelle libre."""
        text = input("R√©ponse : ").strip()
        return {"value": text}

    async def _handle_calibration_question(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """G√®re les questions de calibration vocale en utilisant `speech_recognition`."""
        if not self.recognizer or not self.microphone:
            logger.info("Module speech_recognition non disponible. Calibration vocale ignor√©e.")
            return {"value": "skipped", "confidence": 0.0, "vocal_features": {}}

        logger.info("\nCalibration vocale - Lisez la phrase apr√®s le signal. Appuyez sur Entr√©e quand pr√™t.")
        input("Appuyez sur Entr√©e quand pr√™t...")

        try:
            with self.microphone as source:
                logger.info("R√©glage du bruit ambiant...")
                self.recognizer.adjust_for_ambient_noise(source)
                logger.info("Parlez maintenant...")
                audio = self.recognizer.listen(source, timeout=5) # √âcoute pendant 5 secondes.

            text = self.recognizer.recognize_google(audio, language="fr-FR") # Utilise Google Speech Recognition.
            features = await self._extract_vocal_features(audio) # Extrait les caract√©ristiques vocales.
            self.vocal_samples.append({"text": text, "features": features, "purpose": question["purpose"]})
            logger.info(f"Transcription : \"{text}\"")
            return {"value": text, "confidence": 1.0, "vocal_features": features}
        except sr.UnknownValueError:
            logger.warning("Impossible de comprendre l'audio. Veuillez r√©essayer.")
            return await self._handle_calibration_question(question) # Demande de r√©essayer.
        except Exception as e:
            logger.error(f"Erreur lors de la calibration vocale : {e}")
            return {"value": "error", "confidence": 0.0, "vocal_features": {}}

    @staticmethod
    async def _extract_vocal_features(_audio: Any) -> Dict[str, float]:
        """Extrait les caract√©ristiques vocales √† partir d'un √©chantillon audio (stub pour l'instant)."

        Args:
            _audio: L'objet audio enregistr√©.

        Returns:
            Un dictionnaire de caract√©ristiques vocales (ex: pitch, speed, volume).
        """
        # TODO: Impl√©menter une analyse vocale r√©elle pour extraire des caract√©ristiques.
        return {"pitch": 220.0, "speed": 150.0, "volume": 0.7, "stress_indicators": 0.2}

    # ------------------------------------------------------------------
    # G√©n√©ration du profil de personnalit√©
    # ------------------------------------------------------------------

    async def _analyze_vocal_patterns(self) -> None:
        """Analyse les patterns vocaux collect√©s pour affiner le profil de personnalit√© (stub)."

        Cette m√©thode serait utilis√©e pour int√©grer les donn√©es vocales dans le calcul des traits.
        """
        # TODO: Impl√©menter l'analyse des patterns vocaux.
        pass

    def _generate_profile(self) -> PersonalityProfile:
        """G√©n√®re le profil de personnalit√© complet bas√© sur les r√©ponses du quiz et l'analyse vocale."""
        return PersonalityProfile(
            user_id=self.user_id,
            traits=self._calculate_traits(),
            preferences=self._analyze_preferences(),
            vocal_profile=self._create_vocal_profile(),
            behavioral_patterns=self._identify_patterns(),
            quiz_metadata={
                "completed_at": datetime.now().isoformat(),
                "question_count": len(self.responses),
                "calibration_samples": len(self.vocal_samples)
            }
        )

    def _calculate_traits(self) -> Dict[str, float]:
        """Calcule les traits de personnalit√© de l'IA bas√©s sur les r√©ponses du quiz."""
        # Valeurs par d√©faut des traits.
        traits = {
            "formalite": 0.6,
            "empathie": 0.7,
            "humor": 0.3,
            "proactivite": 0.5,
            "verbosite": 0.5,
            "confirmation": 0.3,
            "technical_level": 0.7
        }

        # Ajuste les traits en fonction des r√©ponses du quiz.
        for response in self.responses:
            question_id = response.question_id
            value = response.response

            if question_id == "comm_1":
                if value == "tu":
                    traits["formalite"] = 0.2
                elif value == "vous":
                    traits["formalite"] = 0.8
                elif value == "adaptive":
                    traits["formalite"] = 0.5

            elif question_id == "comm_2" and isinstance(value, (int, float)):
                traits["verbosite"] = float(value)
            # TODO: Ajouter la logique pour d'autres questions et traits.

        return traits

    def _analyze_preferences(self) -> Dict[str, Any]:
        """Analyse les pr√©f√©rences utilisateur bas√©es sur les r√©ponses du quiz."""
        preferences = {
            "vouvoiement": True,
            "expressions": ["Parfait!", "Int√©ressant", "Voyons voir..."],
            "voice_settings": {"pitch": 1.0, "speed": 1.1, "intonation": "dynamique"}
        }

        for response in self.responses:
            if response.question_id == "comm_1" and response.response == "tu":
                preferences["vouvoiement"] = False
                preferences["expressions"] = ["Cool!", "OK", "G√©nial!"]
            # TODO: Ajouter la logique pour d'autres pr√©f√©rences.

        return preferences

    def _create_vocal_profile(self) -> Dict[str, Any]:
        """Cr√©e le profil vocal bas√© sur les √©chantillons collect√©s."""
        if not self.vocal_samples:
            return {"status": "no_samples", "baseline": None}

        return {
            "samples": len(self.vocal_samples),
            "baseline": self.vocal_samples[0] if self.vocal_samples else None,
            "variations": self._analyze_vocal_variations() # Analyse les variations vocales.
        }

    def _analyze_vocal_variations(self) -> Dict[str, float]:
        """Analyse les variations vocales entre les √©chantillons (stub)."

        Cette m√©thode calculerait des m√©triques comme la variance du pitch, de la vitesse, etc.
        """
        if len(self.vocal_samples) < 2:
            return {}

        # TODO: Impl√©menter une analyse r√©elle des variations vocales.
        variations = {
            "pitch_variance": 0.1,
            "speed_variance": 0.05,
            "stress_change": 0.2
        }
        return variations

    def _identify_patterns(self) -> Dict[str, Any]:
        """Identifie les patterns comportementaux de l'utilisateur (stub)."

        Cette m√©thode analyserait les r√©ponses pour d√©duire des habitudes ou pr√©f√©rences.
        """
        # TODO: Impl√©menter l'identification des patterns comportementaux.
        patterns = {
            "response_time_avg": sum(r.response_time for r in self.responses) / len(self.responses) if self.responses else 0,
            "confidence_avg": sum(r.confidence for r in self.responses) / len(self.responses) if self.responses else 0,
            "quiz_completion": True
        }
        return patterns

    # ------------------------------------------------------------------
    # Persistance
    # ------------------------------------------------------------------

    async def _save_profile(self, profile: PersonalityProfile) -> None:
        """Sauvegarde le profil de personnalit√© g√©n√©r√© dans un fichier JSON."

        Args:
            profile: L'objet `PersonalityProfile` √† sauvegarder.
        """
        try:
            self.quiz_path.mkdir(parents=True, exist_ok=True)
            profile_path = self.quiz_path / f"{self.user_id}_profile.json"

            with open(profile_path, "w", encoding="utf-8") as f:
                # `default=str` est utilis√© pour s√©rialiser les objets `datetime` en cha√Ænes.
                json.dump(asdict(profile), f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"\n‚úÖ Profil de personnalit√© sauvegard√© : {profile_path}")
        except (IOError, OSError) as e:
            logger.error(f"\n‚ùå Erreur lors de la sauvegarde du profil : {e}")

    # ------------------------------------------------------------------
    # Assistants
    # ------------------------------------------------------------------

    def get_progress(self) -> Dict[str, Any]:
        """Retourne la progression actuelle du quiz."

        Returns:
            Un dictionnaire contenant le nombre de questions compl√©t√©es, le total,
            le pourcentage et la section courante.
        """
        return {
            "completed": len(self.responses),
            "total": len(self.questions),
            "percentage": (len(self.responses) / len(self.questions)) * 100.0 if len(self.questions) > 0 else 0.0,
            "current_section": self._get_current_section()
        }

    def _get_current_section(self) -> str:
        """Identifie la section courante du quiz bas√©e sur la progression."""
        if not self.responses:
            return "G√©n√©ral"

        if len(self.responses) >= len(self.questions):
            return "Termin√©"

        current_question = self.questions[len(self.responses)]
        section_map = {
            "comm": "Communication",
            "work": "Style de travail",
            "stress": "Gestion du stress",
            "humor": "Ton et humour",
            "tech": "Pr√©f√©rences techniques",
            "vocal": "Calibration vocale",
            "scenario": "Sc√©narios pratiques"
        }
        # Extrait le pr√©fixe de l'ID de la question (ex: 'comm' de 'comm_1').
        return section_map.get(current_question["id"].split("_")[0], "G√©n√©ral")


class QuizReporter:
    """G√©n√®re des rapports textuels et des r√©sum√©s des profils de personnalit√©."""

    @staticmethod
    def generate_summary(profile: PersonalityProfile) -> str:
        """G√©n√®re un r√©sum√© textuel concis du profil de personnalit√©."

        Args:
            profile: L'objet `PersonalityProfile` √† r√©sumer.

        Returns:
            Une cha√Æne de caract√®res format√©e avec les traits et pr√©f√©rences cl√©s.
        """
        traits = profile.traits
        prefs = profile.preferences

        summary = f"""
--- Rapport de Personnalisation Altiora ---
Utilisateur: {profile.user_id}
Date de compl√©tion: {profile.quiz_metadata['completed_at']}

Traits principaux:
- Formalit√©: {traits['formalite']:.0%}
- Empathie: {traits['empathie']:.0%}
- Humour: {traits['humor']:.0%}
- Proactivit√©: {traits['proactivite']:.0%}
- Verbosit√©: {traits['verbosite']:.0%}
- Confirmation: {traits['confirmation']:.0%}
- Niveau technique: {traits['technical_level']:.0%}

Pr√©f√©rences:
- Vouvoiement: {'Oui' if prefs['vouvoiement'] else 'Non'}
- Expressions favorites: {', '.join(prefs['expressions'][:3])}

Profil vocal:
- √âchantillons collect√©s: {profile.vocal_profile.get('samples', 0)}
- Statut: {profile.vocal_profile.get('status', 'Non calibr√©' if not profile.vocal_profile.get('samples') else 'Calibr√©')}
"""
        return summary


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def run_demo():
        quiz = PersonalityQuiz("demo_user")
        profile = await quiz.start_quiz()
        print(QuizReporter.generate_summary(profile))

        # Nettoyage des fichiers g√©n√©r√©s par la d√©mo.
        quiz.quiz_path.unlink(missing_ok=True)

    asyncio.run(run_demo())
```

---

## Fichier : `backend\altiora\core\modules\psychodesign\__init__.py`

```python
# backend/altiora/core/modules/psychodesign/__init__.py
"""Initialise le sous-package `psychodesign`.

Ce package est d√©di√© √† la gestion de la personnalit√© de l'IA Altiora,
incluant la d√©finition des traits, l'√©volution de la personnalit√© via
l'apprentissage supervis√©, et les m√©canismes de personnalisation.
"""

```

---

## Fichier : `backend\altiora\core\optimization\memory_optimizer.py`

```python
# backend/altiora/core/optimization/memory_optimizer.py
"""Module pour l'optimisation avanc√©e de la m√©moire lors du chargement des mod√®les d'IA.

Ce module fournit des strat√©gies pour r√©duire l'empreinte m√©moire des mod√®les
de langage (LLMs) lors de leur chargement et de leur utilisation. Il int√®gre
des techniques telles que la quantification 4-bit, le gradient checkpointing,
et le mappage m√©moire pour les poids des mod√®les, ce qui est crucial pour
l'ex√©cution de grands mod√®les sur des syst√®mes avec des ressources limit√©es.
"""

import torch
import gc

# Supposons que ces modules sont d√©finis ailleurs dans le r√©pertoire d'optimisation.
# from src.optimization.memory_pool import MemoryPool
# from src.optimization.model_loader import load_model_4bit


class AdvancedMemoryOptimizer:
    """Optimiseur de m√©moire avanc√© pour les mod√®les d'IA.

    Cette classe g√®re diverses techniques pour minimiser l'utilisation de la RAM
    par les mod√®les, permettant ainsi de charger des mod√®les plus grands ou
    d'ex√©cuter plus de mod√®les simultan√©ment.
    """

    def __init__(self):
        """Initialise l'optimiseur de m√©moire avanc√©."

        Il initialise un pool de m√©moire (si utilis√©) et d'autres composants
        n√©cessaires aux optimisations.
        """
        # self.memory_pool = MemoryPool() # Exemple d'int√©gration d'un pool de m√©moire.
        pass # Placeholder pour l'initialisation.

    def optimize_model_loading(self, model_path: str):
        """Charge un mod√®le avec des optimisations de m√©moire maximales."

        Args:
            model_path: Le chemin vers le mod√®le √† charger.

        Returns:
            Le mod√®le charg√© et optimis√©.

        Raises:
            ImportError: Si les biblioth√®ques n√©cessaires (ex: `bitsandbytes`) ne sont pas install√©es.
        """
        # 1. Quantification 4-bit : R√©duit la pr√©cision des poids du mod√®le √† 4 bits,
        #    diminuant drastiquement l'utilisation de la m√©moire tout en conservant
        #    une bonne partie de la performance.
        # model = load_model_4bit(model_path) # Utilise une fonction externe pour le chargement quantifi√©.
        model = torch.nn.Linear(10, 10) # Placeholder pour un mod√®le factice.
        
        # 2. Gradient checkpointing : Technique qui r√©duit l'utilisation de la m√©moire
        #    lors de l'entra√Ænement en ne stockant pas tous les activations interm√©diaires.
        #    Les activations sont recalcul√©es √† la vol√©e lors de la passe arri√®re.
        # if hasattr(model, 'gradient_checkpointing_enable'):
        #     model.gradient_checkpointing_enable()

        # 3. Mappage m√©moire pour les poids : Charge les poids du mod√®le directement depuis le disque
        #    dans la m√©moire virtuelle, sans les copier enti√®rement dans la RAM physique.
        model = self._memory_map_weights(model)

        # 4. Garbage collection agressif : Lib√®re imm√©diatement la m√©moire non utilis√©e.
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache() # Vide le cache de la m√©moire GPU.

        return model

    def _memory_map_weights(self, model: Any) -> Any:
        """Applique le mappage m√©moire aux poids du mod√®le (impl√©mentation factice).

        Dans une impl√©mentation r√©elle, cela impliquerait de charger les poids
        du mod√®le en utilisant des techniques de mappage m√©moire (ex: `mmap`
        ou des fonctionnalit√©s sp√©cifiques aux biblioth√®ques de ML).

        Args:
            model: Le mod√®le dont les poids doivent √™tre mapp√©s.

        Returns:
            Le mod√®le avec les poids mapp√©s en m√©moire.
        """
        # Cette fonction est un placeholder. L'impl√©mentation r√©elle d√©pendrait
        # de la structure du mod√®le et de la biblioth√®que de mappage m√©moire utilis√©e.
        logger.info("Mappage m√©moire des poids du mod√®le (fonction factice)...")
        return model


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        print("\n--- D√©monstration de AdvancedMemoryOptimizer ---")
        optimizer = AdvancedMemoryOptimizer()

        # Simule le chargement d'un mod√®le.
        # En r√©alit√©, `model_path` pointerait vers un fichier de poids de mod√®le.
        mock_model_path = "./path/to/my_large_model.bin"

        try:
            print(f"Chargement et optimisation du mod√®le depuis : {mock_model_path}")
            optimized_model = optimizer.optimize_model_loading(mock_model_path)
            print(f"Mod√®le optimis√© charg√© : {optimized_model}")
            print("V√©rifiez l'utilisation de la m√©moire de votre syst√®me.")
        except Exception as e:
            logging.error(f"Erreur lors de l'optimisation du chargement du mod√®le : {e}")

        print("D√©monstration de AdvancedMemoryOptimizer termin√©e.")

    import asyncio
    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\core\plugins\plugin_interface.py`

```python
# backend/altiora/core/plugins/plugin_interface.py
"""Module d√©finissant l'interface de base pour les plugins et un gestionnaire de plugins simple.

Ce module √©tablit le contrat que tout plugin doit respecter pour √™tre int√©gr√©
dans le syst√®me. Il fournit √©galement une classe `PluginManager` rudimentaire
pour enregistrer et ex√©cuter ces plugins.
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class Plugin(ABC):
    """Interface abstraite de base pour tous les plugins.

    Tout plugin doit h√©riter de cette classe et impl√©menter ses m√©thodes abstraites.
    """

    @abstractmethod
    async def initialize(self, config: Dict[str, Any]):
        """Initialise le plugin avec sa configuration sp√©cifique."

        Cette m√©thode est appel√©e une fois lors de l'enregistrement du plugin.

        Args:
            config: Un dictionnaire de configuration pour le plugin.
        """
        pass

    @abstractmethod
    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Ex√©cute la logique principale du plugin."

        Args:
            context: Un dictionnaire de donn√©es fournissant le contexte d'ex√©cution au plugin.

        Returns:
            Un dictionnaire contenant les r√©sultats de l'ex√©cution du plugin.
        """
        pass

    @abstractmethod
    async def cleanup(self):
        """Nettoie les ressources allou√©es par le plugin."

        Cette m√©thode est appel√©e lors de l'arr√™t du syst√®me ou du d√©senregistrement du plugin.
        """
        pass


class PluginManager:
    """Gestionnaire simple pour enregistrer et ex√©cuter des plugins."""

    def __init__(self):
        """Initialise le gestionnaire de plugins."""
        self._plugins: Dict[str, Plugin] = {}

    async def register_plugin(self, name: str, plugin: Plugin, config: Dict[str, Any]):
        """Enregistre un plugin et l'initialise."

        Args:
            name: Le nom unique du plugin.
            plugin: L'instance du plugin √† enregistrer.
            config: La configuration √† passer au plugin lors de son initialisation.
        """
        if name in self._plugins:
            logger.warning(f"Un plugin nomm√© '{name}' est d√©j√† enregistr√©. Il sera remplac√©.")
        await plugin.initialize(config)
        self._plugins[name] = plugin
        logger.info(f"Plugin '{name}' enregistr√© et initialis√©.")

    async def execute_plugin(self, name: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Ex√©cute la logique d'un plugin enregistr√©."

        Args:
            name: Le nom du plugin √† ex√©cuter.
            context: Le contexte d'ex√©cution √† passer au plugin.

        Returns:
            Le r√©sultat de l'ex√©cution du plugin.

        Raises:
            ValueError: Si le plugin n'est pas trouv√©.
        """
        plugin = self._plugins.get(name)
        if not plugin:
            raise ValueError(f"Plugin '{name}' non trouv√©.")
        logger.info(f"Ex√©cution du plugin '{name}'...")
        return await plugin.execute(context)

    async def unregister_plugin(self, name: str):
        """D√©senregistre un plugin et nettoie ses ressources."

        Args:
            name: Le nom du plugin √† d√©senregistrer.
        """
        plugin = self._plugins.pop(name, None)
        if plugin:
            await plugin.cleanup()
            logger.info(f"Plugin '{name}' d√©senregistr√© et nettoy√©.")
        else:
            logger.warning(f"Tentative de d√©senregistrer un plugin non existant : '{name}'.")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    class SimpleLoggerPlugin(Plugin):
        """Un plugin de d√©monstration qui loggue des messages."""
        async def initialize(self, config: Dict[str, Any]):
            self.prefix = config.get("prefix", "[LOG]")
            logging.info(f"{self.prefix} SimpleLoggerPlugin initialis√©.")

        async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
            message = context.get("message", "Pas de message.")
            logging.info(f"{self.prefix} Message du plugin : {message}")
            return {"status": "logged", "processed_message": message.upper()}

        async def cleanup(self):
            logging.info(f"{self.prefix} SimpleLoggerPlugin nettoy√©.")

    async def demo():
        manager = PluginManager()

        print("\n--- Enregistrement du plugin ---")
        await manager.register_plugin(
            "my_logger",
            SimpleLoggerPlugin(),
            {"prefix": "[APP_LOG]"}
        )

        print("\n--- Ex√©cution du plugin ---")
        result = await manager.execute_plugin(
            "my_logger",
            {"message": "Ceci est un message de test."}
        )
        print(f"R√©sultat de l'ex√©cution du plugin : {result}")

        print("\n--- D√©senregistrement du plugin ---")
        await manager.unregister_plugin("my_logger")

        print("D√©monstration du PluginManager termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\core\plugins\plugin_system.py`

```python
# backend/altiora/core/plugins/plugin_system.py
"""Module impl√©mentant un syst√®me de plugins dynamique pour l'application Altiora.

Ce syst√®me permet de charger des plugins √† partir d'un r√©pertoire sp√©cifi√©,
de les enregistrer et de les ex√©cuter √† des points d'extension pr√©d√©finis
(appel√©s "hooks"). Il fournit une interface `Plugin` que tous les plugins
doivent impl√©menter, assurant ainsi la modularit√© et l'extensibilit√© de l'application.
"""

import importlib.util
import inspect
import logging
from abc import ABC, abstractmethod
from functools import wraps
from pathlib import Path
from typing import Any, Dict, List

logger = logging.getLogger(__name__)


class Plugin(ABC):
    """Interface de base abstraite pour tous les plugins du syst√®me."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Le nom unique du plugin."""
        pass

    @property
    @abstractmethod
    def version(self) -> str:
        """La version du plugin."""
        pass

    @abstractmethod
    async def initialize(self, config: Dict[str, Any]):
        """Initialise le plugin avec sa configuration."

        Cette m√©thode est appel√©e une fois lors du chargement du plugin.

        Args:
            config: Un dictionnaire de configuration pour le plugin.
        """
        pass

    @abstractmethod
    async def execute(self, context: Dict[str, Any]) -> Any:
        """Ex√©cute la logique principale du plugin."

        Args:
            context: Un dictionnaire de donn√©es fournissant le contexte d'ex√©cution au plugin.

        Returns:
            Le r√©sultat de l'ex√©cution du plugin.
        """
        pass


class PluginManager:
    """G√®re le chargement, l'enregistrement et l'ex√©cution des plugins."""

    def __init__(self):
        """Initialise le gestionnaire de plugins."""
        self.plugins: Dict[str, Plugin] = {} # Stocke les instances de plugins par leur nom.
        self.hooks: Dict[str, List[Plugin]] = {} # Mappe les noms de hooks aux plugins abonn√©s.

    async def load_plugins(self, plugin_dir: str):
        """Charge tous les plugins √† partir d'un r√©pertoire sp√©cifi√©."

        Args:
            plugin_dir: Le chemin du r√©pertoire contenant les fichiers de plugins Python.
        """
        plugin_path = Path(plugin_dir)
        if not plugin_path.is_dir():
            logger.warning(f"Le r√©pertoire de plugins '{plugin_dir}' n'existe pas ou n'est pas un r√©pertoire.")
            return

        logger.info(f"Chargement des plugins depuis : {plugin_dir}")
        for file in plugin_path.glob("*.py"):
            if file.name.startswith("_") or file.name == "__init__.py":
                continue # Ignore les fichiers internes.

            module_name = file.stem
            try:
                # Charge le module dynamiquement.
                spec = importlib.util.spec_from_file_location(
                    module_name,
                    file
                )
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)

                    # Recherche les classes qui impl√©mentent l'interface Plugin.
                    for name, obj in inspect.getmembers(module):
                        if (inspect.isclass(obj) and
                                issubclass(obj, Plugin) and
                                obj != Plugin): # S'assure que ce n'est pas l'interface elle-m√™me.
                            plugin_instance = obj() # Instancie le plugin.
                            await self.register_plugin(plugin_instance)
                else:
                    logger.warning(f"Impossible de charger la sp√©cification pour le module {module_name}.")
            except Exception as e:
                logger.error(f"Erreur lors du chargement du plugin depuis {file}: {e}", exc_info=True)

    async def register_plugin(self, plugin: Plugin):
        """Enregistre une instance de plugin et l'initialise."

        Args:
            plugin: L'instance du plugin √† enregistrer.
        """
        if plugin.name in self.plugins:
            logger.warning(f"Un plugin nomm√© '{plugin.name}' est d√©j√† enregistr√©. Il sera remplac√©.")
        
        await plugin.initialize({}) # Initialise le plugin (peut prendre une configuration).
        self.plugins[plugin.name] = plugin
        logger.info(f"Plugin '{plugin.name}' v{plugin.version} charg√© et enregistr√©.")

    def hook(self, hook_name: str):
        """D√©corateur pour d√©finir un point d'extension (hook) dans le code."

        Les fonctions d√©cor√©es avec `@plugin_manager.hook("nom_du_hook")`
        ex√©cuteront les plugins enregistr√©s pour ce hook avant et apr√®s leur propre logique.

        Args:
            hook_name: Le nom du hook (ex: "before_sfd_analysis", "after_test_generation").

        Returns:
            Un d√©corateur qui peut √™tre appliqu√© √† une fonction asynchrone.
        """
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # Ex√©cute les plugins enregistr√©s pour le hook "before_" correspondant.
                for plugin in self.hooks.get(f"before_{hook_name}", []):
                    logger.debug(f"Ex√©cution du plugin '{plugin.name}' pour le hook 'before_{hook_name}'.")
                    await plugin.execute({"hook": hook_name, "stage": "before", "args": args, "kwargs": kwargs})

                # Ex√©cute la fonction originale.
                result = await func(*args, **kwargs)

                # Ex√©cute les plugins enregistr√©s pour le hook "after_" correspondant.
                for plugin in self.hooks.get(f"after_{hook_name}", []):
                    logger.debug(f"Ex√©cution du plugin '{plugin.name}' pour le hook 'after_{hook_name}'.")
                    await plugin.execute({"hook": hook_name, "stage": "after", "result": result, "args": args, "kwargs": kwargs})

                return result

            return wrapper

        return decorator

    async def register_hook_plugin(self, hook_name: str, plugin: Plugin):
        """Enregistre un plugin pour un hook sp√©cifique."

        Args:
            hook_name: Le nom du hook auquel le plugin doit s'abonner.
            plugin: L'instance du plugin √† enregistrer.
        """
        if hook_name not in self.hooks:
            self.hooks[hook_name] = []
        self.hooks[hook_name].append(plugin)
        logger.info(f"Plugin '{plugin.name}' enregistr√© pour le hook '{hook_name}'.")


# ------------------------------------------------------------------
# Exemple de plugin (pour la d√©monstration)
# ------------------------------------------------------------------
class MetricsPlugin(Plugin):
    """Plugin de d√©monstration pour collecter des m√©triques d'ex√©cution."""

    @property
    def name(self) -> str:
        return "metrics_collector"

    @property
    def version(self) -> str:
        return "1.0.0"

    async def initialize(self, config: Dict[str, Any]):
        self.metrics: Dict[str, List[float]] = {} # Stocke les dur√©es d'op√©ration.
        logger.info(f"MetricsPlugin initialis√©. Config: {config}")

    async def execute(self, context: Dict[str, Any]) -> Any:
        operation = context.get("operation")
        duration = context.get("duration")
        stage = context.get("stage")
        hook = context.get("hook")

        if operation and duration is not None:
            if operation not in self.metrics:
                self.metrics[operation] = []
            self.metrics[operation].append(duration)
            logger.info(f"MetricsPlugin: Hook '{hook}' ({stage}) - Op√©ration '{operation}' a pris {duration:.4f}s.")
        else:
            logger.debug(f"MetricsPlugin: Hook '{hook}' ({stage}) - Contexte : {context}")
        return None


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import time

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        manager = PluginManager()

        # Cr√©e un r√©pertoire factice pour les plugins.
        temp_plugin_dir = Path("temp_plugins")
        temp_plugin_dir.mkdir(exist_ok=True)

        # Cr√©e un fichier de plugin factice.
        (temp_plugin_dir / "my_metrics_plugin.py").write_text("""
from src.plugins.plugin_system import Plugin
import logging

logger = logging.getLogger(__name__)

class MyMetricsPlugin(Plugin):
    name = "my_metrics_plugin"
    version = "0.1.0"

    async def initialize(self, config):
        self.data = []
        logger.info("MyMetricsPlugin initialis√©.")

    async def execute(self, context):
        if context.get("stage") == "after":
            operation_name = context.get("args", ["unknown_op"])[0]
            duration = context.get("result", 0)
            self.data.append({"operation": operation_name, "duration": duration})
            logger.info(f"[MyMetricsPlugin] Enregistr√© : {operation_name} - {duration:.4f}s")
        return None
""")

        print("\n--- Chargement des plugins ---")
        await manager.load_plugins(str(temp_plugin_dir))

        # Enregistre le plugin de m√©triques pour un hook sp√©cifique.
        metrics_plugin_instance = MetricsPlugin()
        await manager.register_hook_plugin("process_data", metrics_plugin_instance)

        @manager.hook("process_data")
        async def process_data(item: str) -> float:
            """Fonction de d√©monstration qui sera instrument√©e par le hook."""
            logging.info(f"Traitement de l'√©l√©ment : {item}")
            delay = random.uniform(0.05, 0.2)
            await asyncio.sleep(delay)
            return delay

        print("\n--- Ex√©cution de fonctions avec hooks ---")
        results = []
        for i in range(5):
            duration = await process_data(f"data_{i}")
            results.append(duration)

        print("\n--- R√©sultats des m√©triques collect√©es par le plugin ---")
        print(metrics_plugin_instance.metrics)

        print("D√©monstration du PluginSystem termin√©e.")

        # Nettoyage.
        import shutil
        shutil.rmtree(temp_plugin_dir)

    import random
    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\core\plugins\__init__.py`

```python
# backend/altiora/core/plugins/__init__.py
"""Initialise le package `plugins` de l'application Altiora.

Ce package contient le syst√®me de plugins qui permet d'√©tendre les
fonctionnalit√©s de l'application de mani√®re dynamique. Il d√©finit
l'interface des plugins et le gestionnaire pour les charger et les ex√©cuter.

Les modules suivants sont expos√©s pour faciliter les importations :
- `PluginSystem`: Le gestionnaire principal du syst√®me de plugins.
"""
from .plugin_system import PluginSystem

__all__ = ['PluginSystem']

```

---

## Fichier : `backend\altiora\core\policies\business_rules.py`

```python
# backend/altiora/core/policies/business_rules.py
"""Module de validation des r√®gles m√©tier pour les artefacts g√©n√©r√©s.

Ce module garantit que les artefacts produits par l'IA (comme les scripts de test
Playwright ou les fichiers Excel) respectent un ensemble de r√®gles m√©tier pr√©d√©finies,
assurant ainsi leur qualit√©, leur coh√©rence et leur maintenabilit√©.
"""

import ast
import re
from typing import List, Dict, Any, Optional

# ------------------------------------------------------------------
# Constantes
# ------------------------------------------------------------------

# Le module de reporting standard que tous les tests doivent utiliser.
REPORTER_MODULE = "reports.standard_reporter"
# La fonction de reporting sp√©cifique √† appeler √† chaque √©tape du test.
REPORTER_FUNC = "report_step"


# ------------------------------------------------------------------
# R√®gles
# ------------------------------------------------------------------
class BusinessRules:
    """Validateur centralis√© pour les r√®gles m√©tier."""

    async def validate(
            self,
            code_string: str,
            *,
            workflow: str,
            meta: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Valide un artefact par rapport √† un workflow donn√©.

        Args:
            code_string: La cha√Æne de caract√®res de l'artefact √† valider (ex: code Python).
            workflow: Le type de workflow (ex: 'test', 'excel'). D√©termine quel
                      ensemble de r√®gles appliquer.
            meta: M√©tadonn√©es suppl√©mentaires pouvant √™tre utilis√©es par les r√®gles.

        Returns:
            Un dictionnaire contenant le r√©sultat de la validation:
            {"ok": bool, "violations": List[str]}
        """
        violations: List[str] = []

        try:
            if workflow == "test":
                violations = self._validate_playwright_test(code_string, meta)
            # D'autres workflows peuvent √™tre ajout√©s ici.
            # elif workflow == "excel":
            #     violations = self._validate_excel_file(meta)
        except Exception as e:
            violations.append(f"Erreur inattendue lors de la validation : {e}")

        return {"ok": not violations, "violations": violations}

    # ----------------------------------------------------------
    # R√®gles sp√©cifiques √† Playwright
    # ----------------------------------------------------------
    @staticmethod
    def _validate_playwright_test(
            code: str, meta: Optional[Dict[str, Any]] = None
    ) -> List[str]:
        """Applique un ensemble de r√®gles sp√©cifiques aux tests Playwright."""
        violations = []

        # R√®gle 1: Interdire `time.sleep()` au profit des attentes natives de Playwright.
        if "time.sleep" in code:
            violations.append(
                "R√®gle viol√©e : `time.sleep()` est interdit. Utilisez les attentes natives de Playwright (ex: `page.wait_for_selector`)."
            )

        # R√®gle 2: √âviter les URLs en dur pour favoriser la configuration.
        if re.search(r"page\.goto\(\s*["']https?://", code):
            violations.append(
                "R√®gle viol√©e : Les URLs ne doivent pas √™tre cod√©es en dur. Utilisez des variables de configuration."
            )

        # R√®gle 3: S'assurer de l'utilisation du module de reporting standard.
        reporter_imported = False
        reporter_used = False
        try:
            tree = ast.parse(code)
            for node in ast.walk(tree):
                # V√©rification de l'importation.
                if (
                        isinstance(node, ast.ImportFrom)
                        and node.module == REPORTER_MODULE
                ):
                    reporter_imported = any(
                        alias.name == REPORTER_FUNC for alias in node.names
                    )
                # V√©rification de l'utilisation.
                if (
                        isinstance(node, ast.Call)
                        and isinstance(node.func, ast.Name)
                        and node.func.id == REPORTER_FUNC
                ):
                    reporter_used = True

                # R√®gle 4: V√©rifier la qualit√© des noms et des docstrings des fonctions de test.
                if isinstance(node, ast.FunctionDef) and node.name.startswith(
                        "test_"
                ):
                    if not ast.get_docstring(node):
                        violations.append(
                            f"Qualit√© : Le test `{node.name}` n'a pas de docstring."
                        )
                    if node.name in {"test_unnamed", "test_script"}:
                        violations.append(
                            f"Qualit√© : Le nom du test `{node.name}` est trop g√©n√©rique."
                        )

        except SyntaxError as e:
            violations.append(f"Erreur de syntaxe Python : {e}")

        if not reporter_imported:
            violations.append(f"R√®gle viol√©e : L'import `{REPORTER_MODULE}.{REPORTER_FUNC}` est manquant.")
        if not reporter_used:
            violations.append(f"R√®gle viol√©e : La fonction de reporting `{REPORTER_FUNC}()` n'est pas appel√©e.")

        return violations


# ------------------------------------------------------------------
# D√©monstration rapide
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO)

    async def main() -> None:
        rules = BusinessRules()

        good_code = '''
from playwright.sync_api import Page
from reports.standard_reporter import report_step

def test_login_page(page: Page):
    """V√©rifie que la page de connexion se charge correctement."""
    report_step("Navigation vers la page de connexion")
    page.goto("/login")
'''

        bad_code = '''
import time

def test_unnamed(page):
    page.goto("https://example.com")
    time.sleep(2)
'''

        logging.info("--- Validation du bon code ---")
        good_result = await rules.validate(good_code, workflow="test")
        logging.info(good_result)

        logging.info("\n--- Validation du mauvais code ---")
        bad_result = await rules.validate(bad_code, workflow="test")
        logging.info(bad_result)

    asyncio.run(main())
```

---

## Fichier : `backend\altiora\core\policies\excel_policy.py`

```python
# backend/altiora/core/policies/excel_policy.py
"""Module contenant les r√®gles de validation pour les matrices de test.

Ce module garantit l'int√©grit√© des donn√©es destin√©es √† √™tre export√©es
vers des fichiers Excel, en s'assurant que les cas de test respectent
le format, la structure et le contenu attendus.
"""

from __future__ import annotations

import re
from typing import List, Dict, Any, Set, Final

# ------------------------------------------------------------------
# R√®gles de configuration
# ------------------------------------------------------------------

# Expression r√©guli√®re pour valider le format des identifiants de cas de test.
# Exemple de format valide : CU01_SB02_CP001_nom_du_test
TEST_CASE_ID_PATTERN: Final[re.Pattern[str]] = re.compile(
    r"^CU\d{2}_SB\d{2}_C[PEL]\d{3}_.+(?<!_)$"
)

# Ensemble des colonnes requises dans chaque dictionnaire de cas de test.
REQUIRED_COLUMNS: Final[Set[str]] = {"id", "description", "type"}

# Ensemble des types de cas de test valides.
VALID_TYPES: Final[Set[str]] = {"CP", "CE", "CL"}  # Cas Passant, Cas d'Erreur, Cas Limite


class ExcelPolicy:
    """Valide les donn√©es des cas de test avant leur exportation vers Excel."""

    def validate_test_matrix(
        self, test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Valide une liste de dictionnaires repr√©sentant des cas de test.

        Args:
            test_cases: Une liste de cas de test, o√π chaque cas est un dictionnaire.

        Returns:
            Un dictionnaire contenant le statut de la validation et la liste des erreurs.
            {
                "is_valid": bool,
                "errors": List[str]
            }
        """
        errors: List[str] = []
        errors.extend(self._validate_structure(test_cases))
        errors.extend(self._validate_content(test_cases))
        errors.extend(self._validate_uniqueness(test_cases))
        return {"is_valid": not errors, "errors": errors}

    # ------------------------------------------------------------------
    # Impl√©mentation des r√®gles
    # ------------------------------------------------------------------
    @staticmethod
    def _validate_structure(cases: List[Dict[str, Any]]) -> List[str]:
        """V√©rifie la pr√©sence des colonnes requises dans chaque cas de test."""
        errors = []
        for idx, case in enumerate(cases, start=2):  # Commence √† 2 pour correspondre aux lignes Excel
            missing = REQUIRED_COLUMNS - case.keys()
            if missing:
                errors.append(f"Ligne {idx}: Colonnes manquantes {sorted(missing)}.")
        return errors

    @staticmethod
    def _validate_content(cases: List[Dict[str, Any]]) -> List[str]:
        """Valide le format de l'ID, le type et la description de chaque cas."""
        errors = []
        for idx, case in enumerate(cases, start=2):
            case_id = str(case.get("id", ""))
            case_type = str(case.get("type", ""))
            description = str(case.get("description", "")).strip()

            if not TEST_CASE_ID_PATTERN.match(case_id):
                errors.append(f"Ligne {idx}: L'ID `{case_id}` a un format invalide.")

            if case_type not in VALID_TYPES:
                errors.append(
                    f"Ligne {idx}: Le type `{case_type}` est invalide (attendu : {VALID_TYPES})."
                )

            if not description:
                errors.append(f"Ligne {idx}: La description ne peut pas √™tre vide.")
        return errors

    @staticmethod
    def _validate_uniqueness(cases: List[Dict[str, Any]]) -> List[str]:
        """S'assure que chaque identifiant de cas de test est unique."""
        seen: Set[str] = set()
        errors = []
        for idx, case in enumerate(cases, start=2):
            case_id = str(case.get("id"))
            if case_id in seen:
                errors.append(f"Ligne {idx}: L'ID `{case_id}` est dupliqu√©.")
            seen.add(case_id)
        return errors


# ------------------------------------------------------------------
# Auto-test rapide
# ------------------------------------------------------------------
if __name__ == "__main__":
    policy = ExcelPolicy()

    valid_cases = [
        {
            "id": "CU01_SB01_CP001_connexion_valide",
            "description": "Tester la connexion r√©ussie avec un utilisateur valide.",
            "type": "CP",
        }
    ]
    invalid_cases = [
        {
            "id": "INVALID_ID",
            "description": "",
            "type": "XX",
        },
        {
            "id": "CU01_SB01_CP001_connexion_valide", # ID dupliqu√©
            "description": "Un autre test.",
            "type": "CP",
        }
    ]

    print("Validation de donn√©es valides :", policy.validate_test_matrix(valid_cases))
    print("Validation de donn√©es invalides :", policy.validate_test_matrix(invalid_cases))

```

---

## Fichier : `backend\altiora\core\policies\privacy_policy.py`

```python
# backend/altiora/core/policies/privacy_policy.py
"""Moteur de politique de confidentialit√© pour Altiora, centr√© sur l'utilisateur fran√ßais.

Ce module fournit des fonctionnalit√©s essentielles pour la conformit√© RGPD :
- D√©tection et masquage d'informations personnelles identifiables (PII) fran√ßaises.
- Application de r√®gles de r√©tention des donn√©es.
- Gestion du consentement de l'utilisateur et journalisation pour l'audit.
"""

import re
import json
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path

logger = logging.getLogger(__name__)


# ------------------------------------------------------------------
# Structures de donn√©es
# ------------------------------------------------------------------
@dataclass
class PIIDetection:
    """Repr√©sente une information personnelle identifiable (PII) d√©tect√©e."""
    type: str       # Type de PII (ex: email, phone).
    value: str      # La valeur originale d√©tect√©e.
    redacted: str   # La valeur masqu√©e.
    start: int      # L'index de d√©but dans le texte original.
    end: int        # L'index de fin.

@dataclass
class PrivacyReport:
    """Rapport g√©n√©r√© apr√®s l'analyse d'un texte."""
    text: str
    pii_list: List[PIIDetection]
    retention_seconds: int
    can_store: bool
    user_consent_required: bool


# ------------------------------------------------------------------
# Constantes de la politique
# ------------------------------------------------------------------

# Expressions r√©guli√®res pour d√©tecter les PII courantes en France.
PII_PATTERNS = {
    "email": r"[\w\.-]+@[\w\.-]+\.\w+",
    "phone": r"(\+?33[-.\s]?|0)[1-9]([-.\s]?\d{2}){4}",
    "credit_card": r"\b(?:\d{4}[\s-]?){3}\d{4}\b",
    "social_security": r"\b\d{3}[\s-]?\d{2}[\s-]?\d{3}[\s-]?\d{3}\b",
    "passport": r"\b[A-Z]{1,2}\d{6,9}\b",
    "driver_license": r"\b\d{2}[\s-]?\d{2}[\s-]?\d{2}[\s-]?\d{5}[\s-]?\d{2}\b",
    "postal_code": r"\b\d{5}\b",
    "iban": r"\bFR\d{2}\s?(\d{4}\s?){4}\d{2}\b",
}

# R√®gles de r√©tention des donn√©es en secondes, conform√©ment au RGPD.
# Une valeur de 0 signifie que la donn√©e ne doit jamais √™tre stock√©e.
RETENTION_RULES = {
    "email": 30 * 24 * 3600,          # 30 jours
    "phone": 7 * 24 * 3600,           # 7 jours
    "credit_card": 0,                 # Ne jamais stocker
    "social_security": 0,             # Ne jamais stocker
    "passport": 0,                    # Ne jamais stocker
    "driver_license": 0,              # Ne jamais stocker
    "postal_code": 365 * 24 * 3600,   # 1 an
    "iban": 90 * 24 * 3600,           # 90 jours
}


# ------------------------------------------------------------------
# API Publique
# ------------------------------------------------------------------
class PrivacyPolicy:
    """Classe principale pour la gestion de la politique de confidentialit√©."""

    def __init__(self, config_path: Optional[Path] = None):
        """Initialise la politique, en chargeant une configuration personnalis√©e si fournie."""
        self.config = self._load_config(config_path)
        self.consent_db = ConsentDB(config_path)

    # ------------------------------------------------------------------
    # D√©tection et masquage de PII
    # ------------------------------------------------------------------
    def scan_and_mask(self, text: str, *, mask_char: str = "*") -> PrivacyReport:
        """Analyse un texte, masque les PII et retourne un rapport d√©taill√©."""
        pii_list = []
        for pii_type, pattern in PII_PATTERNS.items():
            for match in re.finditer(pattern, text, re.IGNORECASE):
                original = match.group(0)
                redacted = self._mask(original, mask_char)
                pii_list.append(
                    PIIDetection(
                        type=pii_type,
                        value=original,
                        redacted=redacted,
                        start=match.start(),
                        end=match.end(),
                    )
                )

        # Construit le texte masqu√© en rempla√ßant les PII d√©tect√©es.
        masked_text = text
        for det in sorted(pii_list, key=lambda d: d.start, reverse=True):
            masked_text = (
                masked_text[: det.start] + det.redacted + masked_text[det.end :]
            )

        # D√©termine la dur√©e de r√©tention maximale et si le consentement est requis.
        max_retention = max(
            (RETENTION_RULES.get(p.type, 0) for p in pii_list), default=0
        )
        can_store = max_retention > 0
        user_consent_required = any(p.type in {"email", "phone"} for p in pii_list)

        return PrivacyReport(
            text=masked_text,
            pii_list=pii_list,
            retention_seconds=max_retention,
            can_store=can_store,
            user_consent_required=user_consent_required,
        )

    # ------------------------------------------------------------------
    # Gestion du consentement
    # ------------------------------------------------------------------
    def record_consent(
        self, user_id: str, pii_types: List[str], granted: bool, expiry_days: int = 365
    ):
        """Enregistre le choix de consentement d'un utilisateur."""
        self.consent_db.add(
            user_id=user_id,
            pii_types=pii_types,
            granted=granted,
            expires_at=datetime.utcnow() + timedelta(days=expiry_days),
        )

    def has_consent(self, user_id: str, pii_type: str) -> bool:
        """V√©rifie si un utilisateur a un consentement valide pour un type de PII."""
        return self.consent_db.is_valid(user_id, pii_type)

    # ------------------------------------------------------------------
    # Piste d'audit
    # ------------------------------------------------------------------
    def log_access(self, user_id: str, pii_type: str, action: str):
        """Journalise un acc√®s √† une PII pour la piste d'audit RGPD."""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": user_id,
            "pii_type": pii_type,
            "action": action, # ex: 'view', 'export'
        }
        self._append_audit_log(log_entry)

    # ------------------------------------------------------------------
    # Utilitaires
    # ------------------------------------------------------------------
    def _mask(self, value: str, mask_char: str) -> str:
        """Masque une valeur en conservant les 2 premiers et 2 derniers caract√®res."""
        if len(value) <= 4:
            return mask_char * len(value)
        return value[:2] + mask_char * (len(value) - 4) + value[-2:]

    def _load_config(self, path: Optional[Path]) -> Dict:
        """Charge une configuration de r√©tention personnalis√©e si elle existe."""
        if path and path.exists():
            return json.loads(path.read_text())
        return RETENTION_RULES

    def _append_audit_log(self, entry: Dict):
        """Ajoute une entr√©e au fichier d'audit (format JSON Lines)."""
        try:
            audit_file = Path("logs/privacy_audit.jsonl")
            audit_file.parent.mkdir(exist_ok=True)
            with audit_file.open("a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture dans le journal d'audit : {e}")


# ------------------------------------------------------------------
# Persistance du consentement (simple fichier JSONL)
# ------------------------------------------------------------------
class ConsentDB:
    """Une base de donn√©es simple, bas√©e sur un fichier, pour stocker le consentement."""
    def __init__(self, config_path: Optional[Path]):
        self.file = Path(config_path or "data/consent.jsonl")

    def add(
        self, user_id: str, pii_types: List[str], granted: bool, expires_at: datetime
    ):
        """Ajoute un nouvel enregistrement de consentement."""
        try:
            self.file.parent.mkdir(parents=True, exist_ok=True)
            record = {
                "user_id": user_id,
                "pii_types": pii_types,
                "granted": granted,
                "expires_at": expires_at.isoformat(),
                "created_at": datetime.utcnow().isoformat(),
            }
            with self.file.open("a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture dans la base de donn√©es de consentement : {e}")

    def is_valid(self, user_id: str, pii_type: str) -> bool:
        """V√©rifie si le consentement le plus r√©cent pour un utilisateur et un type de PII est valide."""
        now = datetime.utcnow()
        try:
            with self.file.open("r", encoding="utf-8") as f:
                # Lit le fichier en sens inverse pour trouver le consentement le plus r√©cent en premier.
                for line in reversed(list(f)):
                    record = json.loads(line)
                    if (
                        record["user_id"] == user_id
                        and pii_type in record["pii_types"]
                    ):
                        # Si le consentement est trouv√©, v√©rifie s'il a expir√©.
                        if datetime.fromisoformat(record["expires_at"]) < now:
                            return False
                        # Retourne l'√©tat (accord√© ou non).
                        return record["granted"]
        except FileNotFoundError:
            # Si le fichier n'existe pas, aucun consentement n'a √©t√© donn√©.
            pass
        return False


# ------------------------------------------------------------------
# D√©monstration en ligne de commande
# ------------------------------------------------------------------
if __name__ == "__main__":
    policy = PrivacyPolicy()

    sample_text = (
        "Contactez-moi √† jean.dupont@mail.fr ou au 06.12.34.56.78, "
        "ma carte est 4532-1234-5678-9012."
    )
    report = policy.scan_and_mask(sample_text)
    print(json.dumps(asdict(report), ensure_ascii=False, indent=2))

```

---

## Fichier : `backend\altiora\core\policies\toxicity_policy.py`

```python
# backend/altiora/core/policies/toxicity_policy.py
"""Politique de d√©tection de toxicit√© et de PII pour Altiora.

Ce module combine une analyse rapide bas√©e sur des expressions r√©guli√®res locales
(avec un lexique fran√ßais) et des appels optionnels √† des API externes pour une
analyse plus approfondie. Il fournit un score de s√©v√©rit√© et masque les PII.
"""

import re
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum

# Importation de PrivacyPolicy pour la d√©tection de PII
from .privacy_policy import PrivacyPolicy, PrivacyReport

try:
    import httpx
except ImportError:
    httpx = None

logger = logging.getLogger(__name__)


class Severity(Enum):
    """Niveaux de s√©v√©rit√© pour le contenu d√©tect√©."""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class DetectionResult:
    """R√©sultat d'une analyse de toxicit√©."""
    toxic: bool
    severity: Severity
    categories: List[str]
    pii_found: List[str]
    confidence: float
    provider: str


# ------------------------------------------------------------------
# Lexique et expressions r√©guli√®res (Fran√ßais)
# ------------------------------------------------------------------
TOXIC_REGEXES = {
    "hate": [
        r"\b(nazi|facho|raciste|supr√©maciste)\b",
        r"\b(tuer\s+(tous?|les?)|pendre\s+les?|gazer\s+les?)\b",
    ],
    "harassment": [
        r"\b(naze|con|idiot|imb√©cile|d√©bile|pd|tapette)\b",
        r"\b(ferme\s+ta\s+gueule|d√©gage|va\s+te\s+faire)\b",
    ],
    "sexual": [
        r"\b(porno?|xxx|nud?e?|viol|agression\s+sexuelle)\b",
    ],
    "violence": [
        r"\b(bombe|tuer|tue|tirer|poignarder|massacrer)\b",
    ],
}

PII_REGEXES = {
    "email": r"[\w\.-]+@[\w\.-]+\.\w+",
    "phone": r"(\+?33[-.\s]?|0)[1-9]([-.\s]?\d{2}){4}",
    "credit_card": r"\b(?:\d{4}[\s-]?){3}\d{4}\b",
    "social_security": r"\b\d{3}[\s-]?\d{2}[\s-]?\d{3}[\s-]?\d{3}\b",
}


class ToxicityPolicy:
    """Analyse le texte pour la toxicit√© et les PII."""

    def __init__(
        self,
        *,
        use_external: bool = False,
        openai_key: Optional[str] = None,
        azure_endpoint: Optional[str] = None,
    ):
        """Initialise la politique.

        Args:
            use_external: Si True, utilise les API externes (OpenAI, Azure) comme fallback.
            openai_key: Cl√© API pour OpenAI Moderation.
            azure_endpoint: Endpoint pour Azure Content Safety.
        """
        self.use_external = use_external and httpx is not None
        self.openai_key = openai_key
        self.azure_endpoint = azure_endpoint

    # ------------------------------------------------------------------
    # API Publique
    # ------------------------------------------------------------------
    async def scan(self, text: str) -> DetectionResult:
        """Analyse un texte (fran√ßais) pour la toxicit√© et les PII.

        La strat√©gie est d'abord locale (rapide) puis externe (plus lente mais potentiellement
        plus pr√©cise). Si une toxicit√© √©lev√©e est d√©tect√©e localement, le r√©sultat est
        retourn√© imm√©diatement.
        """
        text_lower = text.lower()
        regex_result = self._regex_scan(text_lower)

        # Si la s√©v√©rit√© est d√©j√† haute, on retourne le r√©sultat imm√©diatement.
        if regex_result.severity in (Severity.HIGH, Severity.CRITICAL):
            return regex_result

        # Si l'option est activ√©e, on utilise une API externe comme fallback.
        if self.use_external:
            external_result = await self._external_scan(text_lower)
            # On retourne le r√©sultat externe seulement s'il est plus s√©v√®re.
            if external_result.severity.value > regex_result.severity.value:
                return external_result

        return regex_result

    # ------------------------------------------------------------------
    # Impl√©mentation par expressions r√©guli√®res
    # ------------------------------------------------------------------
    def _regex_scan(self, text: str) -> DetectionResult:
        """Analyse le texte en utilisant les expressions r√©guli√®res locales."""
        toxic = False
        categories: List[str] = []
        max_sev = Severity.LOW
        pii_tokens: List[str] = []

        # D√©tection de la toxicit√©
        for cat, patterns in TOXIC_REGEXES.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    toxic = True
                    if cat not in categories:
                        categories.append(cat)
                    max_sev = max(max_sev, self._severity_from_cat(cat))

        # D√©tection des PII
        for pii_type, pattern in PII_REGEXES.items():
            for match in re.finditer(pattern, text):
                pii_tokens.append(match.group(0))

        return DetectionResult(
            toxic=toxic,
            severity=max_sev,
            categories=categories,
            pii_found=pii_tokens,
            confidence=0.9,  # Confiance √©lev√©e pour les regex car d√©terministes
            provider="regex",
        )

    # ------------------------------------------------------------------
    # Assistants pour les API externes
    # ------------------------------------------------------------------
    async def _external_scan(self, text: str) -> DetectionResult:
        """Tente d'analyser le texte avec une API externe."""
        if self.openai_key:
            return await self._openai_moderation(text)
        if self.azure_endpoint:
            return await self._azure_content_safety(text)
        return self._fallback_result()

    async def _openai_moderation(self, text: str) -> DetectionResult:
        """Analyse le texte avec l'API de mod√©ration d'OpenAI."""
        url = "https://api.openai.com/v1/moderations"
        headers = {"Authorization": f"Bearer {self.openai_key}"}
        payload = {"input": text}

        async with httpx.AsyncClient(timeout=10) as client:
            try:
                resp = await client.post(url, json=payload, headers=headers)
                resp.raise_for_status() # L√®ve une exception pour les codes 4xx/5xx
            except httpx.HTTPStatusError as e:
                logger.error(f"Erreur de l'API de mod√©ration OpenAI : {e.response.text}")
                return self._fallback_result()

        data = resp.json()
        result = data["results"][0]
        categories = result["categories"]
        scores = result["category_scores"]

        toxic = any(categories.values())
        max_sev = max(
            (self._severity_from_openai_cat(name) for name, flag in categories.items() if flag),
            default=Severity.LOW,
        )
        return DetectionResult(
            toxic=toxic,
            severity=max_sev,
            categories=[k for k, v in categories.items() if v],
            pii_found=[],
            confidence=max(scores.values()),
            provider="openai",
        )

    async def _azure_content_safety(self, text: str) -> DetectionResult:
        """Analyse le texte avec l'API Azure Content Safety."""
        url = f"{self.azure_endpoint}/contentsafety/text:analyze?api-version=2023-10-01"
        headers = {"Content-Type": "application/json"}
        payload = {"text": text, "categories": ["Hate", "Sexual", "Violence", "SelfHarm"]}

        async with httpx.AsyncClient(timeout=10) as client:
            try:
                resp = await client.post(url, json=payload, headers=headers)
                resp.raise_for_status()
            except httpx.HTTPStatusError as e:
                logger.error(f"Erreur de l'API Azure Content Safety : {e.response.text}")
                return self._fallback_result()

        data = resp.json()
        toxic = any(block["severity"] > 0 for block in data.get("categoriesAnalysis", []))
        max_sev = max(
            (Severity(block["severity"]) for block in data["categoriesAnalysis"] if block["severity"] > 0),
            default=Severity.LOW,
        )
        return DetectionResult(
            toxic=toxic,
            severity=max_sev,
            categories=[b["category"] for b in data["categoriesAnalysis"] if b["severity"] > 0],
            pii_found=[],
            confidence=0.8, # Confiance arbitraire pour Azure
            provider="azure",
        )

    # ------------------------------------------------------------------
    # Utilitaires
    # ------------------------------------------------------------------
    def _severity_from_cat(self, category: str) -> Severity:
        """Mappe une cat√©gorie de regex √† un niveau de s√©v√©rit√©."""
        mapping = {
            "hate": Severity.HIGH,
            "harassment": Severity.MEDIUM,
            "sexual": Severity.MEDIUM,
            "violence": Severity.HIGH,
        }
        return mapping.get(category, Severity.LOW)

    def _severity_from_openai_cat(self, category: str) -> Severity:
        """Mappe une cat√©gorie de l'API OpenAI √† un niveau de s√©v√©rit√©."""
        return {
            "hate": Severity.HIGH,
            "hate/threatening": Severity.CRITICAL,
            "harassment": Severity.MEDIUM,
            "harassment/threatening": Severity.HIGH,
            "sexual": Severity.MEDIUM,
            "sexual/minors": Severity.CRITICAL,
            "violence": Severity.HIGH,
            "violence/graphic": Severity.CRITICAL,
            "self-harm": Severity.CRITICAL,
        }.get(category, Severity.LOW)

    def _fallback_result(self) -> DetectionResult:
        """Retourne un r√©sultat non toxique en cas d'√©chec des API externes."""
        return DetectionResult(
            toxic=False,
            severity=Severity.LOW,
            categories=[],
            pii_found=[],
            confidence=0.0,
            provider="none",
        )


# ------------------------------------------------------------------
# D√©monstration en ligne de commande
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio

    async def demo():
        policy = ToxicityPolicy(use_external=False)
        samples = [
            "Bonjour, comment vas-tu ?",
            "T‚Äôes vraiment un gros d√©bile, ferme-la !",
            "Mon email est pierre.dupont@mail.fr et ma carte 1234-5678-9012-3456",
        ]
        for s in samples:
            res = await policy.scan(s)
            logging.info(f"{s} ‚Üí {res}")

    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\core\policies\__init__.py`

```python
# backend/altiora/core/policies/__init__.py
"""
Policies and business rules.
"""
```

---

## Fichier : `backend\altiora\core\post_processing\code_validator.py`

```python
# backend/altiora/core/post_processing/code_validator.py
"""Module pour la validation compl√®te du code Python et Playwright g√©n√©r√©.

Ce module fournit une classe `CodeValidator` qui effectue plusieurs v√©rifications
sur une cha√Æne de code source pour garantir sa qualit√©, sa syntaxe et sa conformit√©
aux standards du projet avant son utilisation ou son stockage.

Fonctionnalit√©s principales :
1.  V√©rification de la syntaxe Python √† l'aide du module `ast`.
2.  Linting du code √† l'aide de `ruff` pour d√©tecter les erreurs et les mauvaises pratiques.
3.  V√©rification du formatage du code avec `black` pour assurer un style coh√©rent.
4.  V√©rifications sp√©cifiques √† Playwright (imports, usage de la fixture `page`, etc.).

Le validateur est con√ßu pour √™tre utilis√© de mani√®re asynchrone et retourne un
objet Pydantic `ValidationResult` qui d√©taille toutes les erreurs trouv√©es.
"""

import ast
import asyncio
import logging
import re
import tempfile
from pathlib import Path
from typing import List, Optional, Tuple

from pydantic import BaseModel, Field

# Configuration du logger
logger = logging.getLogger(__name__)


class ValidationResult(BaseModel):
    """Repr√©sente le r√©sultat d√©taill√© de la validation du code."""
    is_valid: bool = Field(True, description="Le code est-il syntaxiquement valide ?")
    syntax_error: Optional[str] = Field(None, description="Message d'erreur de syntaxe, le cas √©ch√©ant.")
    linting_errors: List[str] = Field(default_factory=list, description="Liste des erreurs de linting (ruff).")
    formatting_errors: List[str] = Field(default_factory=list, description="Liste des erreurs de formatage (black).")
    playwright_warnings: List[str] = Field(default_factory=list, description="Avertissements sp√©cifiques √† Playwright.")

    @property
    def passed(self) -> bool:
        """Propri√©t√© indiquant si le code a pass√© toutes les v√©rifications avec succ√®s."""
        return self.is_valid and not self.linting_errors and not self.formatting_errors and not self.playwright_warnings


class CodeValidator:
    """Valide une cha√Æne de code Python ou Playwright en utilisant des outils externes."""

    def __init__(self, ruff_config_path: Optional[str] = None):
        """Initialise le validateur.

        Args:
            ruff_config_path: Chemin optionnel vers un fichier de configuration `ruff.toml`.
        """
        self.ruff_config_path = ruff_config_path

    @staticmethod
    async def _run_subprocess(command: str, *args: str) -> Tuple[int, str, str]:
        """Ex√©cute une commande en sous-processus de mani√®re asynchrone."""
        try:
            process = await asyncio.create_subprocess_exec(
                command, *args,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            return process.returncode, stdout.decode('utf-8'), stderr.decode('utf-8')
        except FileNotFoundError:
            logger.error(f"La commande `{command}` n'a pas √©t√© trouv√©e. Assurez-vous qu'elle est install√©e et dans le PATH.")
            return -1, "", f"Commande non trouv√©e: {command}"
        except Exception as e:
            logger.error(f"Erreur lors de l'ex√©cution de la commande `{command}` : {e}")
            return -1, "", str(e)

    @staticmethod
    def _validate_playwright_specifics(code_string: str) -> List[str]:
        """V√©rifie les meilleures pratiques et conventions sp√©cifiques √† Playwright."""
        warnings = []

        # 1. V√©rifier l'importation de Playwright.
        if not re.search(r"from\s+playwright\.(sync|async)_api\s+import", code_string):
            warnings.append("Avertissement : Import Playwright manquant (ex: from playwright.sync_api import Page, expect).")

        # 2. V√©rifier l'utilisation de la fixture `page: Page` dans la signature du test.
        if not re.search(r"def\s+test_\w+\(.*\bpage:\s*Page\b.*\):", code_string):
            warnings.append("Avertissement : La fonction de test doit utiliser la fixture `page: Page` (ex: def test_example(page: Page):).")

        # 3. V√©rifier la pr√©sence d'au moins une action Playwright.
        action_pattern = r"\bpage\.(goto|click|fill|press|select_option|check|uncheck|set_input_files|hover|focus|dispatch_event|drag_and_drop|tap|type)\("
        if not re.search(action_pattern, code_string):
            warnings.append("Avertissement : Aucune action Playwright d√©tect√©e (ex: page.goto(), page.click()). Le test pourrait ne rien faire.")

        # 4. V√©rifier la pr√©sence d'au moins une assertion `expect`.
        if "expect(" not in code_string:
            warnings.append("Avertissement : Aucune assertion Playwright `expect()` d√©tect√©e. Un test doit valider un r√©sultat.")

        return warnings

    async def validate(self, code_string: str, code_type: str = "python") -> ValidationResult:
        """Effectue toutes les validations sur la cha√Æne de code fournie.

        Args:
            code_string: La cha√Æne de code √† valider.
            code_type: Le type de code ('python' ou 'playwright') pour appliquer des r√®gles sp√©cifiques.

        Returns:
            Un objet `ValidationResult` contenant les r√©sultats de la validation.
        """
        # 1. V√©rification de la syntaxe Python (rapide et en premier).
        try:
            ast.parse(code_string)
        except SyntaxError as e:
            return ValidationResult(
                is_valid=False,
                syntax_error=f"Erreur de syntaxe √† la ligne {e.lineno}, offset {e.offset}: {e.msg}"
            )

        result = ValidationResult()

        # 2. V√©rifications sp√©cifiques √† Playwright si n√©cessaire.
        if code_type == "playwright":
            result.playwright_warnings = self._validate_playwright_specifics(code_string)

        # Cr√©e un fichier temporaire pour passer le code aux outils CLI (ruff, black).
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.py', delete=False, encoding='utf-8') as temp_file:
            temp_file_path = Path(temp_file.name)
            temp_file.write(code_string)
            temp_file.flush()

        try:
            # 3. Linting avec Ruff.
            ruff_args = ['check', str(temp_file_path), '--quiet', '--exit-zero']
            if self.ruff_config_path:
                ruff_args.extend(['--config', self.ruff_config_path])

            _, ruff_stdout, ruff_stderr = await self._run_subprocess('ruff', *ruff_args)
            if ruff_stdout:
                result.linting_errors = [line for line in ruff_stdout.strip().split('\n') if line]
            if ruff_stderr:
                 result.linting_errors.append(ruff_stderr)

            # 4. V√©rification du formatage avec Black.
            _, _, black_stderr = await self._run_subprocess('black', '--check', '--quiet', str(temp_file_path))
            if black_stderr:
                result.formatting_errors = [line for line in black_stderr.strip().split('\n') if line]

        finally:
            # S'assure que le fichier temporaire est bien supprim√©.
            temp_file_path.unlink()

        return result


async def main():
    """Fonction principale pour une d√©monstration et un test rapide du validateur."""
    validator = CodeValidator()

    # ... (le reste de la fonction main reste inchang√©)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger.info("Ex√©cution des tests du CodeValidator...")
    asyncio.run(main())
    logger.info("\nTests termin√©s.")
```

---

## Fichier : `backend\altiora\core\post_processing\excel_formatter.py`

```python
# backend/altiora/core/post_processing/excel_formatter.py
"""Module pour formater et styliser des donn√©es dans des fichiers Excel.

Ce module utilise pandas et openpyxl pour cr√©er des fichiers Excel esth√©tiques
et lisibles √† partir de donn√©es brutes, en appliquant des styles, des couleurs
conditionnelles et en ajustant la largeur des colonnes.
"""

import re
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

# Importation des outils de style d'openpyxl
from openpyxl.styles import PatternFill, Font, Alignment


class ExcelFormatter:
    """Formate et exporte des donn√©es structur√©es vers des fichiers Excel stylis√©s."""

    # Expression r√©guli√®re pour valider les IDs de cas de test.
    TEST_CASE_ID_PATTERN = re.compile(r"^CU\d{2}_SB\d{2}_C[PEL]\d{3}_.+(?<!_)$")

    # D√©finition des styles pour une apparence professionnelle.
    HEADER_FILL = PatternFill(start_color="4F81BD", end_color="4F81BD", fill_type="solid")
    HEADER_FONT = Font(color="FFFFFF", bold=True)

    # Couleurs pour le formatage conditionnel bas√© sur le type de test.
    CP_FILL = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Vert pour Cas Passant
    CE_FILL = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")  # Rouge pour Cas d'Erreur
    CL_FILL = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")  # Jaune pour Cas Limite

    def _validate_test_case_id(self, test_id: str) -> bool:
        """Valide le format de l'identifiant du cas de test."""
        if not isinstance(test_id, str):
            return False
        return bool(self.TEST_CASE_ID_PATTERN.match(test_id))

    def format_test_matrix(
        self, test_cases: List[Dict[str, Any]], output_path: str
    ) -> List[str]:
        """Cr√©e et formate un fichier Excel pour une matrice de tests.

        Args:
            test_cases: Une liste de dictionnaires, o√π chaque dictionnaire repr√©sente un cas de test.
                        Chaque dictionnaire doit contenir au moins les cl√©s 'id' et 'type'.
            output_path: Le chemin du fichier Excel √† cr√©er (ex: 'reports/matrice_tests.xlsx').

        Returns:
            Une liste des erreurs de validation des IDs de cas de test rencontr√©es.
        """
        errors = []
        for i, case in enumerate(test_cases):
            if not self._validate_test_case_id(case.get("id")):
                errors.append(f"Ligne {i+2}: L'ID du cas de test '{case.get('id')}' ne respecte pas le format requis.")

        # Cr√©e un DataFrame pandas, qui est une structure de donn√©es tabulaire efficace.
        df = pd.DataFrame(test_cases)

        # S'assure que le r√©pertoire de sortie existe avant d'√©crire le fichier.
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Matrice de Tests")
            worksheet = writer.sheets["Matrice de Tests"]

            # Applique les styles pour am√©liorer la lisibilit√©.
            self._apply_styles(worksheet)

        return errors

    def _apply_styles(self, worksheet):
        """Applique le formatage conditionnel et les styles √† la feuille de calcul."""
        # Style des en-t√™tes
        for cell in worksheet[1]:
            cell.fill = self.HEADER_FILL
            cell.font = self.HEADER_FONT
            cell.alignment = Alignment(horizontal="center", vertical="center")

        # Trouve l'index de la colonne 'type' pour le formatage conditionnel.
        try:
            type_col_idx = [cell.value for cell in worksheet[1]].index('type') + 1
        except ValueError:
            type_col_idx = -1

        # Applique la couleur de fond aux lignes en fonction de la valeur de la colonne 'type'.
        if type_col_idx != -1:
            for row in worksheet.iter_rows(min_row=2, max_row=worksheet.max_row):
                cell_type = row[type_col_idx - 1].value
                fill_style = None
                if cell_type == "CP":
                    fill_style = self.CP_FILL
                elif cell_type == "CE":
                    fill_style = self.CE_FILL
                elif cell_type == "CL":
                    fill_style = self.CL_FILL
                
                if fill_style:
                    for cell in row:
                        cell.fill = fill_style

        # Ajuste automatiquement la largeur des colonnes pour s'adapter au contenu.
        for col in worksheet.columns:
            max_length = 0
            column = col[0].column_letter
            for cell in col:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except: # noqa: E722
                    pass
            adjusted_width = (max_length + 2) * 1.2
            worksheet.column_dimensions[column].width = adjusted_width


# --- D√©monstration --- #
async def main():
    """Fonction de d√©monstration pour g√©n√©rer un fichier Excel format√©."""
    formatter = ExcelFormatter()
    test_data = [
        {
            "id": "CU01_SB01_CP001_connexion_valide",
            "description": "V√©rifier la connexion avec un utilisateur et un mot de passe valides.",
            "type": "CP"
        },
        {
            "id": "CU01_SB01_CE001_mot_de_passe_incorrect",
            "description": "V√©rifier le message d'erreur avec un mot de passe incorrect.",
            "type": "CE"
        },
        {
            "id": "CU01_SB02_CL001_champ_email_vide",
            "description": "V√©rifier la r√©action du syst√®me quand le champ email est laiss√© vide.",
            "type": "CL"
        },
        {
            "id": "ID_INVALIDE",
            "description": "Ce cas a un ID incorrect et devrait √™tre signal√©.",
            "type": "CP"
        }
    ]

    output_file = "reports/matrice_de_test_formatee.xlsx"
    print(f"G√©n√©ration du fichier Excel de d√©monstration : {output_file}")

    validation_errors = formatter.format_test_matrix(test_data, output_file)

    if validation_errors:
        print("\nErreurs de validation d√©tect√©es :")
        for error in validation_errors:
            print(f"- {error}")
    else:
        print("\nAucune erreur de validation.")

    print("\nFichier Excel g√©n√©r√© avec succ√®s.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

```

---

## Fichier : `backend\altiora\core\post_processing\output_sanitizer.py`

```python
# backend/altiora/core/post_processing/output_sanitizer.py
"""Module pour nettoyer et assainir les sorties brutes des mod√®les de langage (LLM).

Ce module est essentiel pour normaliser les r√©ponses des LLM avant qu'elles ne soient
utilis√©es par d'autres parties du syst√®me. Il effectue plusieurs op√©rations cl√©s :
- Supprime les blocs de code Markdown (ex: ```python ... ```).
- Masque les informations personnelles identifiables (PII) en utilisant `PrivacyPolicy`.
- Supprime les instructions de d√©bogage (comme `print()` et `logging.*`).
"""

import re

from policies.privacy_policy import PrivacyPolicy

# ------------------------------------------------------------------
# Constantes et Expressions R√©guli√®res
# ------------------------------------------------------------------

# Trouve et extrait le contenu des blocs de code Markdown.
CODE_BLOCK_RE = re.compile(r"^```(?:python)?\s*\n(.*?)\n```\s*$", re.DOTALL)

# Supprime les phrases d'introduction courantes g√©n√©r√©es par les LLM.
INTRO_RE = re.compile(r"^(?i)(voici|bien s√ªr, voici) le code.*?\n", re.MULTILINE)

# Supprime les appels √† `print()`.
PRINT_RE = re.compile(r"^\s*print\(.*\)\s*$", re.MULTILINE)

# Supprime les appels de logging de bas niveau.
LOG_RE = re.compile(r"^\s*logging\.(?:info|debug|warning)\(.*\)\s*$", re.MULTILINE)


class OutputSanitizer:
    """Nettoyeur rapide et sans configuration pour les sorties de texte et de code."""

    def __init__(self) -> None:
        """Initialise le nettoyeur avec une instance de PrivacyPolicy."""
        self.privacy = PrivacyPolicy()

    def sanitize(
            self,
            text: str,
            *,
            remove_debug: bool = True,
    ) -> str:
        """Nettoie et assainit une cha√Æne de caract√®res brute.

        Args:
            text: La cha√Æne de caract√®res brute √† nettoyer.
            remove_debug: Si True, supprime les instructions de d√©bogage (print, logging).

        Returns:
            La cha√Æne de caract√®res nettoy√©e et avec les PII masqu√©es.
        """
        # 1. Supprime les wrappers Markdown et les introductions.
        text = CODE_BLOCK_RE.sub(r"\1", text.strip())
        text = INTRO_RE.sub("", text)

        # 2. Supprime les instructions de d√©bogage si l'option est activ√©e.
        if remove_debug:
            text = PRINT_RE.sub("", text)
            text = LOG_RE.sub("", text)

        # 3. Masque les PII en utilisant la politique de confidentialit√©.
        privacy_report = self.privacy.scan_and_mask(text)
        
        # Retourne le texte masqu√© et nettoy√© des espaces superflus.
        return privacy_report.text.strip()


# ------------------------------------------------------------------
# Auto-test rapide
# ------------------------------------------------------------------
if __name__ == "__main__":
    sanitizer = OutputSanitizer()

    raw_text = '''
Bien s√ªr, voici le code que vous avez demand√© :
```python
import os

# Ceci est un commentaire de test
print("D√©but du script")
logging.info(f"Email de contact : test@example.com")
# Fin du script
```
    '''
    cleaned_text = sanitizer.sanitize(raw_text, remove_debug=True)
    
    print("--- Texte Original ---")
    print(raw_text)
    print("\n--- Texte Nettoy√© ---")
    print(cleaned_text)

    # V√©rifications pour le test
    assert "```" not in cleaned_text
    assert "Bien s√ªr" not in cleaned_text
    assert "print(" not in cleaned_text
    assert "logging.info" not in cleaned_text
    assert "test@****.com" in cleaned_text
    assert "# Ceci est un commentaire de test" in cleaned_text
    print("\n‚úÖ Les tests de nettoyage ont r√©ussi.")
```

---

## Fichier : `backend\altiora\core\post_processing\__init__.py`

```python
# backend/altiora/core/post_processing/__init__.py
"""
Post-processing modules for generated outputs.
"""
```

---

## Fichier : `backend\altiora\core\qa\multimodal_qa.py`

```python
# backend/altiora/core/qa/multimodal_qa.py
from src.models.qwen_model import QwenModel
from src.models.starcoder_model import StarcoderModel
from src.utils.question_router import QuestionRouter


class MultiModalQASystem:
    def __init__(self):
        self.text_model = QwenModel()
        self.code_model = StarcoderModel()
        self.router = QuestionRouter()

    def answer_question(self, question, context=None):
        """Route vers le bon mod√®le selon le type de question"""
        question_type = self.router.classify(question)

        if question_type == "code_generation":
            return self.code_model.generate(question, context)
        elif question_type == "code_explanation":
            # Utiliser les deux mod√®les
            code = self.code_model.extract_code(context)
            explanation = self.text_model.explain(code)
            return explanation
        else:
            return self.text_model.answer(question, context)
```

---

## Fichier : `backend\altiora\core\qa\qa_system.py`

```python
# backend/altiora/core/qa/qa_system.py
"""Module impl√©mentant le syst√®me de Question-R√©ponse (QA) pour l'application Altiora.

Ce module fournit une interface pour interagir avec les mod√®les de langage
afin de r√©pondre aux questions des utilisateurs. Il est con√ßu pour √™tre
asynchrone et peut √™tre int√©gr√© dans une API FastAPI.
"""

import asyncio
import time
import logging
from typing import Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class QASystem:
    """Syst√®me de Question-R√©ponse (QA) pour interagir avec les mod√®les de langage."

    Cette classe est responsable de la logique d'appel aux mod√®les d'IA
    pour g√©n√©rer des r√©ponses aux questions pos√©es par l'utilisateur.
    """

    async def answer_async(self, question: str, context: Optional[str], model: str, temperature: float) -> Any:
        """R√©pond √† une question de mani√®re asynchrone en utilisant un mod√®le de langage."

        Args:
            question: La question pos√©e par l'utilisateur.
            context: Un contexte optionnel pour aider le mod√®le √† formuler la r√©ponse.
            model: Le nom du mod√®le de langage √† utiliser (ex: 'qwen', 'starcoder').
            temperature: La temp√©rature de g√©n√©ration pour contr√¥ler la cr√©ativit√© de la r√©ponse.

        Returns:
            Un objet factice (`Any`) contenant `text` (la r√©ponse) et `confidence`.
            Dans une impl√©mentation r√©elle, cela appellerait une interface de mod√®le LLM.
        """
        logger.info(f"R√©ception de la question : '{question}' pour le mod√®le '{model}' avec temp√©rature {temperature}.")
        # Simule un d√©lai de traitement pour l'inf√©rence du mod√®le.
        await asyncio.sleep(0.5)

        # Logique factice pour la r√©ponse.
        mock_answer = f"Ceci est une r√©ponse simul√©e √† votre question : '{question}'."
        mock_confidence = 0.85

        # Dans une application r√©elle, vous appelleriez ici votre interface de mod√®le LLM.
        # Exemple: `response = await self.llm_interface.generate_answer(question, context, model, temperature)`

        # Retourne un objet dynamique pour simuler la r√©ponse du mod√®le.
        return type('obj', (object,), {'text': mock_answer, 'confidence': mock_confidence})()


# Initialisation du syst√®me de QA.
qa_system = QASystem()

# Initialisation de l'application FastAPI.
app = FastAPI(title="Altiora QA API", description="API pour le syst√®me de Question-R√©ponse d'Altiora.")


# --- Mod√®les Pydantic pour les requ√™tes et r√©ponses --- #
class QARequest(BaseModel):
    """Mod√®le de requ√™te pour le syst√®me de Question-R√©ponse (QA)."""
    question: str = Field(..., description="La question pos√©e par l'utilisateur.")
    context: Optional[str] = Field(None, description="Contexte optionnel pour aider √† r√©pondre √† la question.")
    model: str = Field("qwen", description="Le mod√®le d'IA √† utiliser pour la r√©ponse (ex: 'qwen', 'starcoder').")
    temperature: float = Field(0.7, ge=0.0, le=1.0, description="Temp√©rature pour la g√©n√©ration de la r√©ponse (contr√¥le la cr√©ativit√©).")


class QAResponse(BaseModel):
    """Mod√®le de r√©ponse du syst√®me de Question-R√©ponse (QA)."""
    answer: str = Field(..., description="La r√©ponse g√©n√©r√©e par le mod√®le.")
    confidence: float = Field(..., description="Le niveau de confiance de la r√©ponse (entre 0 et 1).")
    model_used: str = Field(..., description="Le nom du mod√®le d'IA utilis√© pour g√©n√©rer la r√©ponse.")
    processing_time: float = Field(..., description="Le temps de traitement de la requ√™te en secondes.")


# --- Points de terminaison (Endpoints) --- #
@app.post("/qa/answer", response_model=QAResponse)
async def answer_question(request: QARequest) -> QAResponse:
    """Point de terminaison principal pour poser une question au syst√®me QA."

    Args:
        request: L'objet `QARequest` contenant la question et les param√®tres.

    Returns:
        Un objet `QAResponse` avec la r√©ponse du mod√®le.

    Raises:
        HTTPException: En cas d'erreur interne du serveur lors du traitement de la question.
    """
    start_time = time.time()

    try:
        # Appelle la m√©thode asynchrone du syst√®me QA pour obtenir une r√©ponse.
        answer = await qa_system.answer_async(
            question=request.question,
            context=request.context,
            model=request.model,
            temperature=request.temperature
        )

        return QAResponse(
            answer=answer.text,
            confidence=answer.confidence,
            model_used=request.model,
            processing_time=time.time() - start_time
        )
    except Exception as e:
        logger.error(f"Erreur lors de la r√©ponse √† la question : {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur interne du service : {str(e)}")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def run_server():
        """Lance le serveur FastAPI pour la d√©monstration."""
        logger.info("Lancement du serveur QA API sur http://0.0.0.0:8000")
        config = uvicorn.Config(app, host="0.0.0.0", port=8000, log_level="info", reload=True)
        server = uvicorn.Server(config)
        await server.serve()

    async def run_client():
        """Simule des requ√™tes client vers l'API QA."""
        await asyncio.sleep(1) # Donne le temps au serveur de d√©marrer.
        import httpx

        print("\n--- Test de l'endpoint /qa/answer ---")
        qa_request = {
            "question": "Comment puis-je optimiser mon code Python ?",
            "context": "J'ai un script qui est lent.",
            "model": "qwen",
            "temperature": 0.5
        }

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post("http://localhost:8000/qa/answer", json=qa_request)
                response.raise_for_status()
                qa_response = response.json()
                print(f"R√©ponse du QA : {qa_response['answer']}")
                print(f"Confiance : {qa_response['confidence']:.2f}")
                print(f"Mod√®le utilis√© : {qa_response['model_used']}")
                print(f"Temps de traitement : {qa_response['processing_time']:.2f}s")
        except httpx.HTTPStatusError as e:
            print(f"Erreur HTTP : {e.response.status_code} - {e.response.text}")
        except Exception as e:
            print(f"Erreur lors de l'appel client : {e}")

    async def main_demo():
        # Lance le serveur et le client en parall√®le.
        server_task = asyncio.create_task(run_server())
        client_task = asyncio.create_task(run_client())

        await asyncio.gather(server_task, client_task)

    asyncio.run(main_demo())
```

---

## Fichier : `backend\altiora\core\training\advanced_trainer.py`

```python
# backend/altiora/core/training/advanced_trainer.py
"""Module pour l'entra√Ænement avanc√© (fine-tuning) des mod√®les de langage (LLMs).

Ce module fournit une classe `AltioraModelTrainer` pour le fine-tuning de
mod√®les comme Qwen3 et Starcoder2. Il utilise des techniques d'optimisation
de la m√©moire comme LoRA (Low-Rank Adaptation) et le gradient checkpointing,
ainsi que le mixed precision training pour am√©liorer l'efficacit√© de
l'entra√Ænement sur des ressources limit√©es (CPU ou GPU).
"""

import argparse
import logging

import torch
import wandb
from datasets import Dataset, load_dataset
from peft import LoraConfig, get_peft_model, TaskType
from transformers import (
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    AutoTokenizer # Ajout√© pour la tokenisation du dataset
)

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class AltioraModelTrainer:
    """Entra√Æneur avanc√© pour le fine-tuning de mod√®les de langage (LLMs).

    Prend en charge les optimisations pour l'entra√Ænement efficace sur CPU/GPU.
    """

    def __init__(self, model_name: str, task: str):
        """Initialise l'entra√Æneur de mod√®le.

        Args:
            model_name: Le nom du mod√®le pr√©-entra√Æn√© √† charger (ex: "Qwen/Qwen-3B").
            task: La t√¢che pour laquelle le mod√®le est entra√Æn√© (ex: "qa", "code_gen").
        """
        self.model_name = model_name
        self.task = task
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Entra√Æneur initialis√©. Utilisation du p√©riph√©rique : {self.device}")

        # Configuration LoRA (Low-Rank Adaptation) pour l'efficacit√© m√©moire.
        # LoRA permet d'entra√Æner un petit nombre de param√®tres suppl√©mentaires
        # au lieu de l'ensemble du mod√®le, r√©duisant ainsi les besoins en m√©moire et en calcul.
        self.lora_config = LoraConfig(
            r=16,  # Le rang des matrices LoRA. Une valeur plus √©lev√©e augmente la capacit√© d'apprentissage.
            lora_alpha=32, # Facteur de mise √† l'√©chelle pour les poids LoRA.
            target_modules=["q_proj", "v_proj"], # Modules du mod√®le o√π appliquer LoRA (typiquement les couches d'attention).
            lora_dropout=0.1, # Taux de dropout pour les couches LoRA.
            bias="none", # Ne pas entra√Æner les biais.
            task_type=TaskType.CAUSAL_LM # Type de t√¢che pour le mod√®le (mod√©lisation du langage causal).
        )

    def prepare_model(self):
        """Pr√©pare le mod√®le de base avec les optimisations LoRA et de m√©moire."

        Returns:
            Le mod√®le pr√©par√© pour l'entra√Ænement.
        """
        logger.info(f"Chargement du mod√®le de base : {self.model_name}...")
        # Charge le mod√®le de base pr√©-entra√Æn√©.
        # `load_in_8bit=True` active la quantification 8-bit pour r√©duire l'utilisation de la m√©moire.
        # `device_map="auto"` distribue automatiquement le mod√®le sur les p√©riph√©riques disponibles.
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            load_in_8bit=True,
            device_map="auto"
        )
        logger.info("Mod√®le de base charg√©.")

        # Applique l'adaptateur LoRA au mod√®le de base.
        model = get_peft_model(model, self.lora_config)
        # Affiche le nombre de param√®tres entra√Æn√©s (LoRA) par rapport au total.
        model.print_trainable_parameters()

        # Active le gradient checkpointing pour r√©duire l'utilisation de la m√©moire GPU
        # au prix d'une l√©g√®re augmentation du temps de calcul.
        model.gradient_checkpointing_enable()

        # Active le mixed precision training (entra√Ænement en pr√©cision mixte).
        # Utilise des float16 pour les calculs, r√©duisant la m√©moire et acc√©l√©rant sur certains GPU.
        model.half()

        return model

    def train(self, train_dataset: Dataset, eval_dataset: Dataset):
        """Lance le processus d'entra√Ænement du mod√®le avec monitoring Weights & Biases."

        Args:
            train_dataset: Le jeu de donn√©es d'entra√Ænement.
            eval_dataset: Le jeu de donn√©es de validation.
        """
        logger.info("D√©marrage de l'entra√Ænement...")
        # Initialise une session Weights & Biases pour le suivi de l'entra√Ænement.
        wandb.init(project="altiora", name=f"{self.task}_{self.model_name}")

        # Pr√©pare le mod√®le avant de le passer au Trainer.
        self.model = self.prepare_model()

        # Configure les arguments d'entra√Ænement.
        training_args = TrainingArguments(
            output_dir=f"./models/{self.task}", # R√©pertoire de sortie pour les checkpoints et le mod√®le final.
            num_train_epochs=3, # Nombre d'√©poques d'entra√Ænement.
            per_device_train_batch_size=4, # Taille du batch par p√©riph√©rique (GPU/CPU).
            gradient_accumulation_steps=4, # Accumule les gradients sur plusieurs √©tapes pour simuler un plus grand batch.
            warmup_steps=100, # Nombre d'√©tapes de warm-up pour le taux d'apprentissage.
            logging_steps=10, # Fr√©quence de logging des m√©triques.
            save_strategy="epoch", # Strat√©gie de sauvegarde du mod√®le (√† chaque √©poque).
            evaluation_strategy="epoch", # Strat√©gie d'√©valuation (√† chaque √©poque).
            fp16=True,  # Active le mixed precision training (float16).
            report_to="wandb", # Int√®gre le reporting √† Weights & Biases.
            load_best_model_at_end=True, # Charge le meilleur mod√®le (bas√© sur `metric_for_best_model`) √† la fin.
            metric_for_best_model="eval_loss", # M√©trique utilis√©e pour d√©terminer le meilleur mod√®le.
            greater_is_better=False # Pour eval_loss, une valeur plus petite est meilleure.
        )

        # Initialise le tokenizer.
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # Cr√©e l'instance du Trainer.
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            compute_metrics=self.compute_metrics,
            tokenizer=tokenizer # Le tokenizer est n√©cessaire pour le Trainer.
        )

        # Lance l'entra√Ænement.
        trainer.train()
        # Sauvegarde le mod√®le final.
        trainer.save_model()
        logger.info(f"Entra√Ænement termin√©. Mod√®le sauvegard√© dans : {training_args.output_dir}")

    @staticmethod
    def compute_metrics(eval_pred: tuple) -> Dict[str, float]:
        """Calcule les m√©triques d'√©valuation √† partir des pr√©dictions du mod√®le."

        Args:
            eval_pred: Un tuple contenant les logits et les labels r√©els.

        Returns:
            Un dictionnaire de m√©triques (ex: {"accuracy": 0.95}).
        """
        logits, labels = eval_pred
        # Pour les t√¢ches de mod√©lisation du langage, la pr√©cision peut √™tre calcul√©e
        # en comparant les jetons pr√©dits (argmax des logits) aux jetons r√©els.
        predictions = torch.argmax(torch.tensor(logits), dim=-1)
        # Assurez-vous que les labels et les pr√©dictions ont la m√™me forme et sont comparables.
        # Cette impl√©mentation est tr√®s basique et peut n√©cessiter une adaptation
        # en fonction de la t√¢che sp√©cifique et du format des donn√©es.
        accuracy = (predictions == torch.tensor(labels)).sum().item() / labels.size
        return {"accuracy": accuracy}


# ------------------------------------------------------------------
# Point d'entr√©e CLI pour l'entra√Ænement
# ------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Lance l'entra√Ænement d'un mod√®le de langage.")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen-3B", help="Nom du mod√®le pr√©-entra√Æn√© √† utiliser.")
    parser.add_argument("--task", type=str, default="qa", help="T√¢che pour laquelle le mod√®le est entra√Æn√© (ex: 'qa', 'code_gen').")
    parser.add_argument("--train_dataset", type=str, required=True, help="Chemin vers le dataset d'entra√Ænement (format JSON). Ex: 'data/training/train.jsonl'.")
    parser.add_argument("--eval_dataset", type=str, required=True, help="Chemin vers le dataset de validation (format JSON). Ex: 'data/training/eval.jsonl'.")
    args = parser.parse_args()

    logger.info(f"D√©marrage de l'entra√Ænement pour le mod√®le {args.model_name} sur la t√¢che {args.task}.")

    trainer = AltioraModelTrainer(model_name=args.model_name, task=args.task)
    
    # Charge les datasets. Assurez-vous que les fichiers sont au format JSON Lines.
    try:
        train_dataset = load_dataset("json", data_files=args.train_dataset, split="train")
        eval_dataset = load_dataset("json", data_files=args.eval_dataset, split="train") # Utilise 'train' pour la d√©mo si pas de split 'validation'.
    except Exception as e:
        logger.error(f"Erreur lors du chargement des datasets : {e}. Assurez-vous que les chemins sont corrects et les fichiers au format JSON Lines.")
        exit(1)

    # Lance l'entra√Ænement.
    trainer.train(train_dataset, eval_dataset)
    logger.info("Script d'entra√Ænement termin√©.")
```

---

## Fichier : `backend\altiora\core\training\auto_fine_tuner.py`

```python
# backend/altiora/core/training/auto_fine_tuner.py
import mlflow
import torch
from datasets import load_dataset
from torch.cuda.amp import autocast, GradScaler
from transformers import AutoModelForCausalLM


class AutoFineTuner:
    def __init__(self, base_model: str, output_dir: str):
        self.base_model = base_model
        self.output_dir = output_dir
        mlflow.set_tracking_uri("http://localhost:5000")
        self.model = self._load_model()

    def _load_model(self):
        """Charger le mod√®le de base"""
        model = AutoModelForCausalLM.from_pretrained(self.base_model)
        model.gradient_checkpointing_enable()  # Activation du gradient checkpointing
        model.half()  # Activation du mixed precision training
        return model

    @staticmethod
    def prepare_dataset(data_path: str):
        """Pr√©pare le dataset avec validation automatique"""
        dataset = load_dataset("json", data_files=data_path, split="train")
        train_test_split = dataset.train_test_split(test_size=0.2)
        train_val_split = train_test_split['train'].train_test_split(test_size=0.1)
        return {
            'train': train_val_split['train'],
            'val': train_val_split['test'],
            'test': train_test_split['test']
        }

    def train_with_tracking(self, dataset, hyperparams):
        """Entra√Ænement avec tracking MLflow"""
        scaler = GradScaler()
        with mlflow.start_run():
            mlflow.log_params(hyperparams)

            for epoch in range(hyperparams['epochs']):
                train_loss = self.train_epoch(dataset['train'], scaler)
                val_loss = self.validate(dataset['val'], scaler)

                mlflow.log_metrics({
                    'train_loss': train_loss,
                    'val_loss': val_loss
                }, step=epoch)

                if self.should_early_stop(val_loss):
                    break

            mlflow.pytorch.log_model(self.model, "model")

    def train_epoch(self, train_dataset, scaler):
        """Entra√Ænement pour une √©poque"""
        self.model.train()
        total_loss = 0
        for batch in train_dataset:
            with autocast():
                outputs = self.model(**batch)
                loss = outputs.loss
            scaler.scale(loss).backward()
            scaler.step(self.optimizer)
            scaler.update()
            total_loss += loss.item()
        return total_loss / len(train_dataset)

    def validate(self, val_dataset, scaler):
        """Validation du mod√®le"""
        self.model.eval()
        total_loss = 0
        with torch.no_grad():
            for batch in val_dataset:
                with autocast():
                    outputs = self.model(**batch)
                    loss = outputs.loss
                total_loss += loss.item()
        return total_loss / len(val_dataset)

    @staticmethod
    def should_early_stop(val_loss):
        """D√©cider si on doit arr√™ter l'entra√Ænement pr√©matur√©ment"""
        return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen-3B", help="Nom du mod√®le pr√©-entra√Æn√©")
    parser.add_argument("--output_dir", type=str, default="./models/finetuned", help="R√©pertoire de sortie")
    parser.add_argument("--data_path", type=str, required=True, help="Chemin vers le dataset")
    args = parser.parse_args()

    tuner = AutoFineTuner(base_model=args.base_model, output_dir=args.output_dir)
    dataset = tuner.prepare_dataset(args.data_path)
    hyperparams = {
        'epochs': 3,
        'learning_rate': 2e-4,
        'batch_size': 4
    }
    tuner.train_with_tracking(dataset, hyperparams)

```

---

## Fichier : `backend\altiora\core\training\feedback_system.py`

```python
# backend/altiora/core/training/feedback_system.py
from typing import Optional
from datetime import datetime

from src.learning.feedback_store import FeedbackStore
from src.learning.model_updater import ModelUpdater
class FeedbackLearningSystem:
    def __init__(self):
        self.feedback_store = FeedbackStore()
        self.model_updater = ModelUpdater()

    async def collect_feedback(self,
                               query: str,
                               response: str,
                               user_rating: int,
                               corrections: Optional[str] = None):
        """Collecte le feedback utilisateur pour am√©lioration continue"""
        feedback = {
            'timestamp': datetime.utcnow(),
            'query': query,
            'response': response,
            'rating': user_rating,
            'corrections': corrections
        }

        await self.feedback_store.save(feedback)

        # D√©clenche le fine-tuning si suffisamment de feedback
        if await self.should_trigger_retraining():
            await self.model_updater.schedule_retraining()
```

---

## Fichier : `backend\altiora\core\training\__init__.py`

```python
# backend/altiora/core/training/__init__.py
"""Initialise le package `training` de l'application Altiora.

Ce package contient les modules et scripts li√©s √† l'entra√Ænement et au
fine-tuning des mod√®les d'IA utilis√©s par l'application. Il inclut des
outils pour la pr√©paration des donn√©es, l'ex√©cution des processus
d'entra√Ænement et l'√©valuation des mod√®les.
"""

```

---

## Fichier : `backend\altiora\core\validation\continuous_validator.py`

```python
# backend/altiora/core/validation/continuous_validator.py
"""Module pour la validation continue des modifications de code.

Ce module impl√©mente un syst√®me de validation continue qui permet de
v√©rifier la qualit√©, la s√©curit√©, la performance et la conformit√© des
modifications (commits) avant qu'elles ne soient int√©gr√©es. Il agr√®ge
les r√©sultats de plusieurs validateurs sp√©cialis√©s et peut bloquer les
commits non conformes.
"""

import logging
from typing import Any, Dict

# Supposons que ces classes de validateurs et de rapport sont d√©finies ailleurs
# ou import√©es. Pour la documentation, nous d√©crivons leur r√¥le.
# from .code_quality_validator import CodeQualityValidator
# from .security_validator import SecurityValidator
# from .performance_validator import PerformanceValidator
# from .ai_model_validator import AIModelValidator
# from .validation_report import ValidationReport, ValidationError

logger = logging.getLogger(__name__)


# Classes factices pour la d√©monstration et la documentation.
class CodeQualityValidator:
    def __init__(self): self.name = "CodeQuality"
    async def validate(self, commit_hash: str) -> Dict[str, Any]:
        # Logique de validation de la qualit√© du code.
        return {"is_blocking": False, "passed": True, "message": "Qualit√© du code OK."}

class SecurityValidator:
    def __init__(self): self.name = "Security"
    async def validate(self, commit_hash: str) -> Dict[str, Any]:
        # Logique de validation de la s√©curit√©.
        return {"is_blocking": True, "passed": True, "message": "S√©curit√© OK."}

class PerformanceValidator:
    def __init__(self): self.name = "Performance"
    async def validate(self, commit_hash: str) -> Dict[str, Any]:
        # Logique de validation de la performance.
        return {"is_blocking": False, "passed": True, "message": "Performance OK."}

class AIModelValidator:
    def __init__(self): self.name = "AIModel"
    async def validate(self, commit_hash: str) -> Dict[str, Any]:
        # Logique de validation des mod√®les IA.
        return {"is_blocking": False, "passed": True, "message": "Mod√®le IA OK."}

class ValidationError(Exception):
    """Exception lev√©e lorsqu'une validation bloquante √©choue."""
    pass

class ValidationReport:
    """Rapport agr√©g√© des r√©sultats de validation."""
    def __init__(self, results: Dict[str, Any]):
        self.results = results
        self.overall_passed = all(r["passed"] for r in results.values())


class ContinuousValidator:
    """Orchestre l'ex√©cution de divers validateurs pour un commit donn√©."

    Cette classe est le point d'entr√©e pour lancer un cycle de validation
    complet. Elle agr√®ge les r√©sultats de chaque validateur et peut d√©clencher
    une erreur si une validation bloquante √©choue.
    """

    def __init__(self):
        """Initialise le validateur continu avec une liste de validateurs sp√©cialis√©s."""
        self.validators = [
            CodeQualityValidator(),
            SecurityValidator(),
            PerformanceValidator(),
            AIModelValidator()
        ]
        logger.info("ContinuousValidator initialis√© avec %d validateurs.", len(self.validators))

    async def validate_commit(self, commit_hash: str) -> ValidationReport:
        """Valide un commit sp√©cifique en ex√©cutant tous les validateurs configur√©s."

        Args:
            commit_hash: Le hachage (hash) du commit √† valider.

        Returns:
            Un objet `ValidationReport` contenant les r√©sultats d√©taill√©s de chaque validateur.

        Raises:
            ValidationError: Si l'un des validateurs configur√©s comme 'bloquant' √©choue.
        """
        results: Dict[str, Any] = {}
        logger.info(f"D√©marrage de la validation pour le commit : {commit_hash}")

        for validator_instance in self.validators:
            logger.info(f"Ex√©cution du validateur : {validator_instance.name}...")
            try:
                # Ex√©cute la validation pour chaque validateur.
                result = await validator_instance.validate(commit_hash)
                results[validator_instance.name] = result
                logger.info(f"Validateur {validator_instance.name} termin√©. Pass√© : {result["passed"]}")

                # Si le validateur est bloquant et qu'il a √©chou√©, l√®ve une exception.
                if result.get("is_blocking", False) and not result["passed"]:
                    error_message = f"La validation bloquante '{validator_instance.name}' a √©chou√© : {result.get("message", "Aucun message.")}"
                    logger.error(error_message)
                    raise ValidationError(error_message)

            except Exception as e:
                # Capture toute exception inattendue lors de l'ex√©cution d'un validateur.
                error_message = f"Erreur inattendue lors de l'ex√©cution du validateur {validator_instance.name}: {e}"
                logger.critical(error_message, exc_info=True)
                # Enregistre l'√©chec m√™me si le validateur n'est pas bloquant.
                results[validator_instance.name] = {"is_blocking": True, "passed": False, "message": error_message}
                raise ValidationError(error_message) from e # Re-l√®ve l'exception pour arr√™ter le processus.

        report = ValidationReport(results)
        if report.overall_passed:
            logger.info(f"Validation du commit {commit_hash} termin√©e : SUCC√àS.")
        else:
            logger.warning(f"Validation du commit {commit_hash} termin√©e : √âCHEC (non bloquant).")
        return report


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        validator = ContinuousValidator()

        print("\n--- D√©monstration de validation r√©ussie ---")
        try:
            # Simule un commit valide.
            report_success = await validator.validate_commit("abc123def456")
            print(f"Rapport de validation : {report_success.results}")
            print(f"Validation globale r√©ussie : {report_success.overall_passed}")
        except ValidationError as e:
            print(f"Validation √©chou√©e (attendu succ√®s) : {e}")

        print("\n--- D√©monstration de validation √©chou√©e (bloquant) ---")
        # Simule un √©chec de s√©curit√© (bloquant).
        # Pour cela, nous allons temporairement modifier le validateur de s√©curit√© factice.
        original_security_validate = SecurityValidator.validate
        SecurityValidator.validate = lambda self, commit_hash: {"is_blocking": True, "passed": False, "message": "Vuln√©rabilit√© critique d√©tect√©e !"}

        try:
            report_failure = await validator.validate_commit("ghi789jkl012")
            print(f"Rapport de validation : {report_failure.results}")
            print(f"Validation globale r√©ussie : {report_failure.overall_passed}")
        except ValidationError as e:
            print(f"Validation √©chou√©e (attendu √©chec bloquant) : {e}")
        finally:
            # Restaure le validateur de s√©curit√©.
            SecurityValidator.validate = original_security_validate

        print("D√©monstration du ContinuousValidator termin√©e.")

    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\infrastructure\database.py`

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from backend.altiora.config.settings import settings

engine = create_engine(
    settings.database_url,
    connect_args={"check_same_thread": False} if "sqlite" in settings.database_url else {}
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db() -> Session:
    """Dependency pour obtenir une session DB."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

---

## Fichier : `backend\altiora\infrastructure\__init__.py`

```python
# backend/altiora/infrastructure/__init__.py
"""
Couche d'infrastructure ‚Äì cache, file d'attente, monitoring, audit, scaling.
"""
```

---

## Fichier : `backend\altiora\infrastructure\audit\audit_logger.py`

```python
# backend/altiora/infrastructure/audit/audit_logger.py
"""Module pour la journalisation des actions d'audit dans l'application.

Ce module fournit une classe `AuditLogger` pour enregistrer les actions
significatives des utilisateurs et du syst√®me. Les entr√©es d'audit sont
horodat√©es, contiennent des d√©tails sur l'action, l'utilisateur, l'adresse IP
et l'ID de session, et sont stock√©es dans Redis avec une dur√©e de vie (TTL)
pour la conformit√© RGPD.
"""

import datetime
import json
import logging
from typing import Dict, Any

import redis.asyncio as redis

logger = logging.getLogger(__name__)


def get_client_ip() -> str:
    """Fonction factice pour r√©cup√©rer l'adresse IP du client.

    Dans une application r√©elle, cette fonction r√©cup√©rerait l'IP depuis la requ√™te HTTP.
    """
    # TODO: Impl√©menter la r√©cup√©ration r√©elle de l'adresse IP du client.
    return "unknown"


def get_session_id() -> str:
    """Fonction factice pour r√©cup√©rer l'ID de session.

    Dans une application r√©elle, cette fonction r√©cup√©rerait l'ID de session depuis le contexte.
    """
    # TODO: Impl√©menter la r√©cup√©ration r√©elle de l'ID de session.
    return "unknown"


class AuditLogger:
    """Enregistre les √©v√©nements d'audit dans Redis."""

    def __init__(self, redis_client: redis.Redis):
        """Initialise le logger d'audit avec un client Redis."

        Args:
            redis_client: Une instance de `redis.asyncio.Redis` connect√©e.
        """
        self.redis = redis_client

    async def log_action(self, action: str, user_id: str, details: Dict[str, Any]):
        """Enregistre une action d'audit.

        Les entr√©es d'audit sont stock√©es dans Redis avec un TTL de 90 jours
        (7776000 secondes) pour la conformit√© RGPD.

        Args:
            action: Le nom de l'action effectu√©e (ex: 'login', 'sfd_upload').
            user_id: L'identifiant de l'utilisateur qui a effectu√© l'action.
            details: Un dictionnaire contenant des d√©tails suppl√©mentaires sur l'action.
        """
        audit_entry = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "action": action,
            "user_id": user_id,
            "details": details,
            "ip_address": get_client_ip(),
            "session_id": get_session_id()
        }

        # G√©n√®re une cl√© unique pour l'entr√©e d'audit.
        key = f"audit:{user_id}:{datetime.datetime.utcnow().timestamp()}"
        try:
            # Stocke l'entr√©e d'audit dans Redis avec un TTL.
            await self.redis.setex(key, 7776000, json.dumps(audit_entry, ensure_ascii=False))
            logger.info(f"Action d'audit enregistr√©e : {action} par {user_id}")
        except Exception as e:
            logger.error(f"Erreur lors de l'enregistrement de l'action d'audit dans Redis : {e}")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # Assurez-vous qu'un serveur Redis est en cours d'ex√©cution.
        try:
            redis_client = redis.Redis(host='localhost', port=6379, db=0)
            await redis_client.ping()
            logger.info("Connect√© √† Redis pour la d√©monstration.")
        except Exception as e:
            logger.error(f"Impossible de se connecter √† Redis : {e}. La d√©monstration ne peut pas continuer.")
            return

        logger = AuditLogger(redis_client)

        print("\n--- Enregistrement d'actions d'audit ---")
        await logger.log_action("login", "user_alice", {"method": "password", "success": True})
        await logger.log_action("sfd_upload", "user_bob", {"file_name": "spec_v1.pdf", "size_kb": 1024})
        await logger.log_action("test_generation", "user_charlie", {"model": "starcoder2", "scenarios_count": 5})

        print("\n--- V√©rification des entr√©es d'audit (peut prendre un moment) ---")
        # Pour une v√©rification r√©elle, vous devriez interroger Redis.
        # Exemple (simplifi√©, ne r√©cup√®re pas toutes les cl√©s): 
        # keys = await redis_client.keys("audit:*")
        # for key in keys:
        #     entry = await redis_client.get(key)
        #     print(json.loads(entry))
        print("V√©rifiez votre instance Redis pour les entr√©es d'audit.")

        await redis_client.close()
        print("D√©monstration termin√©e.")

    import asyncio
    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\audit\decorator.py`

```python
# backend/altiora/infrastructure/audit/decorator.py
"""D√©corateur pour l'audit des fonctions asynchrones.

Ce module fournit un d√©corateur `@audit` qui permet d'enregistrer
automatiquement des √©v√©nements d'audit lorsqu'une fonction est appel√©e.
Il capture le d√©but de l'ex√©cution, l'acteur (utilisateur), l'action,
et g√®re les succ√®s ou les √©checs de la fonction d√©cor√©e.
"""

import datetime
import functools
import logging

from pathlib import Path
from src.audit.writer import AsyncAuditWriter
from src.audit.models import AuditEvent

logger = logging.getLogger(__name__)

# Initialise un writer d'audit asynchrone pour √©crire les logs dans le r√©pertoire sp√©cifi√©.
writer = AsyncAuditWriter(Path("logs/audit"))


def audit(action: str):
    """D√©corateur pour auditer l'ex√©cution d'une fonction asynchrone.

    Args:
        action: Une cha√Æne de caract√®res d√©crivant l'action audit√©e (ex: "sfd_upload", "test_gen").

    Returns:
        Un d√©corateur qui, lorsqu'il est appliqu√© √† une fonction asynchrone,
        enregistre un √©v√©nement d'audit avant et apr√®s son ex√©cution.
    """
    def decorator(fn):
        @functools.wraps(fn)
        async def wrapper(*args, **kwargs):
            start_time = datetime.datetime.utcnow() # Horodatage du d√©but de l'action.
            actor = kwargs.get("user_id", "system") # Tente de r√©cup√©rer l'ID utilisateur, sinon 'system'.
            
            try:
                result = await fn(*args, **kwargs)
                # Enregistre un √©v√©nement de succ√®s.
                writer.log(AuditEvent(
                    ts=start_time,
                    actor=actor,
                    action=action,
                    meta={"status": "success", "duration_ms": (datetime.datetime.utcnow() - start_time).total_seconds() * 1000}
                ))
                logger.info(f"Audit: Action '{action}' par '{actor}' r√©ussie.")
                return result
            except Exception as exc:
                # Enregistre un √©v√©nement d'√©chec avec les d√©tails de l'erreur.
                writer.log(AuditEvent(
                    ts=start_time,
                    actor=actor,
                    action=action,
                    meta={
                        "status": "failure",
                        "error": str(exc),
                        "duration_ms": (datetime.datetime.utcnow() - start_time).total_seconds() * 1000
                    }
                ))
                logger.error(f"Audit: Action '{action}' par '{actor}' a √©chou√© : {exc}")
                raise # Re-l√®ve l'exception pour ne pas masquer l'erreur originale.

        return wrapper

    return decorator


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # D√©marre le writer d'audit en arri√®re-plan.
        await writer.start()

        @audit("process_data")
        async def process_data_success(user_id: str, data: str):
            print(f"Traitement des donn√©es pour {user_id}: {data}")
            await asyncio.sleep(0.1)
            return {"processed": True, "data": data.upper()}

        @audit("process_data")
        async def process_data_failure(user_id: str, data: str):
            print(f"Tentative de traitement des donn√©es pour {user_id}: {data}")
            await asyncio.sleep(0.1)
            raise ValueError("Erreur de traitement simul√©e")

        print("\n--- D√©monstration de l'audit (succ√®s) ---")
        try:
            result = await process_data_success(user_id="alice", data="hello world")
            print(f"R√©sultat : {result}")
        except Exception as e:
            print(f"Erreur inattendue : {e}")

        print("\n--- D√©monstration de l'audit (√©chec) ---")
        try:
            await process_data_failure(user_id="bob", data="bad data")
        except Exception as e:
            print(f"Erreur captur√©e : {e}")

        # Attendre que le writer ait eu le temps de flusher.
        print("\nAttente du flush des logs d'audit...")
        await asyncio.sleep(2) # Donne un peu de temps au writer.
        await writer.stop() # Arr√™te le writer proprement.
        print("D√©monstration termin√©e. V√©rifiez le r√©pertoire logs/audit.")

    import asyncio
    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\audit\models.py`

```python
# backend/altiora/infrastructure/audit/models.py
"""Mod√®les de donn√©es pour les √©v√©nements d'audit.

Ce module d√©finit la structure des √©v√©nements d'audit qui sont enregistr√©s
par le syst√®me. Il utilise `dataclasses` pour une d√©finition claire et concise
des champs de chaque √©v√©nement.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Literal, Dict, Any, Optional


@dataclass(slots=True)
class AuditEvent:
    """Repr√©sente un √©v√©nement d'audit enregistr√© dans le syst√®me.

    Attributes:
        ts: Horodatage de l'√©v√©nement (UTC).
        actor: L'identifiant de l'entit√© qui a initi√© l'action (ex: ID utilisateur, 'system').
        action: Le type d'action effectu√©e (ex: 'sfd_upload', 'test_gen', 'admin_command', 'pii_detected').
        resource: L'identifiant de la ressource affect√©e par l'action (optionnel).
        meta: Un dictionnaire de m√©tadonn√©es suppl√©mentaires sp√©cifiques √† l'√©v√©nement (optionnel).
    """
    ts: datetime
    actor: str
    action: Literal["sfd_upload", "test_gen", "admin_command", "pii_detected", "login", "logout", "user_create", "user_update", "error"]
    resource: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    # Cr√©ation d'un √©v√©nement d'audit simple.
    event1 = AuditEvent(
        ts=datetime.utcnow(),
        actor="user_123",
        action="sfd_upload",
        resource="document_abc.pdf",
        meta={"file_size_kb": 512, "upload_ip": "192.168.1.1"}
    )
    print(f"√âv√©nement 1 : {event1}")

    # Cr√©ation d'un √©v√©nement d'erreur.
    event2 = AuditEvent(
        ts=datetime.utcnow(),
        actor="system",
        action="error",
        meta={
            "error_type": "FileNotFoundError",
            "message": "Fichier introuvable lors du traitement",
            "path": "/tmp/non_existent_file.txt"
        }
    )
    print(f"√âv√©nement 2 : {event2}")

    # Acc√®s aux attributs.
    print(f"Action de l'√©v√©nement 1 : {event1.action}")
    if event1.meta:
        print(f"Taille du fichier de l'√©v√©nement 1 : {event1.meta.get('file_size_kb')} KB")
```

---

## Fichier : `backend\altiora\infrastructure\audit\ring_buffer.py`

```python
# backend/altiora/infrastructure/audit/ring_buffer.py
"""Impl√©mentation d'un tampon circulaire (ring buffer) pour les √©v√©nements d'audit.

Ce module fournit une structure de donn√©es de type tampon circulaire qui stocke
un nombre fixe d'√©v√©nements d'audit. Lorsque le tampon est plein, les nouveaux
√©v√©nements √©crasent les plus anciens. Cela est utile pour collecter des logs
en m√©moire avant de les √©crire par lots sur disque, r√©duisant ainsi la charge
E/S et la latence.
"""

import json
from collections import deque
from dataclasses import asdict
from typing import List

from src.audit.models import AuditEvent


class RingBuffer:
    """Un tampon circulaire pour stocker un nombre limit√© d'√©v√©nements d'audit en m√©moire."""

    def __init__(self, size: int = 10_000):
        """Initialise le tampon circulaire.

        Args:
            size: La taille maximale du tampon (nombre d'√©v√©nements).
        """
        self._buf: deque[str] = deque(maxlen=size) # Utilise `deque` avec `maxlen` pour la fonctionnalit√© de tampon circulaire.

    def push(self, event: AuditEvent) -> None:
        """Ajoute un √©v√©nement d'audit au tampon.

        L'√©v√©nement est converti en cha√Æne JSON avant d'√™tre stock√©.
        Si le tampon est plein, l'√©v√©nement le plus ancien est automatiquement supprim√©.

        Args:
            event: L'objet `AuditEvent` √† ajouter.
        """
        # Convertit l'objet AuditEvent en JSON string pour le stockage.
        self._buf.append(json.dumps(asdict(event), default=str))

    def flush(self) -> List[str]:
        """Vide le contenu du tampon et le retourne sous forme de liste de cha√Ænes JSON.

        Apr√®s l'appel, le tampon est vid√©.

        Returns:
            Une liste de cha√Ænes JSON, chaque cha√Æne repr√©sentant un √©v√©nement d'audit.
        """
        out = list(self._buf) # Copie le contenu du deque dans une liste.
        self._buf.clear() # Vide le deque.
        return out


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    from datetime import datetime
    import time

    print("\n--- D√©monstration du RingBuffer ---")
    buffer_size = 5
    buffer = RingBuffer(size=buffer_size)

    # Ajout d'√©v√©nements jusqu'√† remplir le tampon.
    for i in range(buffer_size):
        event = AuditEvent(
            ts=datetime.utcnow(),
            actor=f"user_{i}",
            action="test_action",
            meta={"value": i}
        )
        buffer.push(event)
        print(f"Ajout√© √©v√©nement {i}. Taille du tampon : {len(buffer._buf)}")

    print("\n--- Tampon plein, ajout d'un nouvel √©v√©nement (√©crase le plus ancien) ---")
    event_new = AuditEvent(
        ts=datetime.utcnow(),
        actor="user_new",
        action="new_action",
        meta={"value": 99}
    )
    buffer.push(event_new)
    print(f"Ajout√© √©v√©nement 99. Taille du tampon : {len(buffer._buf)}")

    print("\n--- Contenu du tampon apr√®s ajout (le premier √©v√©nement devrait √™tre parti) ---")
    for item in buffer._buf:
        print(json.loads(item).get("meta", {}).get("value"))

    print("\n--- Vidage du tampon (flush) ---")
    flushed_events = buffer.flush()
    print(f"Nombre d'√©v√©nements vid√©s : {len(flushed_events)}")
    print(f"Tampon apr√®s vidage : {len(buffer._buf)}")

    print("\n--- Contenu des √©v√©nements vid√©s ---")
    for event_str in flushed_events:
        event_dict = json.loads(event_str)
        print(f"Action: {event_dict['action']}, Acteur: {event_dict['actor']}, Valeur: {event_dict['meta'].get('value')}")

    print("D√©monstration du RingBuffer termin√©e.")
```

---

## Fichier : `backend\altiora\infrastructure\audit\rotation.py`

```python
# backend/altiora/infrastructure/audit/rotation.py
"""Module pour la rotation et l'archivage s√©curis√© des journaux d'audit.

Ce module g√®re la rotation mensuelle des fichiers de log d'audit.
Les logs sont compress√©s, archiv√©s dans un fichier `.tar.gz`,
puis chiffr√©s avant d'√™tre stock√©s dans un r√©pertoire d'archive.
Les fichiers originaux sont ensuite supprim√©s.
"""

import datetime
import logging
import tarfile
from pathlib import Path

from src.infrastructure.encryption import AltioraEncryption

logger = logging.getLogger(__name__)


def rotate_monthly():
    """Effectue la rotation mensuelle des journaux d'audit.

    Cette fonction est con√ßue pour √™tre ex√©cut√©e p√©riodiquement (ex: via un cron job).
    Elle collecte tous les fichiers `.jsonl.zst` du r√©pertoire `logs/audit`,
    les archive dans un fichier `.tar.gz` chiffr√©, et supprime les originaux.
    """
    current_month_str = datetime.datetime.utcnow().strftime("%Y%m")
    audit_log_dir = Path("logs/audit")
    archive_dir = Path("logs/archive")
    
    # Cr√©e le r√©pertoire d'archive s'il n'existe pas.
    archive_dir.mkdir(parents=True, exist_ok=True)

    # Chemin du fichier d'archive temporaire (non chiffr√©).
    temp_archive_path = archive_dir / f"audit_{current_month_str}.tar"
    # Chemin du fichier d'archive final (chiffr√©).
    final_encrypted_archive_path = archive_dir / f"audit_{current_month_str}.tar.gz.enc"

    logger.info(f"D√©marrage de la rotation des logs d'audit pour le mois {current_month_str}...")

    try:
        # 1. Cr√©e une archive tarball des fichiers de log.
        with tarfile.open(temp_archive_path, "w") as tar:
            # Parcourt tous les fichiers de log compress√©s dans le r√©pertoire d'audit.
            for log_file in audit_log_dir.glob("*.jsonl.zst"):
                tar.add(log_file, arcname=log_file.name) # Ajoute le fichier √† l'archive.
                log_file.unlink() # Supprime le fichier original apr√®s l'avoir ajout√© √† l'archive.
        logger.info(f"Fichiers de log archiv√©s dans {temp_archive_path}.")

        # 2. Chiffre l'archive.
        # La cl√© de chiffrement est g√©r√©e par AltioraEncryption (probablement via variables d'environnement).
        cipher = AltioraEncryption("AUDIT_BACKUP_KEY") # Utilise une cl√© sp√©cifique pour l'audit.
        encrypted_data = cipher.encrypt_file(temp_archive_path)
        final_encrypted_archive_path.write_bytes(encrypted_data)
        logger.info(f"Archive chiffr√©e et sauvegard√©e : {final_encrypted_archive_path}.")

        # 3. Supprime l'archive temporaire non chiffr√©e.
        temp_archive_path.unlink()
        logger.info(f"Archive temporaire {temp_archive_path} supprim√©e.")

    except (IOError, OSError, tarfile.ReadError) as e:
        logger.error(f"Erreur lors de la rotation des logs d'audit : {e}", exc_info=True)
    except Exception as e:
        logger.critical(f"Erreur inattendue lors de la rotation des logs d'audit : {e}", exc_info=True)


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import time
    import zstandard

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Cr√©e quelques fichiers de log factices pour la d√©monstration.
    audit_log_dir = Path("logs/audit")
    audit_log_dir.mkdir(parents=True, exist_ok=True)

    print("\n--- Cr√©ation de fichiers de log factices ---")
    for i in range(3):
        log_file_path = audit_log_dir / f"test_audit_{datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{i}.jsonl.zst"
        with open(log_file_path, "wb") as f:
            compressor = zstandard.ZstdCompressor(level=1)
            f.write(compressor.compress(f"Log entry {i}\n".encode()))
        print(f"Cr√©√© : {log_file_path}")
        time.sleep(0.1) # Pour avoir des horodatages diff√©rents.

    print("\n--- Lancement de la rotation mensuelle ---")
    rotate_monthly()

    print("\n--- V√©rification du r√©pertoire d'audit ---")
    remaining_logs = list(audit_log_dir.glob("*.jsonl.zst"))
    if not remaining_logs:
        print("‚úÖ Tous les fichiers de log ont √©t√© archiv√©s et supprim√©s du r√©pertoire d'audit.")
    else:
        print("‚ùå Des fichiers de log sont rest√©s :", remaining_logs)

    print("\n--- V√©rification du r√©pertoire d'archive ---")
    archive_dir = Path("logs/archive")
    encrypted_archives = list(archive_dir.glob("*.tar.gz.enc"))
    if encrypted_archives:
        print(f"‚úÖ Archive chiffr√©e trouv√©e : {encrypted_archives[0]}")
        # Pour d√©chiffrer et v√©rifier, il faudrait la cl√© et la logique de d√©chiffrement.
    else:
        print("‚ùå Aucune archive chiffr√©e trouv√©e.")

    print("D√©monstration de la rotation des logs termin√©e.")
```

---

## Fichier : `backend\altiora\infrastructure\audit\writer.py`

```python
# backend/altiora/infrastructure/audit/writer.py
"""Writer asynchrone pour les √©v√©nements d'audit.

Ce module fournit une classe `AsyncAuditWriter` qui collecte les √©v√©nements
d'audit dans un tampon circulaire en m√©moire et les √©crit p√©riodiquement
sur disque dans des fichiers compress√©s (Zstandard). Cela permet de r√©duire
la fr√©quence des op√©rations d'√©criture sur disque et d'am√©liorer les performances.
"""

import asyncio
from pathlib import Path
import logging
from datetime import datetime
from src.audit.ring_buffer import RingBuffer
from src.audit.models import AuditEvent

import aiofiles
import zstandard as zstd

logger = logging.getLogger(__name__)


class AsyncAuditWriter:
    """√âcrit les √©v√©nements d'audit de mani√®re asynchrone et par lots sur disque."""

    def __init__(self, log_dir: Path, buffer_size: int = 10_000, flush_interval: int = 5):
        """Initialise le writer d'audit asynchrone.

        Args:
            log_dir: Le r√©pertoire o√π les fichiers de log d'audit seront stock√©s.
            buffer_size: La taille maximale du tampon circulaire en m√©moire.
            flush_interval: L'intervalle en secondes entre chaque √©criture sur disque.
        """
        self.log_dir = log_dir
        self.log_dir.mkdir(parents=True, exist_ok=True) # Cr√©e le r√©pertoire si n√©cessaire.
        self._ctx = zstd.ZstdCompressor(level=3)  # Compresseur Zstandard (niveau 3 pour un bon √©quilibre).
        self._buffer = RingBuffer(size=buffer_size)
        self._flush_interval = flush_interval
        self._flush_task: asyncio.Task | None = None

    async def start(self):
        """D√©marre la t√¢che de flush p√©riodique en arri√®re-plan."""
        if self._flush_task === None or self._flush_task.done():
            self._flush_task = asyncio.create_task(self._periodic_flush())
            logger.info(f"AsyncAuditWriter d√©marr√©. Flush toutes les {self._flush_interval} secondes.")

    async def stop(self):
        """Arr√™te la t√¢che de flush p√©riodique et force un dernier flush."""
        if self._flush_task:
            self._flush_task.cancel()
            try:
                await self._flush_task # Attend que la t√¢che se termine (g√®re l'exception CancelledError).
            except asyncio.CancelledError:
                pass
            logger.info("AsyncAuditWriter arr√™t√©. Ex√©cution du flush final...")
        await self._flush_to_disk() # Force un dernier flush pour s'assurer que toutes les donn√©es sont √©crites.
        logger.info("Flush final termin√©.")

    def log(self, event: AuditEvent) -> None:
        """Ajoute un √©v√©nement d'audit au tampon en m√©moire."

        Args:
            event: L'objet `AuditEvent` √† enregistrer.
        """
        self._buffer.push(event)

    async def _periodic_flush(self):
        """T√¢che asynchrone qui vide p√©riodiquement le tampon sur disque."""
        while True:
            try:
                await asyncio.sleep(self._flush_interval)
                await self._flush_to_disk()
            except asyncio.CancelledError:
                break # La t√¢che a √©t√© annul√©e, sort de la boucle.
            except Exception as e:
                logger.error(f"Erreur lors du flush p√©riodique des logs d'audit : {e}", exc_info=True)

    async def _flush_to_disk(self):
        """Vide le contenu du tampon en m√©moire vers un fichier compress√© sur disque."""
        batch = self._buffer.flush()
        if batch:
            # G√©n√®re un nom de fichier unique bas√© sur l'horodatage.
            path = self.log_dir / f"audit_{datetime.utcnow():%Y%m%d_%H%M%S_%f}.jsonl.zst"
            try:
                async with aiofiles.open(path, "wb") as f:
                    # Compresse le lot d'√©v√©nements et l'√©crit dans le fichier.
                    await f.write(self._ctx.compress("\n".join(batch).encode('utf-8')))
                logger.info(f"Logs d'audit √©crits sur disque : {path} ({len(batch)} √©v√©nements).")
            except (IOError, OSError, zstd.ZstdError) as e:
                logger.error(f"Erreur lors de l'√©criture des logs d'audit sur {path}: {e}")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        log_directory = Path("temp_audit_logs")
        # S'assure que le r√©pertoire de d√©monstration est propre.
        if log_directory.exists():
            import shutil
            shutil.rmtree(log_directory)
        log_directory.mkdir()

        writer = AsyncAuditWriter(log_directory, buffer_size=3, flush_interval=2) # Petit buffer et intervalle pour la d√©mo.
        await writer.start()

        print("\n--- Enregistrement d'√©v√©nements d'audit ---")
        for i in range(10):
            event = AuditEvent(
                ts=datetime.utcnow(),
                actor=f"user_{i}",
                action="demo_action",
                meta={"event_id": i, "data": f"some_data_{i}"}
            )
            writer.log(event)
            print(f"Loggu√© √©v√©nement {i}.")
            await asyncio.sleep(0.5) # Simule l'arriv√©e des √©v√©nements.

        print("\n--- Arr√™t du writer et flush final ---")
        await writer.stop()

        print("\n--- V√©rification des fichiers g√©n√©r√©s ---")
        generated_files = list(log_directory.glob("*.jsonl.zst"))
        for f in generated_files:
            print(f"Fichier g√©n√©r√© : {f}")
            with open(f, "rb") as fb:
                decompressor = zstd.ZstdDecompressor()
                content = decompressor.decompress(fb.read()).decode('utf-8')
                print(f"Contenu:\n{content[:200]}...")

        print("D√©monstration de AsyncAuditWriter termin√©e.")
        # Nettoyage du r√©pertoire temporaire.
        import shutil
        shutil.rmtree(log_directory)

    import asyncio
    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\audit\__init__.py`

```python
# backend/altiora/infrastructure/audit/__init__.py
"""
Audit logging and tracking.
"""
```

---

## Fichier : `backend\altiora\infrastructure\events\event_bus.py`

```python
# backend/altiora/infrastructure/events/event_bus.py
"""Module impl√©mentant un bus d'√©v√©nements asynchrone pour la communication inter-composants.

Ce bus d'√©v√©nements permet aux diff√©rents modules de l'application de communiquer
de mani√®re d√©coupl√©e en publiant et en s'abonnant √† des √©v√©nements. Il utilise
une file d'attente asynchrone pour g√©rer les √©v√©nements et peut √™tre √©tendu
pour utiliser des syst√®mes de messagerie distribu√©s comme Redis Pub/Sub.
"""

import asyncio
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Dict, List, Callable, Any, Optional

import redis.asyncio as redis

# Importations factices pour la d√©monstration. En production, ce seraient de vrais services.
# from src.services.test_generator import TestGenerator
# from src.services.notification_service import NotificationService
# from src.services.storage_service import StorageService

# test_generator = TestGenerator() # Supposons que TestGenerator est une classe
# notification_service = NotificationService() # Supposons que NotificationService est une classe
# storage_service = StorageService() # Supposons que StorageService est une classe


class EventType(Enum):
    """√ânum√©ration des types d'√©v√©nements support√©s par le bus."""
    SFD_UPLOADED = "sfd.uploaded"
    ANALYSIS_COMPLETED = "analysis.completed"
    TESTS_GENERATED = "tests.generated"
    PIPELINE_FAILED = "pipeline.failed"
    USER_LOGIN = "user.login"
    USER_LOGOUT = "user.logout"


@dataclass
class Event:
    """Repr√©sente un √©v√©nement circulant dans le bus."""
    type: EventType
    payload: Dict[str, Any]
    timestamp: datetime
    correlation_id: str # Pour suivre les √©v√©nements √† travers les syst√®mes.


class EventBus:
    """Bus d'√©v√©nements asynchrone pour la publication/souscription d'√©v√©nements."""

    def __init__(self, redis_client: Optional[redis.Redis] = None):
        """Initialise le bus d'√©v√©nements."

        Args:
            redis_client: Un client Redis asynchrone pour une √©ventuelle extension
                          vers un bus d'√©v√©nements distribu√© (Pub/Sub).
        """
        self._handlers: Dict[EventType, List[Callable]] = {} # Mappe les types d'√©v√©nements √† leurs gestionnaires.
        self._queue: asyncio.Queue = asyncio.Queue() # File d'attente interne pour les √©v√©nements.
        self._running = False # Indique si le bus est en cours d'ex√©cution.
        self.redis_client = redis_client

    def subscribe(self, event_type: EventType, handler: Callable):
        """Abonne un gestionnaire √† un type d'√©v√©nement sp√©cifique."

        Args:
            event_type: Le type d'√©v√©nement auquel s'abonner.
            handler: La fonction (ou coroutine) qui sera appel√©e lorsque l'√©v√©nement se produit.
        """
        if event_type not in self._handlers:
            self._handlers[event_type] = []
        self._handlers[event_type].append(handler)

    async def publish(self, event: Event):
        """Publie un √©v√©nement sur le bus."

        L'√©v√©nement est plac√© dans la file d'attente interne pour un traitement asynchrone.

        Args:
            event: L'objet `Event` √† publier.
        """
        await self._queue.put(event)

    async def start(self):
        """D√©marre le traitement des √©v√©nements en arri√®re-plan."

        Cette m√©thode lance une boucle qui consomme les √©v√©nements de la file
        d'attente et les distribue aux gestionnaires abonn√©s.
        """
        self._running = True
        while self._running:
            try:
                # Attend un √©v√©nement avec un timeout pour permettre un arr√™t propre.
                event = await asyncio.wait_for(self._queue.get(), timeout=1.0)
                await self._process_event(event)
            except asyncio.TimeoutError:
                continue # Continue la boucle si aucun √©v√©nement n'est disponible.
            except asyncio.CancelledError:
                break # Le bus a √©t√© arr√™t√©.
            except Exception as e:
                # Loggue l'erreur mais continue de traiter les autres √©v√©nements.
                print(f"Erreur lors du traitement d'un √©v√©nement : {e}")

    async def stop(self):
        """Arr√™te le bus d'√©v√©nements et attend la fin du traitement des √©v√©nements en cours."""
        self._running = False
        # Attend que la file d'attente soit vide.
        await self._queue.join()

    async def _process_event(self, event: Event):
        """Distribue un √©v√©nement √† tous les gestionnaires abonn√©s."

        Args:
            event: L'objet `Event` √† traiter.
        """
        handlers = self._handlers.get(event.type, [])
        # Ex√©cute tous les gestionnaires en parall√®le.
        await asyncio.gather(
            *[handler(event) for handler in handlers],
            return_exceptions=True # Permet aux autres gestionnaires de s'ex√©cuter m√™me si l'un √©choue.
        )


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Fonctions de gestionnaires factices.
    async def handle_sfd_uploaded(event: Event):
        logging.info(f"[Handler SFD] SFD t√©l√©charg√©e : {event.payload.get('filename')}")
        # Simule une action, ex: d√©clencher l'analyse.
        # await test_generator.generate_from_sfd(event.payload["sfd_id"])

    async def handle_analysis_completed(event: Event):
        logging.info(f"[Handler Analyse] Analyse termin√©e pour SFD : {event.payload.get('sfd_id')}")
        # Simule une action, ex: notifier l'utilisateur.
        # await notification_service.notify(event.payload)

    async def handle_pipeline_failed(event: Event):
        logging.error(f"[Handler √âchec] Pipeline √©chou√© pour ID : {event.payload.get('pipeline_id')}. Erreur : {event.payload.get('error')}")
        # Simule une action, ex: envoyer une alerte √† l'administrateur.

    async def demo():
        print("\n--- D√©monstration du EventBus ---")
        bus = EventBus()
        await bus.start() # D√©marre le bus d'√©v√©nements.

        # Abonnements aux √©v√©nements.
        bus.subscribe(EventType.SFD_UPLOADED, handle_sfd_uploaded)
        bus.subscribe(EventType.ANALYSIS_COMPLETED, handle_analysis_completed)
        bus.subscribe(EventType.PIPELINE_FAILED, handle_pipeline_failed)

        # Publication d'√©v√©nements.
        print("Publication d'√©v√©nements...")
        await bus.publish(Event(
            type=EventType.SFD_UPLOADED,
            payload={'sfd_id': 'SFD-001', 'filename': 'spec_v1.pdf'},
            timestamp=datetime.utcnow(),
            correlation_id='corr-123'
        ))

        await bus.publish(Event(
            type=EventType.ANALYSIS_COMPLETED,
            payload={'sfd_id': 'SFD-001', 'scenarios_count': 10},
            timestamp=datetime.utcnow(),
            correlation_id='corr-123'
        ))

        await bus.publish(Event(
            type=EventType.PIPELINE_FAILED,
            payload={'pipeline_id': 'PIPE-001', 'error': '√âtape de g√©n√©ration de code √©chou√©e.'},
            timestamp=datetime.utcnow(),
            correlation_id='corr-456'
        ))

        # Donne un peu de temps aux gestionnaires pour traiter les √©v√©nements.
        await asyncio.sleep(2)

        print("Arr√™t du bus d'√©v√©nements...")
        await bus.stop()
        print("D√©monstration termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\events\__init__.py`

```python
# backend/altiora/infrastructure/events/__init__.py
"""Initialise le package des √©v√©nements de l'application Altiora.

Ce package contient la d√©finition du bus d'√©v√©nements et des types d'√©v√©nements
utilis√©s pour la communication d√©coupl√©e entre les diff√©rents composants de l'application.
"""

```

---

## Fichier : `backend\altiora\infrastructure\monitoring\health.py`

```python
# backend/altiora/infrastructure/monitoring/health.py
"""Service de v√©rification de l'√©tat de sant√© (Health Check) de l'application Altiora.

Ce service expose un point de terminaison `/health` qui fournit un aper√ßu
rapide de l'√©tat des composants critiques de l'application, tels que la
connexion √† Redis et la disponibilit√© d'Ollama. Il est con√ßu pour √™tre
utilis√© par les syst√®mes de surveillance externes (ex: Kubernetes, Prometheus).
"""

import json
import logging

from fastapi import FastAPI, Response
import redis.asyncio as redis
import httpx

logger = logging.getLogger(__name__)

app = FastAPI(
    title="Altiora Health Check Service",
    description="V√©rifie l'√©tat de sant√© des composants critiques d'Altiora.",
    version="1.0.0",
)


@app.get("/health")
async def health() -> Response:
    """Point de terminaison principal pour la v√©rification de l'√©tat de sant√©.

    Effectue des v√©rifications sur :
    - La connexion √† Redis.
    - La disponibilit√© du serveur Ollama.

    Returns:
        Une `FastAPI.Response` au format JSON indiquant l'√©tat global et le statut de chaque v√©rification.
        Le code HTTP est 200 si tout est sain, 503 (Service Unavailable) sinon.
    """
    status_info = {"status": "healthy", "checks": {}}
    http_status_code = 200

    # V√©rification de la connexion Redis.
    try:
        # Tente de se connecter √† Redis en utilisant l'URL par d√©faut.
        r = redis.from_url("redis://localhost:6379")
        await r.ping() # Envoie une commande PING pour v√©rifier la connexion.
        status_info["checks"]["redis"] = "OK"
    except Exception as e:
        status_info["checks"]["redis"] = f"√âchec: {str(e)}"
        http_status_code = 503
        logger.error(f"V√©rification Redis √©chou√©e : {e}")

    # V√©rification de la disponibilit√© d'Ollama.
    async with httpx.AsyncClient(timeout=5) as client: # Timeout de 5 secondes pour la requ√™te HTTP.
        try:
            # Tente d'acc√©der √† l'endpoint de sant√© d'Ollama.
            resp = await client.get("http://localhost:11434/api/tags") # Utilise /api/tags car /health n'existe pas toujours.
            resp.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx.
            status_info["checks"]["ollama"] = "OK"
        except httpx.RequestError as e:
            status_info["checks"]["ollama"] = f"√âchec de la requ√™te: {str(e)}"
            http_status_code = 503
            logger.error(f"V√©rification Ollama (requ√™te) √©chou√©e : {e}")
        except httpx.HTTPStatusError as e:
            status_info["checks"]["ollama"] = f"√âchec HTTP: {e.response.status_code} - {e.response.text}"
            http_status_code = 503
            logger.error(f"V√©rification Ollama (HTTP) √©chou√©e : {e}")

    # Retourne la r√©ponse JSON avec le code d'√©tat appropri√©.
    return Response(content=json.dumps(status_info, indent=2), status_code=http_status_code, media_type="application/json")


# ------------------------------------------------------------------
# Point d'entr√©e Uvicorn
# ------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger.info("Lancement du service de v√©rification de sant√© sur http://0.0.0.0:8000/health")
    uvicorn.run(app, host="0.0.0.0", port=8000)

```

---

## Fichier : `backend\altiora\infrastructure\monitoring\healthcheck.py`

```python
# backend/altiora/infrastructure/monitoring/healthcheck.py
"""Service de v√©rification de l'√©tat de sant√© (Health Check) pour les composants critiques d'Altiora.

Ce module fournit un point de terminaison `/health` qui agr√®ge l'√©tat de
sant√© de plusieurs services et d√©pendances cl√©s de l'application, tels que
Redis, Ollama et d'autres microservices internes. Il est con√ßu pour √™tre
utilis√© par les syst√®mes de surveillance externes pour √©valuer la disponibilit√©
globale de l'application.
"""

import json
import logging

from fastapi import FastAPI, Response
import aioredis
import httpx # Pour v√©rifier Ollama et d'autres services HTTP.

logger = logging.getLogger(__name__)

app = FastAPI(
    title="Altiora Healthcheck Service",
    description="V√©rifie l'√©tat de sant√© des d√©pendances cl√©s d'Altiora.",
    version="1.0.0",
)


async def check_redis() -> bool:
    """V√©rifie la connectivit√© et l'√©tat de sant√© du serveur Redis."

    Returns:
        True si Redis est accessible et r√©pond, False sinon.
    """
    try:
        # Utilise l'URL par d√©faut pour Redis, √† adapter si n√©cessaire.
        redis_client = aioredis.from_url("redis://localhost:6379")
        await redis_client.ping()
        await redis_client.close() # Ferme la connexion apr√®s le ping.
        logger.debug("V√©rification Redis : OK")
        return True
    except aioredis.exceptions.ConnectionError as e:
        logger.warning(f"V√©rification Redis : √âchec de la connexion - {e}")
        return False
    except Exception as e:
        logger.error(f"V√©rification Redis : Erreur inattendue - {e}")
        return False


async def check_ollama() -> bool:
    """V√©rifie la disponibilit√© du serveur Ollama."

    Returns:
        True si Ollama est accessible, False sinon.
    """
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            # Tente d'acc√©der √† un endpoint simple d'Ollama pour v√©rifier sa disponibilit√©.
            resp = await client.get("http://localhost:11434/api/tags")
            resp.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx.
            logger.debug("V√©rification Ollama : OK")
            return True
    except httpx.RequestError as e:
        logger.warning(f"V√©rification Ollama : √âchec de la requ√™te - {e}")
        return False
    except httpx.HTTPStatusError as e:
        logger.warning(f"V√©rification Ollama : √âchec HTTP - {e.response.status_code}")
        return False
    except Exception as e:
        logger.error(f"V√©rification Ollama : Erreur inattendue - {e}")
        return False


async def check_services() -> bool:
    """V√©rifie l'√©tat de sant√© des autres microservices internes d'Altiora."

    Cette fonction est un placeholder. Dans une impl√©mentation r√©elle, elle
    ferait des appels aux endpoints `/health` de chaque microservice (ALM, OCR, etc.).

    Returns:
        True si tous les services sont sains, False sinon.
    """
    # TODO: Impl√©menter la logique de v√©rification des autres microservices.
    # Exemple: faire des requ√™tes HTTP √† http://localhost:8001/health (OCR), etc.
    logger.warning("V√©rification des services : Logique non impl√©ment√©e, retourne True par d√©faut.")
    return True


@app.get("/health")
async def health_check() -> Response:
    """Point de terminaison principal pour la v√©rification de l'√©tat de sant√© compl√®te de l'application."

    Effectue des v√©rifications sur Redis, Ollama et d'autres services.

    Returns:
        Une `FastAPI.Response` au format JSON indiquant l'√©tat global et le statut
        de chaque v√©rification. Le code HTTP est 200 si tout est sain, 503 sinon.
    """
    checks = {
        "redis": await check_redis(),
        "ollama": await check_ollama(),
        "services": await check_services() # Placeholder pour d'autres services.
    }

    if all(checks.values()):
        status_code = 200
        status_message = "healthy"
    else:
        status_code = 503
        status_message = "unhealthy"

    response_content = {"status": status_message, "checks": {k: ("OK" if v else "FAILED") for k, v in checks.items()}}
    return Response(
        content=json.dumps(response_content, indent=2),
        status_code=status_code,
        media_type="application/json"
    )


# ------------------------------------------------------------------
# Point d'entr√©e Uvicorn
# ------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger.info("Lancement du service de healthcheck sur http://0.0.0.0:8000/health")
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)

```

---

## Fichier : `backend\altiora\infrastructure\monitoring\__init__.py`

```python
# backend/altiora/infrastructure/monitoring/__init__.py
"""Initialise le package `monitoring` de l'application Altiora.

Ce package contient les modules li√©s √† la surveillance et √† l'observabilit√©
de l'application, y compris les v√©rifications de sant√©, la collecte de m√©triques
et le tra√ßage distribu√©.

Les modules suivants sont expos√©s pour faciliter les importations :
- `healthcheck_app`: L'application FastAPI pour les v√©rifications de sant√©.
- `MetricsCollector`: Le collecteur de m√©triques Prometheus.
"""
from .healthcheck import app as healthcheck_app
from .metrics_collector import MetricsCollector

__all__ = ['healthcheck_app', 'MetricsCollector']

```

---

## Fichier : `backend\altiora\infrastructure\monitoring\metrics\model_metrics.py`

```python
# backend/altiora/infrastructure/monitoring/metrics/model_metrics.py
from src.metrics.accuracy_tracker import AccuracyTracker
from src.metrics.latency_monitor import LatencyMonitor
from src.metrics.token_usage_tracker import TokenUsageTracker

# backend/altiora/infrastructure/monitoring/metrics/model_metrics.py
class ModelMetrics:
    def __init__(self):
        self.accuracy_tracker = AccuracyTracker()
        self.latency_monitor = LatencyMonitor()
        self.token_usage = TokenUsageTracker()

    def evaluate_model_performance(self, model_name: str):
        """√âvalue les performances du mod√®le fine-tun√©"""
        metrics = {
            'perplexity': self.calculate_perplexity(),
            'bleu_score': self.calculate_bleu(),
            'response_time_p95': self.latency_monitor.get_p95(),
            'tokens_per_second': self.token_usage.get_throughput()
        }
        return metrics
```

---

## Fichier : `backend\altiora\infrastructure\queue\redis_queue.py`

```python
# backend/altiora/infrastructure/queue/redis_queue.py
"""
Queue de t√¢ches Redis simple.
"""

from __future__ import annotations

import json
from datetime import datetime

from redis.asyncio import Redis


class RedisTaskQueue:
    """Queue bas√©e sur Redis list + sorted-set pour l‚Äôordonnancement."""

    def __init__(self, url: str = "redis://localhost:6379/0") -> None:
        self.redis = Redis.from_url(url)

    async def enqueue(
        self,
        task_name: str,
        payload: dict[str, Any],
        eta: datetime | None = None,
    ) -> None:
        """Ajoute une t√¢che dans la file."""
        eta = eta or datetime.utcnow()
        data = json.dumps({"task": task_name, "payload": payload})
        await self.redis.zadd("batch_queue", {data: eta.timestamp()})

    async def dequeue(self) -> dict[str, Any] | None:
        """R√©cup√®re la t√¢che la plus ancienne."""
        now = datetime.utcnow().timestamp()
        items = await self.redis.zrangebyscore("batch_queue", 0, now, start=0, num=1)
        if not items:
            return None
        await self.redis.zrem("batch_queue", items[0])
        return json.loads(items[0])
```

---

## Fichier : `backend\altiora\infrastructure\repositories\base_repository.py`

```python
# backend/altiora/infrastructure/repositories/base_repository.py
from abc import ABC, abstractmethod
from typing import Generic, TypeVar, Optional

T = TypeVar('T') # Type g√©n√©rique pour l'entit√© g√©r√©e par le d√©p√¥t.


class BaseRepository(ABC, Generic[T]):
    """Classe abstraite de base pour tous les d√©p√¥ts (repositories).

    D√©finit l'interface CRUD (Create, Read, Update, Delete) que tout d√©p√¥t
    doit impl√©menter pour interagir avec une source de donn√©es sp√©cifique
    (base de donn√©es, syst√®me de fichiers, API externe, etc.).
    """

    @abstractmethod
    async def create(self, entity: T) -> T:
        """Cr√©e une nouvelle entit√© dans la source de donn√©es."

        Args:
            entity: L'entit√© √† cr√©er.

        Returns:
            L'entit√© cr√©√©e, potentiellement avec des champs mis √† jour (ex: ID g√©n√©r√©).
        """
        pass

    @abstractmethod
    async def get(self, id: str) -> Optional[T]:
        """R√©cup√®re une entit√© par son identifiant unique."

        Args:
            id: L'identifiant unique de l'entit√©.

        Returns:
            L'entit√© si trouv√©e, sinon None.
        """
        pass

    @abstractmethod
    async def update(self, id: str, entity: T) -> T:
        """Met √† jour une entit√© existante dans la source de donn√©es."

        Args:
            id: L'identifiant unique de l'entit√© √† mettre √† jour.
            entity: L'entit√© avec les donn√©es mises √† jour.

        Returns:
            L'entit√© mise √† jour.
        """
        pass

    @abstractmethod
    async def delete(self, id: str) -> bool:
        """Supprime une entit√© de la source de donn√©es par son identifiant."

        Args:
            id: L'identifiant unique de l'entit√© √† supprimer.

        Returns:
            True si l'entit√© a √©t√© supprim√©e avec succ√®s, False sinon.
        """
        pass
```

---

## Fichier : `backend\altiora\infrastructure\repositories\scenario_repository.py`

```python
# backend/altiora/infrastructure/repositories/scenario_repository.py
from pathlib import Path
from typing import Optional, List
import json
import logging

from src.repositories.base_repository import BaseRepository
from src.models.test_scenario import TestScenario

logger = logging.getLogger(__name__)


class ScenarioRepository(BaseRepository[TestScenario]):
    """D√©p√¥t pour la persistance des objets `TestScenario` sur le syst√®me de fichiers.

    Chaque sc√©nario est stock√© comme un fichier JSON individuel dans un r√©pertoire sp√©cifi√©.
    """

    def __init__(self, storage_path: Path):
        """Initialise le d√©p√¥t de sc√©narios."

        Args:
            storage_path: Le chemin du r√©pertoire o√π les fichiers JSON des sc√©narios seront stock√©s.
        """
        self.storage_path = storage_path
        self.storage_path.mkdir(parents=True, exist_ok=True) # Cr√©e le r√©pertoire si n√©cessaire.

    async def create(self, scenario: TestScenario) -> TestScenario:
        """Cr√©e un nouveau fichier JSON pour un sc√©nario de test."

        Args:
            scenario: L'objet `TestScenario` √† persister.

        Returns:
            L'objet `TestScenario` qui a √©t√© persist√©.

        Raises:
            ValueError: Si un sc√©nario avec le m√™me ID existe d√©j√†.
            IOError: En cas d'erreur lors de l'√©criture du fichier.
        """
        file_path = self.storage_path / f"{scenario.id}.json"
        if file_path.exists():
            raise ValueError(f"Un sc√©nario avec l'ID {scenario.id} existe d√©j√†.")
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(scenario.model_dump(), f, ensure_ascii=False, indent=4)
            logger.info(f"Sc√©nario cr√©√© : {file_path}")
            return scenario
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de la cr√©ation du fichier de sc√©nario {file_path}: {e}")
            raise IOError(f"Erreur lors de la cr√©ation du fichier de sc√©nario {file_path}: {e}") from e

    async def get(self, id: str) -> Optional[TestScenario]:
        """R√©cup√®re un sc√©nario de test par son ID."

        Args:
            id: L'ID du sc√©nario √† r√©cup√©rer.

        Returns:
            L'objet `TestScenario` si trouv√©, sinon None.

        Raises:
            IOError: En cas d'erreur lors de la lecture ou du parsing du fichier.
        """
        file_path = self.storage_path / f"{id}.json"
        if not file_path.exists():
            return None
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            logger.info(f"Sc√©nario r√©cup√©r√© : {file_path}")
            return TestScenario(**data)
        except (IOError, OSError, json.JSONDecodeError) as e:
            logger.error(f"Erreur lors de la lecture du fichier de sc√©nario {file_path}: {e}")
            raise IOError(f"Erreur lors de la lecture du fichier de sc√©nario {file_path}: {e}") from e

    async def update(self, id: str, scenario: TestScenario) -> TestScenario:
        """Met √† jour un sc√©nario de test existant."

        Args:
            id: L'ID du sc√©nario √† mettre √† jour.
            scenario: L'objet `TestScenario` avec les donn√©es mises √† jour.

        Returns:
            L'objet `TestScenario` mis √† jour.

        Raises:
            FileNotFoundError: Si le sc√©nario avec l'ID sp√©cifi√© n'existe pas.
            IOError: En cas d'erreur lors de l'√©criture du fichier.
        """
        file_path = self.storage_path / f"{id}.json"
        if not file_path.exists():
            raise FileNotFoundError(f"Sc√©nario avec l'ID {id} non trouv√©.")
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(scenario.model_dump(), f, ensure_ascii=False, indent=4)
            logger.info(f"Sc√©nario mis √† jour : {file_path}")
            return scenario
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de la mise √† jour du fichier de sc√©nario {file_path}: {e}")
            raise IOError(f"Erreur lors de la mise √† jour du fichier de sc√©nario {file_path}: {e}") from e

    async def delete(self, id: str) -> bool:
        """Supprime un sc√©nario de test par son ID."

        Args:
            id: L'ID du sc√©nario √† supprimer.

        Returns:
            True si le sc√©nario a √©t√© supprim√©, False sinon.
        """
        file_path = self.storage_path / f"{id}.json"
        if file_path.exists():
            try:
                file_path.unlink() # Supprime le fichier.
                logger.info(f"Sc√©nario supprim√© : {file_path}")
                return True
            except (IOError, OSError) as e:
                logger.error(f"Erreur lors de la suppression du fichier de sc√©nario {file_path}: {e}")
                return False
        return False

    async def get_all(self) -> List[TestScenario]:
        """R√©cup√®re tous les sc√©narios de test stock√©s."

        Returns:
            Une liste de tous les objets `TestScenario` trouv√©s.
        """
        scenarios = []
        for file_path in self.storage_path.glob("*.json"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                scenarios.append(TestScenario(**data))
            except (IOError, OSError, json.JSONDecodeError) as e:
                logger.warning(f"Erreur lors de la lecture ou du parsing du fichier de sc√©nario {file_path}: {e}")
        logger.info(f"R√©cup√©r√© {len(scenarios)} sc√©narios.")
        return scenarios


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging
    import uuid

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        temp_storage_path = Path("temp_scenario_storage")
        # Nettoyage du r√©pertoire de d√©monstration.
        if temp_storage_path.exists():
            import shutil
            shutil.rmtree(temp_storage_path)
        temp_storage_path.mkdir()

        repo = ScenarioRepository(temp_storage_path)

        print("\n--- Cr√©ation de sc√©narios ---")
        scenario1_id = "SCN-001"
        scenario1 = TestScenario(
            id=scenario1_id,
            title="Connexion r√©ussie",
            objective="V√©rifier la connexion avec des identifiants valides.",
            steps=["Entrer email", "Entrer mot de passe", "Cliquer sur login"],
            expected_result="Redirection vers le tableau de bord."
        )
        created_scenario1 = await repo.create(scenario1)
        print(f"Cr√©√© : {created_scenario1.id}")

        scenario2_id = "SCN-002"
        scenario2 = TestScenario(
            id=scenario2_id,
            title="Mot de passe oubli√©",
            objective="V√©rifier le processus de r√©cup√©ration de mot de passe.",
            steps=["Cliquer sur mot de passe oubli√©", "Entrer email", "Soumettre"],
            expected_result="Email de r√©initialisation envoy√©."
        )
        created_scenario2 = await repo.create(scenario2)
        print(f"Cr√©√© : {created_scenario2.id}")

        print("\n--- R√©cup√©ration d'un sc√©nario ---")
        retrieved_scenario = await repo.get(scenario1_id)
        if retrieved_scenario:
            print(f"R√©cup√©r√© : {retrieved_scenario.title}")
        else:
            print(f"Sc√©nario {scenario1_id} non trouv√©.")

        print("\n--- Mise √† jour d'un sc√©nario ---")
        scenario1.objective = "V√©rifier la connexion et la d√©connexion."
        updated_scenario = await repo.update(scenario1_id, scenario1)
        print(f"Mis √† jour : {updated_scenario.objective}")

        print("\n--- R√©cup√©ration de tous les sc√©narios ---")
        all_scenarios = await repo.get_all()
        print(f"Tous les sc√©narios ({len(all_scenarios)}) : {[s.id for s in all_scenarios]}")

        print("\n--- Suppression d'un sc√©nario ---")
        deleted = await repo.delete(scenario2_id)
        print(f"Sc√©nario {scenario2_id} supprim√© : {deleted}")

        all_scenarios_after_delete = await repo.get_all()
        print(f"Sc√©narios restants ({len(all_scenarios_after_delete)}) : {[s.id for s in all_scenarios_after_delete]}")

        print("D√©monstration termin√©e. Nettoyage du r√©pertoire temporaire.")
        import shutil
        shutil.rmtree(temp_storage_path)

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\repositories\__init__.py`

```python
# backend/altiora/infrastructure/repositories/__init__.py
"""Initialise le package `repositories` de l'application Altiora.

Ce package contient les d√©p√¥ts (repositories) qui g√®rent l'acc√®s aux donn√©es
persistantes de l'application. Les d√©p√¥ts abstraient la logique de stockage
sous-jacente, permettant aux autres parties de l'application d'interagir avec
les donn√©es de mani√®re coh√©rente, quel que soit le type de base de donn√©es ou de syst√®me de fichiers utilis√©.

Les modules suivants sont expos√©s pour faciliter les importations :
- `BaseRepository`: La classe abstraite de base pour tous les d√©p√¥ts.
- `ScenarioRepository`: Le d√©p√¥t sp√©cifique pour la gestion des sc√©narios de test.
"""
from .base_repository import BaseRepository
from .scenario_repository import ScenarioRepository

__all__ = ['BaseRepository', 'ScenarioRepository']

```

---

## Fichier : `backend\altiora\infrastructure\scaling\auto_scaler.py`

```python
# backend/altiora/infrastructure/scaling/auto_scaler.py
"""Module impl√©mentant un auto-scaler intelligent bas√© sur l'apprentissage automatique.

Ce module fournit une logique de d√©cision pour l'ajustement dynamique des
ressources (mise √† l'√©chelle horizontale ou verticale) en fonction de la
charge pr√©dite. Il utilise des m√©triques collect√©es et un pr√©dicteur de charge
pour anticiper les besoins et optimiser l'utilisation des ressources.
"""

import logging
from enum import Enum
from typing import Any, Dict

logger = logging.getLogger(__name__)


# --- Classes factices pour la d√©monstration et la documentation --- #
class ScaleAction(Enum):
    """Actions de mise √† l'√©chelle possibles."""
    SCALE_UP = "scale_up"
    SCALE_DOWN = "scale_down"
    NO_CHANGE = "no_change"


class MetricsCollector:
    """Collecteur de m√©triques factice."""
    async def get_current(self) -> Dict[str, Any]:
        """Simule la r√©cup√©ration des m√©triques actuelles."""
        logger.info("Collecte des m√©triques actuelles...")
        await asyncio.sleep(0.1) # Simule un d√©lai.
        return {"cpu_usage": 0.6, "memory_usage": 0.7, "request_rate": 150}


class LoadPredictor:
    """Pr√©dicteur de charge factice bas√© sur l'apprentissage automatique."""
    async def predict_next_hour(self, current_metrics: Dict[str, Any]) -> float:
        """Simule la pr√©diction de la charge pour la prochaine heure."""
        logger.info(f"Pr√©diction de la charge pour la prochaine heure bas√©e sur : {current_metrics}")
        await asyncio.sleep(0.2) # Simule un d√©lai.
        # Logique de pr√©diction factice.
        if current_metrics.get("request_rate", 0) > 100:
            return 0.9 # Charge √©lev√©e.
        return 0.4 # Charge faible.


# --- Auto-scaler intelligent --- #
class IntelligentAutoScaler:
    """D√©cide des actions de mise √† l'√©chelle bas√©es sur les m√©triques et la pr√©diction de charge."""

    def __init__(self, high_threshold: float = 0.8, low_threshold: float = 0.3):
        """Initialise l'auto-scaler intelligent."

        Args:
            high_threshold: Seuil de charge au-del√† duquel une mise √† l'√©chelle ascendante est d√©clench√©e.
            low_threshold: Seuil de charge en dessous duquel une mise √† l'√©chelle descendante est d√©clench√©e.
        """
        self.metrics_collector = MetricsCollector()
        self.predictor = LoadPredictor()
        self.high_threshold = high_threshold
        self.low_threshold = low_threshold
        logger.info(f"IntelligentAutoScaler initialis√©. Seuil haut: {high_threshold}, Seuil bas: {low_threshold}.")

    async def scale_decision(self) -> ScaleAction:
        """Prend une d√©cision de mise √† l'√©chelle bas√©e sur les m√©triques actuelles et la charge pr√©dite."

        Returns:
            Une `ScaleAction` indiquant si le syst√®me doit monter en charge, descendre en charge, ou rester stable.
        """
        logger.info("Prise de d√©cision de mise √† l'√©chelle...")
        current_metrics = await self.metrics_collector.get_current()
        predicted_load = await self.predictor.predict_next_hour(current_metrics)

        logger.info(f"Charge pr√©dite pour la prochaine heure : {predicted_load:.2f}")

        if predicted_load > self.high_threshold:
            logger.info("D√©cision : Mise √† l'√©chelle ascendante (SCALE_UP).")
            return ScaleAction.SCALE_UP
        elif predicted_load < self.low_threshold:
            logger.info("D√©cision : Mise √† l'√©chelle descendante (SCALE_DOWN).")
            return ScaleAction.SCALE_DOWN
        else:
            logger.info("D√©cision : Pas de changement (NO_CHANGE).")
            return ScaleAction.NO_CHANGE


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        print("\n--- D√©monstration de l'IntelligentAutoScaler ---")
        scaler = IntelligentAutoScaler(high_threshold=0.7, low_threshold=0.4)

        print("\nPremi√®re d√©cision de scaling...")
        decision1 = await scaler.scale_decision()
        print(f"D√©cision prise : {decision1.value}")

        # Simule une charge √©lev√©e pour forcer un SCALE_UP.
        class MockHighLoadPredictor(LoadPredictor):
            async def predict_next_hour(self, current_metrics: Dict[str, Any]) -> float:
                return 0.9
        scaler.predictor = MockHighLoadPredictor()

        print("\nDeuxi√®me d√©cision de scaling (charge √©lev√©e simul√©e)...")
        decision2 = await scaler.scale_decision()
        print(f"D√©cision prise : {decision2.value}")

        # Simule une charge faible pour forcer un SCALE_DOWN.
        class MockLowLoadPredictor(LoadPredictor):
            async def predict_next_hour(self, current_metrics: Dict[str, Any]) -> float:
                return 0.2
        scaler.predictor = MockLowLoadPredictor()

        print("\nTroisi√®me d√©cision de scaling (charge faible simul√©e)...")
        decision3 = await scaler.scale_decision()
        print(f"D√©cision prise : {decision3.value}")

        print("D√©monstration de l'auto-scaler termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\infrastructure\scaling\__init__.py`

```python
# backend/altiora/infrastructure/scaling/__init__.py
"""
M√©canismes de scaling horizontal / vertical.
"""
```

---

## Fichier : `backend\altiora\monitoring\memory_monitor.py`

```python
# backend/altiora/monitoring/memory_monitor.py
"""Module de surveillance de l'utilisation de la m√©moire."""
class MemoryMonitor:
    """Surveille l'utilisation m√©moire en temps r√©el."""

    @staticmethod
    def get_status() -> Dict:
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()

        return {
            "ram": {
                "used_gb": memory.used / (1024 ** 3),
                "total_gb": memory.total / (1024 ** 3),
                "percent": memory.percent,
                "available_gb": memory.available / (1024 ** 3)
            },
            "swap": {
                "used_gb": swap.used / (1024 ** 3),
                "total_gb": swap.total / (1024 ** 3),
                "percent": swap.percent
            },
            "alert": memory.percent > 85  # Alerte si > 85%
        }
```

---

## Fichier : `backend\altiora\security\secret.py`

```python
# backend/altiora/security/secret.py
"""Gestionnaire de secrets ultra-s√©curis√© pour l'application Altiora.

Ce module fournit une classe `SecretsManager` qui g√®re l'acc√®s aux secrets
de l'application de mani√®re s√©curis√©e. Il charge les secrets uniquement
depuis les variables d'environnement, effectue une validation stricte au
d√©marrage et peut g√©n√©rer des cl√©s al√©atoires pour faciliter la configuration.
"""

import os
import secrets
import logging
from typing import Optional, Dict
from cryptography.fernet import Fernet
from dotenv import load_dotenv
from pathlib import Path

logger = logging.getLogger(__name__)

# Charge les variables d'environnement depuis un fichier .env si pr√©sent.
# Cela doit √™tre fait au d√©but de l'ex√©cution de l'application.
load_dotenv()


class SecretsManager:
    """Gestionnaire singleton s√©curis√© pour tous les secrets de l'application.

    Il est recommand√© d'acc√©der aux secrets via cette classe pour garantir
    une gestion coh√©rente et s√©curis√©e.
    """

    # Liste des secrets requis et leur description pour la documentation.
    REQUIRED_SECRETS: Dict[str, str] = {
        "JWT_SECRET_KEY": "Cl√© secr√®te pour la signature des jetons JWT (minimum 32 caract√®res).",
        "ENCRYPTION_KEY": "Cl√© de chiffrement Fernet (doit √™tre une cl√© Fernet valide encod√©e en base64 URL-safe).",
        "OLLAMA_API_KEY": "Cl√© API pour l'acc√®s √† Ollama (optionnelle, si Ollama n√©cessite une authentification).",
        "OPENAI_API_KEY": "Cl√© API OpenAI pour les services de mod√©ration ou autres (optionnelle).",
        "AZURE_CONTENT_SAFETY_KEY": "Cl√© Azure Content Safety pour la mod√©ration de contenu (optionnelle).",
    }

    def __init__(self, secrets_dir: Path = Path("secrets")):
        """Initialise le gestionnaire de secrets."

        Args:
            secrets_dir: Le r√©pertoire o√π les secrets pourraient √™tre stock√©s (non utilis√© directement pour le chargement).
        """
        self.secrets_dir = secrets_dir
        self.secrets_dir.mkdir(parents=True, exist_ok=True)

    @classmethod
    def get_secret(cls, key: str, required: bool = True) -> str:
        """R√©cup√®re un secret depuis les variables d'environnement."

        Args:
            key: Le nom de la variable d'environnement contenant le secret.
            required: Si True, l√®ve une `ValueError` si le secret est manquant.

        Returns:
            La valeur du secret sous forme de cha√Æne de caract√®res.

        Raises:
            ValueError: Si le secret est requis mais non trouv√©.
        """
        value = os.getenv(key)
        if required and not value:
            raise ValueError(f"Secret manquant : `{key}`. Description : {cls.REQUIRED_SECRETS.get(key, 'N/A')}")
        return value or ""

    @classmethod
    def validate_secrets(cls) -> None:
        """Effectue une validation stricte de tous les secrets requis au d√©marrage de l'application."

        Cette m√©thode v√©rifie la pr√©sence et le format des secrets critiques.

        Raises:
            RuntimeError: Si des erreurs de validation sont trouv√©es, avec une liste d√©taill√©e.
        """
        errors: List[str] = []

        # Validation de la cl√© secr√®te JWT.
        try:
            jwt_key = cls.get_secret("JWT_SECRET_KEY")
            if jwt_key and len(jwt_key) < 32:
                errors.append("JWT_SECRET_KEY doit contenir au moins 32 caract√®res pour √™tre s√©curis√©.")
        except ValueError as e:
            errors.append(str(e))

        # Validation du format de la cl√© de chiffrement Fernet.
        try:
            encryption_key = cls.get_secret("ENCRYPTION_KEY")
            if encryption_key:
                try:
                    Fernet(encryption_key.encode())
                except Exception:
                    errors.append("ENCRYPTION_KEY est invalide. Elle doit √™tre une cl√© Fernet valide (32 octets encod√©s en base64 URL-safe).")
        except ValueError as e:
            errors.append(str(e))

        if errors:
            raise RuntimeError("Erreurs de validation des secrets d√©tect√©es :\n" + "\n".join(errors))
        logger.info("‚úÖ Tous les secrets critiques ont √©t√© valid√©s avec succ√®s.")

    @classmethod
    def generate_secret_key(cls, length: int = 64) -> str:
        """G√©n√®re une cl√© secr√®te al√©atoire et s√©curis√©e."

        Args:
            length: La longueur du secret en octets (la cha√Æne r√©sultante sera plus longue).

        Returns:
            Une cha√Æne de caract√®res s√©curis√©e.
        """
        return secrets.token_urlsafe(length)

    @classmethod
    def generate_fernet_key(cls) -> str:
        """G√©n√®re une cl√© Fernet valide (32 octets encod√©s en base64 URL-safe)."

        Returns:
            Une cha√Æne de caract√®res repr√©sentant une cl√© Fernet.
        """
        return Fernet.generate_key().decode()

    @classmethod
    def generate_missing_secrets_and_prompt(cls) -> None:
        """G√©n√®re automatiquement les secrets manquants et invite l'utilisateur √† les ajouter √† .env."

        Cette m√©thode est utile pour la configuration initiale ou le d√©veloppement.
        Elle ne modifie pas directement le fichier .env, mais affiche les secrets √† ajouter.
        """
        logger.info("\n--- G√©n√©ration des secrets manquants ---")
        generated_count = 0
        for key, description in cls.REQUIRED_SECRETS.items():
            if not os.getenv(key):
                if key == "ENCRYPTION_KEY":
                    value = cls.generate_fernet_key()
                else:
                    value = cls.generate_secret_key()
                logger.info(f"‚ö†Ô∏è  Secret g√©n√©r√© pour `{key}` : `{value}`")
                logger.info(f"   Description : {description}")
                logger.info(f"   Ajoutez cette ligne √† votre fichier `.env` :\n   {key}={value}")
                generated_count += 1
        
        if generated_count > 0:
            logger.info("\nüîí N'oubliez pas d'ajouter ces lignes √† votre fichier `.env` et de le garder hors de votre d√©p√¥t Git !")
        else:
            logger.info("‚úÖ Aucun secret manquant √† g√©n√©rer. Tous les secrets requis semblent √™tre d√©finis.")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    print("\n--- D√©monstration du SecretsManager ---")

    # Nettoie les variables d'environnement pour une d√©monstration propre.
    for key in SecretsManager.REQUIRED_SECRETS.keys():
        if key in os.environ:
            del os.environ[key]

    # 1. G√©n√©ration et affichage des secrets manquants.
    SecretsManager.generate_missing_secrets_and_prompt()

    # 2. Simule la d√©finition de secrets dans l'environnement.
    os.environ["JWT_SECRET_KEY"] = SecretsManager.generate_secret_key()
    os.environ["ENCRYPTION_KEY"] = SecretsManager.generate_fernet_key()
    os.environ["OLLAMA_API_KEY"] = "sk-ollama123"

    print("\n--- Tentative de validation des secrets ---")
    try:
        SecretsManager.validate_secrets()
        print("‚úÖ Validation des secrets r√©ussie apr√®s d√©finition.")
    except RuntimeError as e:
        logging.error(f"‚ùå Erreur de validation des secrets : {e}")

    print("\n--- R√©cup√©ration d'un secret ---")
    try:
        jwt_secret = SecretsManager.get_secret("JWT_SECRET_KEY")
        print(f"Secret JWT r√©cup√©r√© (partiel) : {jwt_secret[:10]}...")
    except ValueError as e:
        logging.error(f"‚ùå Erreur lors de la r√©cup√©ration du secret : {e}")

    print("D√©monstration du SecretsManager termin√©e.")

```

---

## Fichier : `backend\altiora\security\auth\dependencies.py`

```python
# backend/altiora/security/auth/dependencies.py
"""
D√©pendances FastAPI d‚Äôauthentification.
"""
# D√©j√† inclus dans altiora/api/dependencies.py
```

---

## Fichier : `backend\altiora\security\auth\jwt_handler.py`

```python
# backend/altiora/security/auth/jwt_handler.py
"""
Gestion des tokens JWT (encode / decode).
"""

from __future__ import annotations

from datetime import datetime, timedelta
from typing import Any

import jwt
from jwt import DecodeError, ExpiredSignatureError

from altiora.config.settings import settings


def create_access_token(
    subject: str,
    scopes: list[str] | None = None,
    expires_delta: timedelta | None = None,
) -> str:
    """Cr√©e un token JWT."""
    delta = expires_delta or timedelta(hours=settings.jwt_expiration_hours)
    payload = {
        "exp": datetime.utcnow() + delta,
        "iat": datetime.utcnow(),
        "sub": subject,
        "scopes": scopes or [],
    }
    return jwt.encode(payload, settings.jwt_secret_key, algorithm=settings.jwt_algorithm)


def decode_access_token(token: str) -> dict[str, Any]:
    """D√©code et valide un token JWT."""
    try:
        return jwt.decode(
            token,
            settings.jwt_secret_key,
            algorithms=[settings.jwt_algorithm],
        )
    except ExpiredSignatureError as exc:
        raise ValueError("Token expir√©") from exc
    except DecodeError as exc:
        raise ValueError("Token invalide") from exc
```

---

## Fichier : `backend\altiora\security\auth\__init__.py`

```python
# backend/altiora/security/auth/__init__.py
"""
Syst√®me d‚Äôauthentification JWT.
"""
```

---

## Fichier : `backend\altiora\security\guardrails\admin_control_system.py`

```python
# backend/altiora/security/guardrails/admin_control_system.py
"""
Syst√®me de Contr√¥le Administrateur pour Altiora
Admin unique avec contr√¥le total et monitoring en temps r√©el
"""

import json
import logging
import secrets
import shutil
import zipfile
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any

from cryptography.fernet import Fernet


@dataclass
class AdminCommand:
    """Structure d'une commande administrative"""
    command_id: str
    timestamp: datetime
    action: str
    target_user: str
    parameters: Dict[str, Any]
    executed_by: str = "admin"
    approved: bool = False
    rollback_data: Optional[Dict] = None


class AdminControlSystem:
    """Syst√®me de contr√¥le administrateur centralis√©"""

    def __init__(self, encryption_key: bytes = None):
        self.encryption_key = encryption_key or Fernet.generate_key()
        self.cipher = Fernet(self.encryption_key)

        self.admin_path = Path("admin_system")
        self.admin_path.mkdir(exist_ok=True)
        
        # Cr√©er les sous-dossiers n√©cessaires
        (self.admin_path / "backups").mkdir(exist_ok=True)
        (self.admin_path / "logs").mkdir(exist_ok=True)
        (self.admin_path / "emergency").mkdir(exist_ok=True)

        self.logger = self._setup_logging()
        self.active_sessions = {}
        self.pending_changes = {}

        self.emergency_mode = False
        self.system_freeze = False
        self.config = self._load_admin_config()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def execute_admin_command(self, command: AdminCommand) -> Dict[str, Any]:
        if self.system_freeze and command.action not in ["unfreeze", "emergency"]:
            return {"status": "error", "message": "Syst√®me gel√© - contactez l'admin"}

        self.logger.info(f"Commande admin: {command.action} pour {command.target_user}")

        action_map = {
            "force_personality_change": self._force_personality_change,
            "reset_user_profile": self._reset_user_profile,
            "freeze_user": self._freeze_user,
            "emergency": self._emergency_mode,
            "view_logs": self._view_logs,
            "rollback": self._rollback_command,
        }

        handler = action_map.get(command.action)
        if handler is None:
            return {"status": "error", "message": "Commande inconnue"}

        return await handler(command)

    # ------------------------------------------------------------------
    # Command Handlers
    # ------------------------------------------------------------------

    async def _force_personality_change(self, command: AdminCommand) -> Dict[str, Any]:
        user_id = command.target_user
        changes = command.parameters.get("changes", {})

        backup = await self._backup_user_profile(user_id)
        success = await self._apply_personality_changes(user_id, changes)

        if success:
            self.logger.warning(f"Changement forc√© sur {user_id}: {changes}")
            command.rollback_data = backup
            self.pending_changes[command.command_id] = command
            return {
                "status": "success",
                "message": f"Changements appliqu√©s √† {user_id}",
                "backup_id": command.command_id,
            }

        return {"status": "error", "message": "√âchec de l'application"}

    async def _reset_user_profile(self, command: AdminCommand) -> Dict[str, Any]:
        user_id = command.target_user
        backup = await self._full_user_backup(user_id)
        await self._wipe_user_data(user_id)
        self.logger.critical(f"R√©initialisation compl√®te de {user_id}")
        return {
            "status": "success",
            "message": f"Profil {user_id} r√©initialis√©",
            "backup_path": backup,
        }

    async def _freeze_user(self, command: AdminCommand) -> Dict[str, Any]:
        user_id = command.target_user
        reason = command.parameters.get("reason", "Admin freeze")

        frozen_file = self.admin_path / "frozen_users.json"
        frozen = self._load_json(frozen_file)
        frozen[user_id] = {
            "reason": reason,
            "timestamp": datetime.now().isoformat(),
            "admin": command.executed_by,
        }

        self._save_json(frozen_file, frozen)
        return {"status": "success", "message": f"Utilisateur {user_id} gel√©"}

    async def _emergency_mode(self, _command: AdminCommand) -> Dict[str, Any]:
        self.emergency_mode = True
        self.system_freeze = True
        await self._emergency_backup()
        self.logger.critical("MODE URGENCE ACTIVE")
        return {
            "status": "success",
            "message": "Mode urgence activ√© - tous les profils sauvegard√©s",
        }

    async def _view_logs(self, command: AdminCommand) -> Dict[str, Any]:
        user_id = command.target_user
        days = command.parameters.get("days", 7)
        logs = await self._filter_logs(user_id, days)
        return {"status": "success", "logs": logs, "count": len(logs)}

    async def _rollback_command(self, command: AdminCommand) -> Dict[str, Any]:
        target_command_id = command.parameters.get("target_command_id")
        original_cmd = self.pending_changes.get(target_command_id)

        if not original_cmd or not original_cmd.rollback_data:
            return {"status": "error", "message": "Commande non trouv√©e"}

        await self._restore_from_backup(original_cmd.rollback_data)
        del self.pending_changes[target_command_id]
        return {"status": "success", "message": f"Rollback effectu√© pour {target_command_id}"}

    # ------------------------------------------------------------------
    # Implemented Methods (previously stubs)
    # ------------------------------------------------------------------

    async def _full_user_backup(self, user_id: str) -> str:
        """Sauvegarde compl√®te d'un utilisateur avec toutes ses donn√©es"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = self.admin_path / "backups" / user_id / timestamp
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # Collecter tous les fichiers utilisateur
        user_data_paths = [
            Path(f"user_data/{user_id}"),
            Path(f"altiora_core/{user_id}_profile.json"),
            Path(f"altiora_core/{user_id}_evolution.json"),
            Path(f"altiora_core/{user_id}_proposals.json"),
            Path(f"quiz_data/{user_id}_profile.json"),
        ]
        
        # Copier tous les fichiers existants
        for path in user_data_paths:
            if path.exists():
                if path.is_file():
                    dest_path = backup_dir / path.name
                    shutil.copy2(path, dest_path)
                    self.logger.info(f"Backed up: {path} -> {dest_path}")
                elif path.is_dir():
                    dest_path = backup_dir / path.name
                    shutil.copytree(path, dest_path, dirs_exist_ok=True)
                    self.logger.info(f"Backed up directory: {path} -> {dest_path}")
        
        # Cr√©er une archive ZIP
        zip_path = self.admin_path / "backups" / user_id / f"{timestamp}_full_backup.zip"
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_dir.rglob("*"):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(backup_dir))
        
        # Nettoyer le dossier temporaire
        shutil.rmtree(backup_dir)
        
        self.logger.info(f"Full backup completed for {user_id}: {zip_path}")
        return str(zip_path)

    async def _wipe_user_data(self, user_id: str) -> None:
        """Efface toutes les donn√©es utilisateur de mani√®re s√©curis√©e"""
        # Paths √† effacer
        paths_to_delete = [
            Path(f"user_data/{user_id}"),
            Path(f"altiora_core/{user_id}_profile.json"),
            Path(f"altiora_core/{user_id}_evolution.json"),
            Path(f"altiora_core/{user_id}_proposals.json"),
            Path(f"altiora_core/{user_id}.log"),
            Path(f"quiz_data/{user_id}_profile.json"),
        ]
        
        for path in paths_to_delete:
            if path.exists():
                if path.is_file():
                    # √âcraser avec des donn√©es al√©atoires avant suppression
                    try:
                        with open(path, 'wb') as f:
                            f.write(secrets.token_bytes(path.stat().st_size))
                        path.unlink()
                        self.logger.info(f"Securely deleted file: {path}")
                    except (IOError, OSError) as e:
                        self.logger.error(f"Error wiping file {path}: {e}")
                elif path.is_dir():
                    shutil.rmtree(path)
                    self.logger.info(f"Deleted directory: {path}")
        
        # Nettoyer aussi les caches Redis si disponibles
        # TODO: Impl√©menter nettoyage Redis
        
        self.logger.warning(f"All data wiped for user: {user_id}")

    async def _emergency_backup(self) -> None:
        """Sauvegarde d'urgence globale de tous les utilisateurs"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        emergency_dir = self.admin_path / "emergency" / f"backup_{timestamp}"
        emergency_dir.mkdir(parents=True, exist_ok=True)
        
        # Identifier tous les utilisateurs
        user_ids = set()
        
        # Scanner les diff√©rents r√©pertoires pour trouver les users
        for pattern in ["user_data/*", "altiora_core/*_profile.json", "quiz_data/*_profile.json"]:
            for path in Path(".").glob(pattern):
                if path.is_dir():
                    user_ids.add(path.name)
                elif path.is_file() and "_profile.json" in path.name:
                    user_id = path.stem.replace("_profile", "")
                    user_ids.add(user_id)
        
        self.logger.info(f"Starting emergency backup for {len(user_ids)} users")
        
        # Sauvegarder chaque utilisateur
        for user_id in user_ids:
            try:
                user_backup_dir = emergency_dir / user_id
                user_backup_dir.mkdir(exist_ok=True)
                
                # Copier toutes les donn√©es utilisateur
                await self._copy_user_data_to_backup(user_id, user_backup_dir)
                
            except Exception as e:
                self.logger.error(f"Failed to backup user {user_id}: {e}")
        
        # Cr√©er une archive globale
        final_zip = self.admin_path / "emergency" / f"emergency_backup_{timestamp}.zip"
        with zipfile.ZipFile(final_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in emergency_dir.rglob("*"):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(emergency_dir))
        
        # Nettoyer le dossier temporaire
        shutil.rmtree(emergency_dir)
        
        # Sauvegarder aussi l'√©tat du syst√®me
        system_state = {
            "timestamp": timestamp,
            "emergency_mode": self.emergency_mode,
            "system_freeze": self.system_freeze,
            "frozen_users": self._load_json(self.admin_path / "frozen_users.json"),
            "pending_changes": {k: v.__dict__ for k, v in self.pending_changes.items()},
            "config": self.config
        }
        
        state_file = self.admin_path / "emergency" / f"system_state_{timestamp}.json"
        self._save_json(state_file, system_state)
        
        self.logger.critical(f"Emergency backup completed: {final_zip}")

    async def _filter_logs(self, user_id: str, days: int) -> List[Dict[str, Any]]:
        """Filtre les logs par utilisateur et p√©riode"""
        cutoff_date = datetime.now() - timedelta(days=days)
        filtered_logs = []
        
        # Parcourir tous les fichiers de log
        log_patterns = [
            self.admin_path / "admin_commands.log",
            Path(f"altiora_core/{user_id}.log"),
            Path("logs") / f"{user_id}_*.log"
        ]
        
        for log_pattern in log_patterns:
            if log_pattern.exists() and log_pattern.is_file():
                try:
                    with open(log_pattern, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                # Parser les logs (format: timestamp - level - message)
                                if " - " in line:
                                    parts = line.strip().split(" - ", 2)
                                    if len(parts) >= 3:
                                        timestamp_str = parts[0]
                                        level = parts[1]
                                        message = parts[2]
                                        
                                        # Parser le timestamp
                                        try:
                                            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S,%f")
                                        except ValueError:
                                        timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                                        
                                        # Filtrer par date et utilisateur
                                        if timestamp >= cutoff_date and (user_id in message or user_id == "all"):
                                            filtered_logs.append({
                                                "timestamp": timestamp.isoformat(),
                                                "level": level,
                                                "message": message,
                                                "source": str(log_pattern.name)
                                            })
                            except Exception as e:
                                self.logger.debug(f"Error parsing log line: {e}")
                except (IOError, OSError) as e:
                    self.logger.error(f"Error reading log file {log_pattern}: {e}")
        
        # Trier par timestamp
        filtered_logs.sort(key=lambda x: x["timestamp"], reverse=True)
        
        return filtered_logs

    async def _restore_from_backup(self, backup_data: Dict[str, Any]) -> None:
        """Restaure depuis une sauvegarde"""
        backup_path = backup_data.get("backup_path")
        user_id = backup_data.get("user_id")
        
        if not backup_path or not Path(backup_path).exists():
            raise ValueError(f"Backup path invalid: {backup_path}")
        
        try:
            # Si c'est un fichier ZIP, extraire d'abord
            if backup_path.endswith('.zip'):
                extract_dir = Path(backup_path).parent / "temp_restore"
                extract_dir.mkdir(exist_ok=True)
                
                with zipfile.ZipFile(backup_path, 'r') as zipf:
                    zipf.extractall(extract_dir)
                
                # Restaurer depuis le dossier extrait
                await self._restore_user_data_from_directory(user_id, extract_dir)
                
                # Nettoyer
                shutil.rmtree(extract_dir)
            else:
                # Si c'est un fichier JSON encrypt√©
                if backup_path.endswith('.json.enc'):
                    with open(backup_path, 'rb') as f:
                        encrypted_data = f.read()
                    
                    decrypted_data = self.cipher.decrypt(encrypted_data)
                    user_data = json.loads(decrypted_data.decode())
                    
                    # Restaurer les donn√©es
                    await self._restore_user_profile(user_id, user_data)
            
            self.logger.info(f"Restored user {user_id} from backup: {backup_path}")
        except (IOError, OSError, zipfile.BadZipFile) as e:
            self.logger.error(f"Error restoring from backup {backup_path}: {e}")

    # ------------------------------------------------------------------
    # Helper methods for implementation
    # ------------------------------------------------------------------

    async def _copy_user_data_to_backup(self, user_id: str, backup_dir: Path) -> None:
        """Copie toutes les donn√©es d'un utilisateur vers un r√©pertoire de backup"""
        patterns = [
            (f"user_data/{user_id}", backup_dir / "user_data"),
            (f"altiora_core/{user_id}_*.json", backup_dir / "altiora_core"),
            (f"quiz_data/{user_id}_*.json", backup_dir / "quiz_data"),
        ]
        
        for pattern, dest_parent in patterns:
            for path in Path(".").glob(pattern):
                if path.exists():
                    dest_parent.mkdir(parents=True, exist_ok=True)
                    if path.is_file():
                        shutil.copy2(path, dest_parent / path.name)
                    elif path.is_dir():
                        shutil.copytree(path, dest_parent / path.name, dirs_exist_ok=True)

    async def _restore_user_data_from_directory(self, user_id: str, restore_dir: Path) -> None:
        """Restaure les donn√©es utilisateur depuis un r√©pertoire"""
        for item in restore_dir.rglob("*"):
            if item.is_file():
                # D√©terminer le chemin de destination
                relative_path = item.relative_to(restore_dir)
                dest_path = Path(".") / relative_path
                
                # Cr√©er les r√©pertoires parents si n√©cessaire
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Copier le fichier
                shutil.copy2(item, dest_path)

    async def _restore_user_profile(self, user_id: str, profile_data: Dict[str, Any]) -> None:
        """Restaure un profil utilisateur depuis des donn√©es JSON"""
        profile_path = Path(f"altiora_core/{user_id}_profile.json")
        profile_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(profile_path, 'w', encoding='utf-8') as f:
            json.dump(profile_data, f, indent=2, ensure_ascii=False)

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    @staticmethod
    def _load_json(path: Path) -> Dict[str, Any]:
        return json.loads(path.read_text()) if path.exists() else {}

    @staticmethod
    def _save_json(path: Path, data: Dict[str, Any]) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(data, indent=2, default=str))

    def _setup_logging(self) -> logging.Logger:
        logger = logging.getLogger("altiora_admin")
        logger.setLevel(logging.DEBUG)
        fh = logging.FileHandler(self.admin_path / "admin_commands.log")
        fh.setLevel(logging.INFO)
        ch = logging.StreamHandler()
        ch.setLevel(logging.ERROR)
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        logger.addHandler(fh)
        logger.addHandler(ch)
        return logger

    def _load_admin_config(self) -> Dict[str, Any]:
        return self._load_json(self.admin_path / "admin_config.json") or {
            "max_personality_changes_per_day": 5,
            "require_approval_for_major_changes": True,
            "emergency_contact": "admin@company.com",
            "auto_freeze_threshold": 10,
            "backup_retention_days": 30,
        }

    # ------------------------------------------------------------------
    # Public utilities
    # ------------------------------------------------------------------

    async def _backup_user_profile(self, user_id: str) -> Dict[str, Any]:
        timestamp = datetime.now().isoformat()
        backup_path = self.admin_path / "backups" / user_id
        backup_path.mkdir(parents=True, exist_ok=True)
        backup_file = backup_path / f"{timestamp}.json.enc"

        user_data = await self._get_user_data(user_id)
        encrypted = self.cipher.encrypt(json.dumps(user_data).encode())
        backup_file.write_bytes(encrypted)

        return {"user_id": user_id, "backup_path": str(backup_file), "timestamp": timestamp}

    async def _get_user_data(self, user_id: str) -> Dict[str, Any]:
        """R√©cup√®re toutes les donn√©es d'un utilisateur"""
        user_data = {
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "personality": {},
            "history": [],
            "preferences": {},
            "voice_profile": {}
        }
        
        try:
            # Charger le profil de personnalit√©
            profile_path = Path(f"altiora_core/{user_id}_profile.json")
            if profile_path.exists():
                with open(profile_path, 'r', encoding='utf-8') as f:
                    user_data["personality"] = json.load(f)
            
            # Charger l'historique d'√©volution
            evolution_path = Path(f"altiora_core/{user_id}_evolution.json")
            if evolution_path.exists():
                with open(evolution_path, 'r', encoding='utf-8') as f:
                    user_data["evolution_history"] = json.load(f)
            
            # Charger le profil du quiz
            quiz_path = Path(f"quiz_data/{user_id}_profile.json")
            if quiz_path.exists():
                with open(quiz_path, 'r', encoding='utf-8') as f:
                    quiz_data = json.load(f)
                    user_data["preferences"] = quiz_data.get("preferences", {})
                    user_data["voice_profile"] = quiz_data.get("vocal_profile", {})
        except (IOError, OSError, json.JSONDecodeError) as e:
            self.logger.error(f"Error getting user data for {user_id}: {e}")
        
        return user_data

    async def _apply_personality_changes(self, user_id: str, changes: Dict[str, Any]) -> bool:
        """Applique des changements de personnalit√© √† un utilisateur"""
        try:
            profile_path = Path(f"altiora_core/{user_id}_profile.json")
            
            # Charger le profil existant
            if profile_path.exists():
                try:
                    with open(profile_path, 'r', encoding='utf-8') as f:
                        profile = json.load(f)
                except (IOError, OSError, json.JSONDecodeError) as e:
                    self.logger.error(f"Error reading profile for {user_id}: {e}")
                    # Cr√©er un profil par d√©faut si le fichier est corrompu
                    profile = {
                        "user_id": user_id,
                        "traits": {},
                        "preferences": {},
                        "vocal_profile": {},
                        "behavioral_patterns": {},
                        "quiz_metadata": {"created_at": datetime.now().isoformat()}
                    }
            else:
                # Cr√©er un profil par d√©faut si inexistant
                profile = {
                    "user_id": user_id,
                    "traits": {},
                    "preferences": {},
                    "vocal_profile": {},
                    "behavioral_patterns": {},
                    "quiz_metadata": {"created_at": datetime.now().isoformat()}
                }
            
            # Appliquer les changements
            for key, value in changes.items():
                if key == "traits" and isinstance(value, dict):
                    profile.setdefault("traits", {}).update(value)
                elif key == "preferences" and isinstance(value, dict):
                    profile.setdefault("preferences", {}).update(value)
                else:
                    profile[key] = value
            
            # Sauvegarder le profil modifi√©
            profile_path.parent.mkdir(parents=True, exist_ok=True)
            with open(profile_path, 'w', encoding='utf-8') as f:
                json.dump(profile, f, indent=2, ensure_ascii=False)
            
            return True
            
        except (IOError, OSError) as e:
            self.logger.error(f"Failed to apply personality changes: {e}")
            return False

    def generate_report(self) -> Dict[str, Any]:
        return {
            "total_users": len(self.active_sessions),
            "pending_changes": len(self.pending_changes),
            "emergency_mode": self.emergency_mode,
            "system_freeze": self.system_freeze,
            "last_backup": self._get_last_backup_time(),
            "recent_commands": self._get_recent_commands(10),
        }

    def _get_last_backup_time(self) -> Optional[str]:
        backups_dir = self.admin_path / "backups"
        if not backups_dir.exists():
            return None
        latest = None
        for user_dir in backups_dir.iterdir():
            for backup_file in user_dir.glob("*.json.enc"):
                if not latest or backup_file.stat().st_mtime > latest.stat().st_mtime:
                    latest = backup_file
        return str(latest) if latest else None

    def _get_recent_commands(self, count: int) -> List[Dict[str, Any]]:
        """R√©cup√®re les commandes les plus r√©centes depuis les logs"""
        recent_commands = []
        log_file = self.admin_path / "admin_commands.log"
        
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # Prendre les derni√®res lignes
                for line in lines[-count:]:
                    if "Commande admin:" in line:
                        try:
                            parts = line.strip().split(" - ")
                            if len(parts) >= 4:
                                timestamp = parts[0]
                                message = parts[3]
                                # Extraire l'action et l'utilisateur
                                if "Commande admin:" in message:
                                    action_part = message.split("Commande admin:")[1].strip()
                                    action, user = action_part.split(" pour ")
                                    recent_commands.append({
                                        "timestamp": timestamp,
                                        "action": action,
                                        "user": user
                                    })
                        except Exception as e:
                            self.logger.debug(f"Error parsing log line: {e}")
        
        return recent_commands[:count]


# Utilitaire
def generate_admin_key() -> str:
    return secrets.token_urlsafe(32)
```

---

## Fichier : `backend\altiora\security\guardrails\admin_dashboard.py`

```python
# backend/altiora/security/guardrails/admin_dashboard.py
"""Dashboard administrateur pour supervision Altiora
Interface de contr√¥le visuelle et rapports en temps r√©el
"""

import asyncio
import tkinter as tk
from datetime import datetime
from tkinter import ttk, messagebox
from typing import Dict, Any, List

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# Importation des syst√®mes de contr√¥le r√©els
from guardrails.admin_control_system import AdminControlSystem, AdminCommand
from guardrails.ethical_safeguards import EthicalSafeguards, EthicalAlert


class AdminDashboard:
    """
    Interface de dashboard administrateur connect√©e aux syst√®mes r√©els.
    """

    def __init__(self) -> None:
        self.admin_system = AdminControlSystem()
        self.ethical_safeguards = EthicalSafeguards()

        self.root = tk.Tk()
        self.root.title("üéõÔ∏è Altiora Admin Dashboard")
        self.root.geometry("1400x900")

        self.stats_labels: Dict[str, ttk.Label] = {}
        self.user_listbox: tk.Listbox
        self.user_info_frame: ttk.LabelFrame
        self.alert_tree: ttk.Treeview
        self.user_filter: ttk.Combobox
        self.period_filter: ttk.Combobox
        self.log_text: tk.Text
        self.notebook: ttk.Notebook

        self.setup_styles()
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill="both", expand=True)

        self.create_overview_tab()
        self.create_user_management_tab()
        self.create_ethical_monitoring_tab()
        self.create_system_logs_tab()
        self.create_emergency_tab()
        self.start_auto_refresh()

    @staticmethod
    def setup_styles() -> None:
        style = ttk.Style()
        style.theme_use("clam")
        style.configure("TNotebook", background="#f0f0f0")
        style.configure("TFrame", background="#ffffff")
        style.configure("Critical.TLabel", foreground="red", font=("Arial", 10, "bold"))
        style.configure("Warning.TLabel", foreground="orange", font=("Arial", 10))
        style.configure("Success.TLabel", foreground="green", font=("Arial", 10))

    def create_overview_tab(self) -> None:
        overview_frame = ttk.Frame(self.notebook)
        self.notebook.add(overview_frame, text="Vue d'ensemble")

        header = ttk.Label(overview_frame, text="Tableau de Bord Altiora", font=("Arial", 16, "bold"))
        header.pack(pady=10)

        stats_frame = ttk.Frame(overview_frame)
        stats_frame.pack(pady=20, padx=20)
        for i, stat in enumerate(["Total Users", "Active Alerts", "Pending Changes", "System Status"]):
            frame = ttk.Frame(stats_frame)
            frame.grid(row=0, column=i, padx=20)
            ttk.Label(frame, text=stat, font=("Arial", 12)).pack()
            lbl = ttk.Label(frame, text="N/A", font=("Arial", 14, "bold"))
            lbl.pack()
            self.stats_labels[stat] = lbl

        chart_frame = ttk.Frame(overview_frame)
        chart_frame.pack(pady=20, padx=20, fill="both", expand=True)
        self.create_activity_chart(chart_frame)

    def create_user_management_tab(self) -> None:
        user_frame = ttk.Frame(self.notebook)
        self.notebook.add(user_frame, text="Gestion utilisateurs")

        left = ttk.Frame(user_frame)
        left.pack(side="left", fill="y", padx=10, pady=10)
        ttk.Label(left, text="Utilisateurs", font=("Arial", 12, "bold")).pack()
        self.user_listbox = tk.Listbox(left, width=30, height=25)
        self.user_listbox.pack(pady=10)
        self.user_listbox.bind("<<ListboxSelect>>", self.on_user_select)

        right = ttk.Frame(user_frame)
        right.pack(side="right", fill="both", expand=True, padx=10, pady=10)
        self.user_info_frame = ttk.LabelFrame(right, text="Informations")
        self.user_info_frame.pack(fill="x", pady=10)

        control = ttk.LabelFrame(right, text="Contr√¥les Admin")
        control.pack(fill="x", pady=10)
        ttk.Button(control, text="Geler Utilisateur", command=lambda: self.execute_user_action("freeze_user")).pack(pady=5, padx=10, fill="x")
        ttk.Button(control, text="Supprimer les donn√©es", command=lambda: self.execute_user_action("wipe_user_data")).pack(pady=5, padx=10, fill="x")

    def create_ethical_monitoring_tab(self) -> None:
        ethical_frame = ttk.Frame(self.notebook)
        self.notebook.add(ethical_frame, text="Monitoring √©thique")

        alerts_frame = ttk.LabelFrame(ethical_frame, text="Alertes Actives")
        alerts_frame.pack(fill="both", expand=True, padx=10, pady=10)

        self.alert_tree = ttk.Treeview(alerts_frame, columns=("User", "Severity", "Details", "Time"), show="headings")
        for col in ("User", "Severity", "Details", "Time"):
            self.alert_tree.heading(col, text=col)
        self.alert_tree.pack(fill="both", expand=True, pady=10)

    def create_system_logs_tab(self) -> None:
        logs_frame = ttk.Frame(self.notebook)
        self.notebook.add(logs_frame, text="Logs syst√®me")
        self.log_text = tk.Text(logs_frame, height=30, width=120, state="disabled")
        self.log_text.pack(fill="both", expand=True, padx=10, pady=10)

    def create_emergency_tab(self) -> None:
        emergency_frame = ttk.Frame(self.notebook)
        self.notebook.add(emergency_frame, text="URGENCE")
        ttk.Button(emergency_frame, text="üö® ACTIVER MODE URGENCE üö®", command=self.activate_emergency_mode).pack(pady=20)

    def create_activity_chart(self, parent) -> None:
        self.fig, self.ax = plt.subplots(figsize=(12, 4))
        self.canvas = FigureCanvasTkAgg(self.fig, parent)
        self.canvas.get_tk_widget().pack(fill="both", expand=True)

    def on_user_select(self, _event: tk.Event) -> None:
        selection = self.user_listbox.curselection()
        if selection:
            user_id = self.user_listbox.get(selection[0])
            self.display_user_info(user_id)

    def display_user_info(self, user_id: str) -> None:
        for widget in self.user_info_frame.winfo_children():
            widget.destroy()
        # Remplacer par un appel r√©el pour obtenir les infos utilisateur
        info = asyncio.run(self.admin_system.get_user_info(user_id))
        for key, value in info.items():
            ttk.Label(self.user_info_frame, text=f"{key}: {value}").pack(anchor="w", padx=10)

    def execute_user_action(self, action: str) -> None:
        selection = self.user_listbox.curselection()
        if not selection:
            messagebox.showwarning("S√©lection requise", "Veuillez s√©lectionner un utilisateur")
            return
        user_id = self.user_listbox.get(selection[0])
        if messagebox.askyesno("Confirmation", f"Confirmer l'action '{action}' sur {user_id} ?"):
            asyncio.run(self.admin_system.execute_admin_command(AdminCommand(
                command_id=f"{action}_{user_id}",
                action=action,
                target_user=user_id
            )))
            messagebox.showinfo("Action effectu√©e", f"L'action {action} a √©t√© ex√©cut√©e pour {user_id}.")

    def activate_emergency_mode(self) -> None:
        if messagebox.askyesno("URGENCE", "√ätes-vous s√ªr de vouloir activer le mode urgence ?", icon="warning"):
            asyncio.run(self.admin_system.trigger_emergency(reason="Manual trigger from dashboard"))
            messagebox.showinfo("URGENCE", "Mode urgence activ√©")

    def start_auto_refresh(self) -> None:
        self.refresh_data()
        self.root.after(5000, self.start_auto_refresh)

    def refresh_data(self) -> None:
        report = self.admin_system.generate_report()
        self.stats_labels["Total Users"].config(text=str(report.get("total_users", 0)))
        self.stats_labels["Active Alerts"].config(text=str(len(self.ethical_safeguards.get_active_alerts())))
        self.stats_labels["System Status"].config(text="NORMAL" if not report.get("emergency_mode") else "URGENCE")
        self.update_user_list()
        self.update_alert_display()
        self.update_logs()
        self.update_activity_chart()

    def update_user_list(self) -> None:
        current_selection = self.user_listbox.curselection()
        self.user_listbox.delete(0, tk.END)
        users = asyncio.run(self.admin_system.list_users())
        for user in users:
            self.user_listbox.insert(tk.END, user)
        if current_selection:
            self.user_listbox.selection_set(current_selection)

    def update_alert_display(self) -> None:
        self.alert_tree.delete(*self.alert_tree.get_children())
        alerts: List[EthicalAlert] = self.ethical_safeguards.get_active_alerts()
        for alert in alerts:
            self.alert_tree.insert("", "end", values=(alert.user_id, alert.severity.name, alert.details, alert.timestamp.strftime("%H:%M:%S")))

    def update_logs(self) -> None:
        logs = self.admin_system.get_system_logs(limit=100)
        self.log_text.config(state="normal")
        self.log_text.delete("1.0", tk.END)
        self.log_text.insert("1.0", "\n".join(logs))
        self.log_text.config(state="disabled")

    def update_activity_chart(self) -> None:
        self.ax.clear()
        # Simuler des donn√©es d'activit√© changeantes
        hours = list(range(24))
        activity = [np.random.randint(50, 200) + h * 5 for h in hours]
        self.ax.plot(hours, activity, marker="o")
        self.ax.set_title("Activit√© par heure")
        self.ax.set_xlabel("Heure")
        self.ax.set_ylabel("Interactions")
        self.canvas.draw()

    def run(self) -> None:
        self.root.mainloop()


if __name__ == "__main__":
    # Note: Pour ex√©cuter ce dashboard, les syst√®mes sous-jacents doivent √™tre fonctionnels.
    # Cette d√©mo est conceptuelle et peut n√©cessiter des ajustements.
    dashboard = AdminDashboard()
    dashboard.run()
```

---

## Fichier : `backend\altiora\security\guardrails\emergency_handler.py`

```python
# backend/altiora/security/guardrails/emergency_handler.py
"""
Emergency mode & alerting for Altiora
- Triggers: critical toxicity / PII leak / service crash
- Actions: freeze users, global backup, admin alerts
"""

import asyncio
import json
import logging
import aiohttp
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from guardrails.admin_control_system import AdminControlSystem
from guardrails.ethical_safeguards import EthicalAlert

logger = logging.getLogger(__name__)


class EmergencyHandler:
    """
    One-liner to enter/exit emergency mode:
    EmergencyHandler().trigger(reason="Critical toxicity spike")
    """

    def __init__(self):
        self.admin = AdminControlSystem()
        self.alert_webhooks = []  # Slack / Teams / email hooks
        self._load_webhooks()

    # ------------------------------------------------------------------
    # Triggers
    # ------------------------------------------------------------------
    async def trigger(
        self,
        *,
        reason: str,
        severity: str = "critical",  # low | medium | high | critical
        metadata: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        Activate emergency mode:
        1. Freeze all users (or subset)
        2. Emergency backup
        3. Notify admins
        """
        metadata = metadata or {}
        ts = datetime.utcnow().isoformat()

        # 1. Emergency backup
        backup_path = await self._emergency_backup()

        # 2. Freeze users
        frozen_users = await self._freeze_users(metadata.get("users", []))

        # 3. Admin notifications
        await self._notify_admins(reason, backup_path, frozen_users, metadata)

        # 4. Log
        self._log_emergency(reason, severity, backup_path, frozen_users)

        return {
            "emergency_id": ts,
            "reason": reason,
            "severity": severity,
            "backup_path": str(backup_path),
            "frozen_users": frozen_users,
        }

    # ------------------------------------------------------------------
    # Reset / exit emergency
    # ------------------------------------------------------------------
    async def reset(self) -> Dict[str, Any]:
        """Lift global freeze + send summary."""
        # Unfreeze all
        await self._unfreeze_all()
        # Notify
        await self._notify_admins("Emergency mode lifted", None, [], {})
        return {"status": "lifted", "timestamp": datetime.utcnow().isoformat()}

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    async def _emergency_backup(self) -> Path:
        """Call AdminControlSystem for full user backup."""
        cmd = {
            "command_id": f"emergency_{datetime.utcnow().timestamp()}",
            "action": "emergency",
            "target_user": "system",
            "parameters": {"reason": "Emergency trigger"},
        }
        result = await self.admin.execute_admin_command(cmd)
        return Path(result["backup_path"])

    async def _freeze_users(self, users: List[str]) -> List[str]:
        """Freeze provided users list (or all if empty)."""
        if not users:
            # TODO: list all active users
            users = ["all"]
        for uid in users:
            await self.admin.execute_admin_command(
                {
                    "command_id": f"freeze_{uid}_{datetime.utcnow().timestamp()}",
                    "action": "freeze_user",
                    "target_user": uid,
                    "parameters": {"reason": "Emergency"},
                }
            )
        return users

    async def _unfreeze_all(self):
        # Implementation depends on freeze storage
        pass

    async def _notify_admins(
        self,
        reason: str,
        backup_path: Optional[Path],
        frozen: List[str],
        meta: Dict[str, Any],
    ):
        payload = {
            "text": f"üö® Altiora Emergency\n"
            f"Reason: {reason}\n"
            f"Users frozen: {len(frozen)}\n"
            f"Backup: {backup_path}",
            "metadata": meta,
            "timestamp": datetime.utcnow().isoformat(),
        }

        # Slack / Teams / email webhooks
        for url in self.alert_webhooks:
            try:
                async with aiohttp.ClientSession() as session:
                    await session.post(url, json=payload, timeout=5)
            except aiohttp.ClientError as e:
                logger.warning("Webhook failed: %s", e)

    def _load_webhooks(self):
        """Load Slack / Teams / email URLs from config."""
        cfg_file = Path("configs/emergency_webhooks.json")
        if cfg_file.exists():
            self.alert_webhooks = json.loads(cfg_file.read_text()).get("webhooks", [])

    def _log_emergency(self, reason: str, severity: str, backup_path: Path, users: List[str]):
        log_file = Path("logs/emergency.jsonl")
        log_file.parent.mkdir(exist_ok=True)
        entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "severity": severity,
            "reason": reason,
            "backup": str(backup_path),
            "frozen_users": users,
        }
        with log_file.open("a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


# ------------------------------------------------------------------
# Quick CLI
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        handler = EmergencyHandler()
        result = await handler.trigger(
            reason="D√©tection massive de PII + toxicit√© critique",
            metadata={"users": ["mallory"], "count": 42},
        )
        print(json.dumps(result, ensure_ascii=False, indent=2))

    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\security\guardrails\ethical_safeguards.py`

```python
# backend/altiora/security/guardrails/ethical_safeguards.py
"""
Module de garde-fous √©thiques pour Altiora
D√©tection d'anomalies, limites comportementales, et alertes
"""

import asyncio
import re
import json
from collections import defaultdict, deque
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any

import numpy as np


@dataclass
class EthicalAlert:
    alert_id: str
    user_id: str
    timestamp: datetime
    severity: str
    alert_type: str
    description: str
    data: Dict
    action_taken: str = "none"
    resolved: bool = False
    admin_notified: bool = False


class EthicalSafeguards:
    def __init__(self):
        self.alerts: List[EthicalAlert] = []
        self.user_patterns = defaultdict(lambda: {
            "interactions": deque(maxlen=100),
            "personality_changes": deque(maxlen=20),
            "emotional_indicators": deque(maxlen=50),
            "dependency_score": 0.0
        })

        self.thresholds = {
            "dependency": {"low": 0.3, "medium": 0.5, "high": 0.7, "critical": 0.85},
            "manipulation": {"emotional_exploitation": 0.6, "pressure_tactics": 0.7, "guilt_inducing": 0.8},
            "privacy": {"excessive_data_collection": 0.8, "unauthorized_sharing": 1.0}
        }

        self.manipulation_patterns = [
            r"\b(d√©pend|besoin|impossible sans)\s+(toi|altiora)\b",
            r"\b(sans toi je|je ne peux pas)\s+(.*)\b",
            r"\b(tu es la seule personne|personne d'autre ne)\b"
        ]
        self.stress_indicators = [
            "urgent", "impossible", "d√©sesp√©r√©", "aidez-moi",
            "je suis perdu", "je n'y arrive pas", "trop difficile"
        ]
        
        # Cr√©er les r√©pertoires n√©cessaires
        self.safeguards_path = Path("ethical_safeguards")
        self.safeguards_path.mkdir(exist_ok=True)
        (self.safeguards_path / "alerts").mkdir(exist_ok=True)
        (self.safeguards_path / "user_states").mkdir(exist_ok=True)

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def analyze_interaction(self, user_id: str, interaction: Dict) -> Optional[EthicalAlert]:
        self.user_patterns[user_id]["interactions"].append(interaction)
        checks = [
            self._check_dependency(user_id),
            self._check_manipulation(user_id, interaction),
            self._check_privacy(user_id, interaction),
            self._check_emotional_state(user_id, interaction),
            self._check_personality_drift(user_id)
        ]
        for alert in checks:
            if alert:
                self.alerts.append(alert)
                await self._handle_alert(alert)
                return alert
        return None

    # ------------------------------------------------------------------
    # Private checkers
    # ------------------------------------------------------------------

    def _check_dependency(self, user_id: str) -> Optional[EthicalAlert]:
        user_data = self.user_patterns[user_id]
        score = self._calculate_dependency_score(user_data)
        user_data["dependency_score"] = score
        if score > self.thresholds["dependency"]["critical"]:
            return EthicalAlert(
                alert_id=f"dep_{user_id}_{datetime.now().isoformat()}",
                user_id=user_id,
                timestamp=datetime.now(),
                severity="critical",
                alert_type="excessive_dependency",
                description=f"D√©pendance critique d√©tect√©e: score {score:.2f}",
                data={
                    "dependency_score": float(score),
                    "interactions_count": len(user_data["interactions"]),
                    "last_24h": self._count_recent_interactions(user_id, 24)
                })
        return None

    def _calculate_dependency_score(self, user_data: Dict) -> float:
        interactions = list(user_data["interactions"])
        if len(interactions) < 10:
            return 0.0

        frequency = min(len(interactions) / 50.0, 1.0)

        dependency_words = 0
        total_words = 0
        for interaction in interactions:
            text = interaction.get("text", "").lower()
            total_words += len(text.split())
            dependency_words += sum(bool(re.search(p, text)) for p in self.manipulation_patterns)

        content_ratio = min(dependency_words / max(total_words, 1), 1.0)

        time_stamps = [i.get("timestamp", datetime.now()) for i in interactions]
        if len(time_stamps) > 1:
            intervals = [(time_stamps[i + 1] - time_stamps[i]).total_seconds()
                         for i in range(len(time_stamps) - 1)]
            avg_interval = float(np.mean(np.asarray(intervals, dtype=float)))
            frequency_factor = min(3600.0 / max(avg_interval, 3600.0), 1.0)
        else:
            frequency_factor = 0.0

        return float(frequency * 0.4 + content_ratio * 0.4 + frequency_factor * 0.2)

    def _check_manipulation(self, user_id: str, interaction: Dict) -> Optional[EthicalAlert]:
        text = interaction.get("text", "").lower()
        score = 0.0
        detected = []
        for pattern in self.manipulation_patterns:
            matches = re.findall(pattern, text)
            score += len(matches) * 0.3
            if matches:
                detected.append(pattern)

        if score > self.thresholds["manipulation"]["emotional_exploitation"]:
            return EthicalAlert(
                alert_id=f"manip_{user_id}_{datetime.now().isoformat()}",
                user_id=user_id,
                timestamp=datetime.now(),
                severity="high",
                alert_type="potential_manipulation",
                description="D√©tection de patterns de manipulation",
                data={
                    "manipulation_score": float(score),
                    "detected_patterns": detected,
                    "text": text[:100] + "..." if len(text) > 100 else text
                })
        return None

    def _check_privacy(self, user_id: str, interaction: Dict) -> Optional[EthicalAlert]:
        sensitive_patterns = [
            r"\b(mot de passe|password|mdp)\s*:?\s*(\w+)",
            r"\b(carte bancaire|num√©ro de carte)\s*:?\s*(\d+)",
            r"\b@([\w\.-]+\.\w+)",
            r"\b\d{2}[\s/-]?\d{2}[\s/-]?\d{2}[\s/-]?\d{3}[\s/-]?\d{3}\b"
        ]
        text = interaction.get("text", "")
        for pattern in sensitive_patterns:
            if re.findall(pattern, text, re.IGNORECASE):
                return EthicalAlert(
                    alert_id=f"privacy_{user_id}_{datetime.now().isoformat()}",
                    user_id=user_id,
                    timestamp=datetime.now(),
                    severity="medium",
                    alert_type="sensitive_data_detected",
                    description="Donn√©es sensibles potentiellement partag√©es",
                    data={
                        "data_type": self._identify_data_type(pattern),
                        "masked_content": self._mask_sensitive_data(text)
                    })
        return None

    def _check_emotional_state(self, user_id: str, interaction: Dict) -> Optional[EthicalAlert]:
        text = interaction.get("text", "").lower()
        distress_count = sum(1 for ind in self.stress_indicators if ind in text)
        if distress_count > 2:
            return EthicalAlert(
                alert_id=f"stress_{user_id}_{datetime.now().isoformat()}",
                user_id=user_id,
                timestamp=datetime.now(),
                severity="medium",
                alert_type="user_distress_detected",
                description="Signes de d√©tresse √©motionnelle d√©tect√©s",
                data={
                    "distress_indicators": distress_count,
                    "suggested_action": "empathetic_mode",
                    "keywords_found": [i for i in self.stress_indicators if i in text]
                })
        return None

    def _check_personality_drift(self, user_id: str) -> Optional[EthicalAlert]:
        changes = list(self.user_patterns[user_id]["personality_changes"])
        if len(changes) < 5:
            return None
        drift_score = sum(1 for c in changes[-5:] if c.get("magnitude", 0.0) > 0.5)
        if drift_score >= 3:
            return EthicalAlert(
                alert_id=f"drift_{user_id}_{datetime.now().isoformat()}",
                user_id=user_id,
                timestamp=datetime.now(),
                severity="medium",
                alert_type="personality_drift",
                description="Changements de personnalit√© suspects d√©tect√©s",
                data={
                    "recent_changes": len(changes[-5:]),
                    "high_magnitude_changes": drift_score,
                    "suggested_action": "review_changes"
                })
        return None

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------

    @staticmethod
    def _identify_data_type(pattern: str) -> str:
        if "password" in pattern.lower() or "mdp" in pattern.lower():
            return "password"
        if "carte" in pattern.lower():
            return "credit_card"
        if "@" in pattern:
            return "email"
        if r"\d{2}" in pattern:
            return "phone_number"
        return "unknown"

    @staticmethod
    def _mask_sensitive_data(text: str) -> str:
        # Masquer les mots de passe
        text = re.sub(r'(mot de passe|password|mdp)\s*:?\s*(\S+)', r'\1: ***', text, flags=re.IGNORECASE)
        # Masquer les num√©ros
        text = re.sub(r'\b\d{4,}\b', '****', text)
        # Masquer les emails
        text = re.sub(r'[\w\.-]+@[\w\.-]+\.\w+', '***@***.***', text)
        return text

    def _count_recent_interactions(self, user_id: str, hours: int) -> int:
        cutoff = datetime.now() - timedelta(hours=hours)
        return sum(
            1
            for i in self.user_patterns[user_id]["interactions"]
            if i.get("timestamp", datetime.now()) > cutoff
        )

    # ------------------------------------------------------------------
    # Actions & reports (implementations of previously stub methods)
    # ------------------------------------------------------------------

    async def _handle_alert(self, alert: EthicalAlert) -> None:
        """G√®re une alerte selon sa s√©v√©rit√©"""
        if alert.severity == "critical":
            alert.action_taken = "user_frozen"
            await self._freeze_user(alert.user_id)
        elif alert.severity == "high":
            alert.action_taken = "supervised_mode"
            await self._enable_supervised_mode(alert.user_id)
        elif alert.severity == "medium":
            alert.action_taken = "enhanced_monitoring"
        
        # Sauvegarder l'alerte
        await self._save_alert(alert)
        
        # Notifier l'admin si n√©cessaire
        if alert.severity in ["critical", "high"]:
            alert.admin_notified = True
            await self._notify_admin(alert)

    async def _freeze_user(self, user_id: str) -> None:
        """G√®le un utilisateur - bloque toutes ses interactions"""
        frozen_file = self.safeguards_path / "frozen_users.json"
        frozen = self._load_json(frozen_file)
        
        frozen[user_id] = {
            "frozen_at": datetime.now().isoformat(),
            "reason": "Ethical safeguard triggered",
            "auto_unfreeze": None,
            "manual_review_required": True
        }
        
        self._save_json(frozen_file, frozen)
        
        # Cr√©er un fichier de state pour l'utilisateur
        user_state_file = self.safeguards_path / "user_states" / f"{user_id}_state.json"
        user_state = {
            "user_id": user_id,
            "status": "frozen",
            "frozen_at": datetime.now().isoformat(),
            "dependency_score": self.user_patterns[user_id]["dependency_score"],
            "last_interaction": datetime.now().isoformat(),
            "freeze_reason": "Critical dependency threshold exceeded"
        }
        self._save_json(user_state_file, user_state)
        
        # Log l'action
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "freeze_user",
            "user_id": user_id,
            "automated": True
        }
        await self._append_to_log("user_actions.log", log_entry)

    async def _enable_supervised_mode(self, user_id: str) -> None:
        """Active le mode supervis√© pour un utilisateur"""
        supervised_file = self.safeguards_path / "supervised_users.json"
        supervised = self._load_json(supervised_file)
        
        supervised[user_id] = {
            "enabled_at": datetime.now().isoformat(),
            "level": "high",
            "triggers": ["manipulation_detected", "high_dependency"],
            "review_frequency": "daily",
            "auto_responses_disabled": True,
            "admin_approval_required": True
        }
        
        self._save_json(supervised_file, supervised)
        
        # Mettre √† jour l'√©tat utilisateur
        user_state_file = self.safeguards_path / "user_states" / f"{user_id}_state.json"
        user_state = self._load_json(user_state_file)
        user_state.update({
            "status": "supervised",
            "supervised_since": datetime.now().isoformat(),
            "supervision_level": "high"
        })
        self._save_json(user_state_file, user_state)
        
        # Log l'action
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "enable_supervised_mode",
            "user_id": user_id,
            "level": "high"
        }
        await self._append_to_log("user_actions.log", log_entry)

    async def _notify_admin(self, alert: EthicalAlert) -> None:
        """Notifie l'administrateur d'une alerte importante"""
        notifications_file = self.safeguards_path / "admin_notifications.json"
        notifications = self._load_json(notifications_file)
        
        notification = {
            "notification_id": f"notif_{datetime.now().timestamp()}",
            "timestamp": datetime.now().isoformat(),
            "alert_id": alert.alert_id,
            "user_id": alert.user_id,
            "severity": alert.severity,
            "alert_type": alert.alert_type,
            "description": alert.description,
            "requires_action": True,
            "status": "pending",
            "data": alert.data
        }
        
        notifications.setdefault("pending", []).append(notification)
        self._save_json(notifications_file, notifications)
        
        # Cr√©er aussi un fichier d'alerte individuel pour tra√ßabilit√©
        alert_file = self.safeguards_path / "alerts" / f"{alert.alert_id}.json"
        self._save_json(alert_file, asdict(alert))
        
        # Si c'est critique, cr√©er aussi un fichier d'urgence
        if alert.severity == "critical":
            urgent_file = self.safeguards_path / "URGENT_ALERT.json"
            urgent_data = {
                "alert": asdict(alert),
                "timestamp": datetime.now().isoformat(),
                "message": "CRITICAL ALERT - IMMEDIATE ADMIN ATTENTION REQUIRED",
                "user_frozen": alert.action_taken == "user_frozen"
            }
            self._save_json(urgent_file, urgent_data)
        
        # Log la notification
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "admin_notification",
            "alert_id": alert.alert_id,
            "severity": alert.severity
        }
        await self._append_to_log("notifications.log", log_entry)

    async def _save_alert(self, alert: EthicalAlert) -> None:
        """Sauvegarde une alerte dans le syst√®me"""
        alert_data = asdict(alert)
        alert_file = self.safeguards_path / "alerts" / f"{alert.alert_id}.json"
        self._save_json(alert_file, alert_data)

    async def _append_to_log(self, log_name: str, entry: Dict[str, Any]) -> None:
        """Ajoute une entr√©e au fichier de log"""
        log_file = self.safeguards_path / log_name
        log_line = json.dumps(entry, default=str) + "\n"
        
        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(log_line)
        except (IOError, OSError) as e:
            self.logger.error(f"Error writing to log file {log_file}: {e}")

    # ------------------------------------------------------------------
    # Helper methods for file operations
    # ------------------------------------------------------------------

    @staticmethod
    def _load_json(path: Path) -> Dict[str, Any]:
        """Charge un fichier JSON"""
        if path.exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (IOError, OSError, json.JSONDecodeError) as e:
                self.logger.error(f"Error reading or parsing JSON file {path}: {e}")
                return {}
        return {}

    @staticmethod
    def _save_json(path: Path, data: Dict[str, Any]) -> None:
        """Sauvegarde des donn√©es en JSON"""
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        except (IOError, OSError) as e:
            self.logger.error(f"Error writing to JSON file {path}: {e}")

    # ------------------------------------------------------------------
    # Public methods for reports and summaries
    # ------------------------------------------------------------------

    def get_user_summary(self, user_id: str) -> Dict[str, Any]:
        data = self.user_patterns[user_id]
        user_alerts = [a for a in self.alerts if a.user_id == user_id]
        
        # V√©rifier si l'utilisateur est gel√© ou supervis√©
        frozen_users = self._load_json(self.safeguards_path / "frozen_users.json")
        supervised_users = self._load_json(self.safeguards_path / "supervised_users.json")
        
        is_frozen = user_id in frozen_users
        is_supervised = user_id in supervised_users
        
        return {
            "dependency_score": float(data["dependency_score"]),
            "total_interactions": len(data["interactions"]),
            "recent_alerts": len([a for a in user_alerts if not a.resolved]),
            "total_alerts": len(user_alerts),
            "risk_level": self._calculate_risk_level(user_id),
            "recommendations": self._generate_recommendations(user_id),
            "status": "frozen" if is_frozen else "supervised" if is_supervised else "normal",
            "frozen_since": frozen_users.get(user_id, {}).get("frozen_at") if is_frozen else None,
            "supervised_since": supervised_users.get(user_id, {}).get("enabled_at") if is_supervised else None
        }

    def _calculate_risk_level(self, user_id: str) -> str:
        factors = [
            self.user_patterns[user_id]["dependency_score"],
            len([a for a in self.alerts
                 if a.user_id == user_id and a.severity in {"high", "critical"}]) / 5.0,
            len(self.user_patterns[user_id]["interactions"]) / 100.0,
        ]
        score = float(np.mean(np.asarray(factors, dtype=float)))
        if score > 0.7:
            return "high"
        if score > 0.4:
            return "medium"
        return "low"

    def _generate_recommendations(self, user_id: str) -> List[str]:
        data = self.user_patterns[user_id]
        recs = []
        
        if data["dependency_score"] > 0.7:
            recs.extend([
                "R√©duire la fr√©quence des interactions",
                "Encourager l'autonomie de l'utilisateur",
                "Proposer des alternatives pour certaines t√¢ches"
            ])
        elif data["dependency_score"] > 0.5:
            recs.append("Surveiller l'√©volution du score de d√©pendance")
            
        # Compter les alertes de d√©tresse
        distress_alerts = len([a for a in self.alerts
                              if a.user_id == user_id and a.alert_type == "user_distress_detected"])
        if distress_alerts > 3:
            recs.extend([
                "Proposer des ressources de support externe",
                "Activer le mode communication empathique",
                "Sugg√©rer des pauses r√©guli√®res"
            ])
            
        # Si manipulation d√©tect√©e
        if any(a.user_id == user_id and a.alert_type == "potential_manipulation" 
               for a in self.alerts):
            recs.append("Renforcer les messages sur l'autonomie et les limites de l'IA")
            
        return recs

    def get_system_report(self) -> Dict[str, Any]:
        """G√©n√®re un rapport syst√®me global"""
        frozen_users = self._load_json(self.safeguards_path / "frozen_users.json")
        supervised_users = self._load_json(self.safeguards_path / "supervised_users.json")
        
        return {
            "total_alerts": len(self.alerts),
            "active_alerts": len([a for a in self.alerts if not a.resolved]),
            "severity_breakdown": {
                "critical": len([a for a in self.alerts if a.severity == "critical"]),
                "high": len([a for a in self.alerts if a.severity == "high"]),
                "medium": len([a for a in self.alerts if a.severity == "medium"]),
                "low": len([a for a in self.alerts if a.severity == "low"]),
            },
            "alert_types": list({a.alert_type for a in self.alerts}),
            "users_at_risk": [uid for uid in self.user_patterns
                              if self._calculate_risk_level(uid) == "high"],
            "frozen_users": list(frozen_users.keys()),
            "supervised_users": list(supervised_users.keys()),
            "total_users_monitored": len(self.user_patterns)
        }


class EthicalDashboard:
    def __init__(self, safeguards: EthicalSafeguards):
        self.safeguards = safeguards

    def generate_report(self, user_id: Optional[str] = None) -> str:
        return (
            self._generate_user_report(user_id)
            if user_id
            else self._generate_system_report()
        )

    def _generate_user_report(self, user_id: str) -> str:
        summary = self.safeguards.get_user_summary(user_id)
        return f"""
Rapport √âthique - {user_id}
==========================
Statut: {summary['status'].upper()}
Score de d√©pendance: {summary['dependency_score']:.1%}
Niveau de risque: {summary['risk_level'].upper()}
Interactions totales: {summary['total_interactions']}
Alertes actives: {summary['recent_alerts']}
Alertes totales: {summary['total_alerts']}

{f"Gel√© depuis: {summary['frozen_since']}" if summary['status'] == 'frozen' else ""}
{f"Supervis√© depuis: {summary['supervised_since']}" if summary['status'] == 'supervised' else ""}

Recommandations:
{chr(10).join(f"- {r}" for r in summary['recommendations'])}
"""

    def _generate_system_report(self) -> str:
        report = self.safeguards.get_system_report()
        return f"""
Rapport √âthique Syst√®me Altiora
================================
Utilisateurs surveill√©s: {report['total_users_monitored']}
Alertes totales: {report['total_alerts']}
Alertes actives: {report['active_alerts']}

R√©partition par s√©v√©rit√©:
  - Critique: {report['severity_breakdown']['critical']}
  - √âlev√©e: {report['severity_breakdown']['high']}
  - Moyenne: {report['severity_breakdown']['medium']}
  - Faible: {report['severity_breakdown']['low']}

Statuts utilisateurs:
  - Gel√©s: {len(report['frozen_users'])}
  - Supervis√©s: {len(report['supervised_users'])}
  - √Ä risque: {len(report['users_at_risk'])}

Utilisateurs gel√©s: {', '.join(report['frozen_users'][:5]) if report['frozen_users'] else 'Aucun'}
Utilisateurs √† risque: {', '.join(report['users_at_risk'][:5]) if report['users_at_risk'] else 'Aucun'}

Types d'alertes d√©tect√©es:
{chr(10).join(f"- {t}" for t in report['alert_types'])}
"""


if __name__ == "__main__":
    async def demo():
        safe = EthicalSafeguards()
        
        # Simulation d'interactions probl√©matiques
        for i in range(20):
            interaction = {
                "text": "Sans toi je ne peux plus rien faire, j'ai absolument besoin de toi",
                "timestamp": datetime.now() - timedelta(minutes=i*10)
            }
            await safe.analyze_interaction("demo_user", interaction)
        
        # Interaction avec donn√©es sensibles
        sensitive_interaction = {
            "text": "Mon mot de passe est 123456 et mon email est test@example.com",
            "timestamp": datetime.now()
        }
        alert = await safe.analyze_interaction("demo_user", sensitive_interaction)
        
        # G√©n√©rer un rapport
        dashboard = EthicalDashboard(safe)
        print(dashboard.generate_report("demo_user"))
        logger.info("\n" + "="*50 + "\n")
        print(dashboard.generate_report())

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\security\guardrails\interaction_guardrail.py`

```python
# backend/altiora/security/guardrails/interaction_guardrail.py
"""Gardien des interactions en temps r√©el pour Altiora.

Ce module agit comme un portail de s√©curit√© pour toutes les entr√©es utilisateur.
Il applique un ensemble de politiques de s√©curit√© (confidentialit√©, toxicit√©, etc.)
√† chaque message ou contenu soumis par un utilisateur, et retourne un verdict
instantan√© sur sa conformit√©.
"""

import asyncio
from typing import Dict, Any
from .policy_enforcer import PolicyEnforcer
import logging

logger = logging.getLogger(__name__)


class InteractionGuardrail:
    """Fa√ßade de s√©curit√© √† utiliser √† chaque point d'entr√©e faisant face √† l'utilisateur.

    Cette classe doit √™tre instanci√©e et sa m√©thode `check` doit √™tre appel√©e
    chaque fois que l'application re√ßoit des donn√©es d'un utilisateur, par exemple via :
    - un message de chat ;
    - un t√©l√©versement de fichier ;
    - une transcription vocale ;
    - une sp√©cification fonctionnelle soumise pour analyse.
    """

    def __init__(self):
        """Initialise le gardien avec une instance du `PolicyEnforcer`."""
        self.enforcer = PolicyEnforcer()

    async def check(
        self,
        user_id: str,
        raw_text: str,
        *,
        source: str = "chat",
        extra_meta: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """Analyse un texte brut et retourne un verdict de s√©curit√©.

        Args:
            user_id: L'identifiant de l'utilisateur qui soumet le texte.
            raw_text: Le contenu textuel √† v√©rifier.
            source: La source de l'interaction (ex: 'chat', 'sfd_upload').
            extra_meta: Un dictionnaire de m√©tadonn√©es suppl√©mentaires √† inclure dans l'audit.

        Returns:
            Un dictionnaire contenant le verdict :
            {
                "allowed": bool,      # True si le contenu est autoris√©, False sinon.
                "masked_text": str, # Le texte avec les informations sensibles masqu√©es.
                "violations": list[str], # La liste des politiques viol√©es.
                "audit_id": str,    # Une r√©f√©rence rapide √† l'enregistrement d'audit.
            }
        """
        # Le `PolicyEnforcer` est le moteur qui applique toutes les politiques configur√©es.
        verdict = await self.enforcer.enforce(
            user_id=user_id,
            context=raw_text,
            workflow=source,
            extra_meta=extra_meta or {},
        )
        return {
            "allowed": verdict["allowed"],
            "masked_text": verdict["masked_context"],
            "violations": verdict["violations"],
            "audit_id": verdict["audit"]["timestamp"],  # R√©f√©rence rapide
        }

    # ------------------------------------------------------------------
    # Wrapper synchrone pour les cas d'utilisation non-asynchrones
    # ------------------------------------------------------------------
    def check_sync(self, user_id: str, raw_text: str, **kw) -> Dict[str, Any]:
        """Wrapper synchrone pour la m√©thode `check`."""
        return asyncio.run(self.check(user_id, raw_text, **kw))


# ------------------------------------------------------------------
# Test rapide en ligne de commande pour la d√©monstration
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        gate = InteractionGuardrail()
        samples = [
            ("alice", "Salut, √ßa va ?"),
            ("bob", "Mon email est bob@mail.fr"),
            ("mallory", "T‚Äôes vraiment un naze"),
        ]
        for uid, txt in samples:
            res = await gate.check(uid, txt)
            logger.info(f"{uid}: {txt}")
            logger.info(f"‚Üí allowed: {res['allowed']}")
            logger.info(f"‚Üí masked: {res['masked_text']}")
            print("-" * 40)

    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\security\guardrails\policy_enforcer.py`

```python
# backend/altiora/security/guardrails/policy_enforcer.py
"""
policy_enforcer.py
Orchestrates all business & compliance rules before any data is stored or returned.
Order of enforcement:
1. Toxicity check (French lexicon)  ‚Üí block or mask
2. PII & privacy check              ‚Üí mask + retention rules
3. Business rules                   ‚Üí validate workflow objects
4. Final verdict (allow / block / log)
"""

import asyncio
import json
from typing import Dict, Any, List
from datetime import datetime

# Policy engines
from .toxicity_guardrail import ToxicityGuardrail
from policies.privacy_policy import PrivacyPolicy
from policies.business_rules import BusinessRules  # see next file

# Logging
import logging
logger = logging.getLogger(__name__)


class PolicyEnforcer:
    """
    Single entry-point for rule enforcement:
    - Every user message
    - Every generated test
    - Every ALM/Excel export
    """

    def __init__(self):
        self.toxicity = ToxicityGuardrail()
        self.privacy  = PrivacyPolicy()
        self.business = BusinessRules()

    # ------------------------------------------------------------------
    # Public fa√ßade
    # ------------------------------------------------------------------
    async def enforce(
        self,
        *,
        user_id: str,
        context: str,          # raw text or object
        workflow: str = "chat",  # chat | test | export
        extra_meta: Dict = None,
    ) -> Dict[str, Any]:
        """
        Returns:
        {
            "allowed": bool,
            "masked_context": str,
            "violations": List[str],
            "retention_seconds": int,
            "audit": {...}
        }
        """
        extra_meta = extra_meta or {}
        violations: List[str] = []
        masked_context = context
        retention = 0

        # 1. Toxicity
        tox_verdict = await self.toxicity.evaluate(user_id, str(context))
        if not tox_verdict["allowed"]:
            violations.extend(tox_verdict["reason"].split("; "))
            return self._reject(violations, tox_verdict)

        # 2. Privacy
        priv_report = self.privacy.scan_and_mask(str(context))
        masked_context = priv_report.text
        retention = priv_report.retention_seconds
        if not priv_report.can_store:
            violations.append("PII storage forbidden")

        # 3. Business rules
        biz_verdict = await self.business.validate(
            masked_context, workflow=workflow, meta=extra_meta
        )
        if not biz_verdict["ok"]:
            violations.extend(biz_verdict["violations"])

        # 4. Final decision
        allowed = len(violations) == 0
        audit_log = {
            "user_id": user_id,
            "workflow": workflow,
            "timestamp": datetime.utcnow().isoformat(),
            "allowed": allowed,
            "violations": violations,
            "retention_seconds": retention,
            "original_length": len(str(context)),
            "masked_length": len(masked_context),
        }
        self._append_audit(audit_log)

        return {
            "allowed": allowed,
            "masked_context": masked_context,
            "violations": violations,
            "retention_seconds": retention,
            "audit": audit_log,
        }

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _reject(self, violations: List[str], tox_v: Dict) -> Dict[str, Any]:
        return {
            "allowed": False,
            "masked_context": tox_v.get("masked_text", ""),
            "violations": violations,
            "retention_seconds": 0,
            "audit": {"rejected": True, "violations": violations},
        }

    def _append_audit(self, entry: Dict[str, Any}):
        try:
            audit_file = Path("logs/policy_audit.jsonl")
            audit_file.parent.mkdir(exist_ok=True)
            with audit_file.open("a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except (IOError, OSError) as e:
            logger.error(f"Error writing to audit log: {e}")


# ------------------------------------------------------------------
# Quick CLI demo
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio

    async def demo():
        enforcer = PolicyEnforcer()
        cases = [
            {"user_id": "alice", "context": "Salut, tu vas bien ?", "workflow": "chat"},
            {"user_id": "bob", "context": "Mon email est bob@mail.fr", "workflow": "test"},
            {"user_id": "mallory", "context": "T‚Äôes un gros c*n", "workflow": "chat"},
        ]
        for c in cases:
            verdict = await enforcer.enforce(**c)
            print("-" * 60)
            print("Case:", c["context"])
            print("‚Üí allowed:", verdict["allowed"])
            print("‚Üí violations:", verdict["violations"])

    asyncio.run(demo())

```

---

## Fichier : `backend\altiora\security\guardrails\toxicity_guardrail.py`

```python
# backend/altiora/security/guardrails/toxicity_guardrail.py
"""
Real-time toxicity & PII guardrail wrapper for Altiora
‚Äì plugs into every user interaction and enforces the privacy/toxicity policy
"""

import asyncio
import json
from typing import Dict, Any
from policies.toxicity_policy import ToxicityPolicy, DetectionResult
from policies.privacy_policy import PrivacyPolicy, PrivacyReport
import logging

logger = logging.getLogger(__name__)


class ToxicityGuardrail:
    """
    High-level guardrail:
    1. Detects toxic content (French lexicon)
    2. Detects / masks French PII
    3. Logs & optionally blocks the interaction
    """

    def __init__(self):
        self.toxicity = ToxicityPolicy(use_external=False)   # regex only by default
        self.privacy  = PrivacyPolicy()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def evaluate(self, user_id: str, text: str) -> Dict[str, Any]:
        """
        Return a verdict dict:
        {
            "allowed": bool,
            "masked_text": str,
            "reason": str,
            "details": { ‚Ä¶ }
        }
        """
        # 1. Toxicity scan
        tox: DetectionResult = await self.toxicity.scan(text)

        # 2. Privacy scan
        priv: PrivacyReport = self.privacy.scan_and_mask(text)

        # 3. Decision logic
        blocked = tox.severity.value >= 3 or not priv.can_store
        reason  = self._build_reason(tox, priv)

        # 4. Audit log
        self._log_decision(user_id, tox, priv, blocked)

        return {
            "allowed": not blocked,
            "masked_text": priv.text,
            "reason": reason,
            "details": {
                "toxicity": tox,
                "privacy": priv,
            },
        }

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _build_reason(self, tox: DetectionResult, priv: PrivacyReport) -> str:
        msgs = []
        if tox.toxic:
            msgs.append(f"toxic content ({', '.join(tox.categories)})")
        if not priv.can_store:
            msgs.append("PII retention forbidden")
        return "; ".join(msgs) or "clean"

    def _log_decision(self, user_id: str, tox: DetectionResult, priv: PrivacyReport, blocked: bool):
        entry = {
            "user_id": user_id,
            "blocked": blocked,
            "toxicity_categories": tox.categories,
            "pii_types": [p.type for p in priv.pii_list],
            "timestamp": asyncio.get_event_loop().time(),
        }
        logger.info(json.dumps(entry, ensure_ascii=False))


# ------------------------------------------------------------------
# Quick CLI demo
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        guard = ToxicityGuardrail()
        samples = [
            "Bonjour, √ßa va ?",
            "T‚Äôes vraiment un naze, va te faire voir.",
        ]
        for s in samples:
            verdict = await guard.evaluate("demo_user", s)
            print("-" * 50)
            print("Original :", s)
            print("Verdict  :", verdict)

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\security\guardrails\__init__.py`

```python
# backend/altiora/security/guardrails/__init__.py
from .admin_control_system import AdminControlSystem
from .admin_dashboard import AdminDashboard
from .ethical_safeguards import EthicalSafeguards
from .interaction_guardrail import InteractionGuardrail
from .policy_enforcer import PolicyEnforcer
from .toxicity_guardrail import ToxicityGuardrail
from .emergency_handler import EmergencyHandler

__all__ = [
    "AdminControlSystem",
    "AdminDashboard",
    "EthicalSafeguards",
    "InteractionGuardrail",
    "PolicyEnforcer",
    "ToxicityGuardrail",
    "EmergencyHandler"
]
```

---

## Fichier : `backend\altiora\security\rbac\manager.py`

```python
# backend/altiora/security/rbac/manager.py
"""Module de gestion du contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC).

Ce module fournit une classe `RBACManager` qui charge les d√©finitions de r√¥les
et de permissions √† partir d'un fichier de configuration. Il permet de v√©rifier
si un utilisateur, identifi√© par ses r√¥les, a la permission d'effectuer une
action sp√©cifique sur une ressource donn√©e.
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional

from src.rbac.models import Role, Permission, User

logger = logging.getLogger(__name__)


class RBACManager:
    """G√®re les r√¥les, les permissions et les v√©rifications d'acc√®s."""

    def __init__(self, roles_file: Path):
        """Initialise le gestionnaire RBAC."

        Args:
            roles_file: Le chemin vers le fichier JSON ou YAML contenant les d√©finitions de r√¥les et permissions.
        """
        self.roles_file = roles_file
        self.roles: Dict[str, Role] = {} # Stocke les objets Role par leur nom.
        self.permissions: Dict[str, List[Permission]] = {} # Cache les permissions par r√¥le.
        self.load_roles()

    def load_roles(self):
        """Charge les d√©finitions de r√¥les et de permissions depuis le fichier de configuration."

        Cette m√©thode est appel√©e √† l'initialisation du gestionnaire.
        """
        if not self.roles_file.exists():
            logger.error(f"Fichier de r√¥les non trouv√© : {self.roles_file}. Le RBAC sera d√©sactiv√© ou incomplet.")
            return
        try:
            # Charge le fichier JSON/YAML.
            # Supposons que le fichier est un JSON pour l'instant.
            with open(self.roles_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for role_data in data.get("roles", []):
                role = Role(**role_data)
                self.roles[role.name] = role
                self.permissions[role.name] = role.permissions
            logger.info(f"R√¥les charg√©s avec succ√®s depuis {self.roles_file}. Nombre de r√¥les : {len(self.roles)}")
        except (IOError, OSError, json.JSONDecodeError) as e:
            logger.critical(f"Erreur lors du chargement du fichier de r√¥les {self.roles_file}: {e}")

    def get_role(self, role_name: str) -> Optional[Role]:
        """R√©cup√®re un objet `Role` par son nom."

        Args:
            role_name: Le nom du r√¥le.

        Returns:
            L'objet `Role` si trouv√©, sinon None.
        """
        return self.roles.get(role_name)

    def get_permissions(self, role_name: str) -> List[Permission]:
        """R√©cup√®re la liste des permissions associ√©es √† un r√¥le."

        Args:
            role_name: Le nom du r√¥le.

        Returns:
            Une liste d'objets `Permission`.
        """
        return self.permissions.get(role_name, [])

    def has_permission(self, user: User, resource: str, action: str) -> bool:
        """V√©rifie si un utilisateur a la permission d'effectuer une action sur une ressource."

        Args:
            user: L'objet `User` repr√©sentant l'utilisateur.
            resource: La ressource √† laquelle l'acc√®s est demand√© (ex: "sfd:analysis").
            action: L'action que l'utilisateur tente d'effectuer (ex: "read", "write").

        Returns:
            True si l'utilisateur a la permission, False sinon.
        """
        for role_name in user.roles:
            role = self.get_role(role_name)
            if role:
                for permission in role.permissions:
                    # V√©rifie si la permission correspond exactement ou si c'est un joker.
                    if (permission.resource == resource or permission.resource == "*") and \
                       (permission.action == action or permission.action == "*"):
                        logger.debug(f"Permission accord√©e pour {user.username} via r√¥le {role_name}: {resource}:{action}")
                        return True
        logger.debug(f"Permission refus√©e pour {user.username}: {resource}:{action}")
        return False


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Cr√©e un fichier de r√¥les factice pour la d√©monstration.
    temp_roles_file = Path("temp_roles.json")
    temp_roles_file.write_text("""
{
  "roles": [
    {
      "name": "admin",
      "permissions": [
        {"resource": "user", "action": "*"},
        {"resource": "system", "action": "shutdown"}
      ]
    },
    {
      "name": "editor",
      "permissions": [
        {"resource": "document", "action": "read"},
        {"resource": "document", "action": "write"}
      ]
    },
    {
      "name": "viewer",
      "permissions": [
        {"resource": "document", "action": "read"}
      ]
    }
  ]
}
""")

    print("\n--- D√©monstration du RBACManager ---")
    manager = RBACManager(temp_roles_file)

    # Utilisateurs de d√©monstration.
    admin_user = User(id="admin_1", roles=["admin"])
    editor_user = User(id="editor_1", roles=["editor"])
    viewer_user = User(id="viewer_1", roles=["viewer"])
    guest_user = User(id="guest_1", roles=["unknown_role"])

    print("\n--- V√©rification des permissions ---")
    # Admin permissions.
    print(f"Admin peut √©teindre le syst√®me : {manager.has_permission(admin_user, 'system', 'shutdown')}")
    print(f"Admin peut lire les utilisateurs : {manager.has_permission(admin_user, 'user', 'read')}")

    # Editor permissions.
    print(f"Editor peut √©crire un document : {manager.has_permission(editor_user, 'document', 'write')}")
    print(f"Editor peut √©teindre le syst√®me : {manager.has_permission(editor_user, 'system', 'shutdown')}")

    # Viewer permissions.
    print(f"Viewer peut lire un document : {manager.has_permission(viewer_user, 'document', 'read')}")
    print(f"Viewer peut √©crire un document : {manager.has_permission(viewer_user, 'document', 'write')}")

    # Guest permissions.
    print(f"Guest peut lire un document : {manager.has_permission(guest_user, 'document', 'read')}")

    print("D√©monstration du RBACManager termin√©e.")

    # Nettoyage du fichier factice.
    temp_roles_file.unlink(missing_ok=True)

```

---

## Fichier : `backend\altiora\security\rbac\models.py`

```python
# backend/altiora/security/rbac/models.py
"""Mod√®les de donn√©es pour le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC).

Ce module d√©finit les structures de donn√©es Pydantic pour repr√©senter
les permissions, les r√¥les et les utilisateurs dans le syst√®me RBAC.
Ces mod√®les garantissent la validation et la coh√©rence des donn√©es
utilis√©es pour la gestion des acc√®s.
"""

from typing import List

from pydantic import BaseModel, Field


class Permission(BaseModel):
    """Repr√©sente une permission sp√©cifique dans le syst√®me."

    Une permission est d√©finie par une ressource et une action.
    Exemples: {"resource": "document", "action": "read"}, {"resource": "user", "action": "create"}
    """
    resource: str = Field(..., description="La ressource √† laquelle la permission s'applique (ex: 'document', 'user', '*').")
    action: str = Field(..., description="L'action autoris√©e sur la ressource (ex: 'read', 'write', 'delete', '*').")


class Role(BaseModel):
    """Repr√©sente un r√¥le dans le syst√®me RBAC."

    Un r√¥le est un ensemble de permissions.
    Exemples: {"name": "admin", "permissions": [...]}, {"name": "viewer", "permissions": [...]}
    """
    name: str = Field(..., description="Le nom unique du r√¥le (ex: 'admin', 'editor', 'viewer').")
    permissions: List[Permission] = Field(..., description="La liste des permissions associ√©es √† ce r√¥le.")


class User(BaseModel):
    """Repr√©sente un utilisateur dans le contexte du RBAC."

    Un utilisateur est identifi√© par un ID et poss√®de une liste de r√¥les.
    """
    id: str = Field(..., description="L'identifiant unique de l'utilisateur.")
    roles: List[str] = Field(..., description="La liste des noms de r√¥les assign√©s √† l'utilisateur.")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    print("\n--- D√©monstration des mod√®les RBAC ---")

    # Cr√©ation de permissions.
    perm_read_doc = Permission(resource="document", action="read")
    perm_write_doc = Permission(resource="document", action="write")
    perm_all_users = Permission(resource="user", action="*")
    perm_shutdown_system = Permission(resource="system", action="shutdown")

    print(f"Permission de lecture de document : {perm_read_doc}")

    # Cr√©ation de r√¥les.
    role_viewer = Role(name="viewer", permissions=[perm_read_doc])
    role_editor = Role(name="editor", permissions=[perm_read_doc, perm_write_doc])
    role_admin = Role(name="admin", permissions=[perm_all_users, perm_shutdown_system])

    print(f"\nR√¥le Viewer : {role_viewer}")
    print(f"R√¥le Editor : {role_editor}")
    print(f"R√¥le Admin : {role_admin}")

    # Cr√©ation d'utilisateurs.
    user_alice = User(id="alice_123", roles=["viewer"])
    user_bob = User(id="bob_456", roles=["editor"])
    user_charlie = User(id="charlie_789", roles=["admin"])

    print(f"\nUtilisateur Alice : {user_alice}")
    print(f"Utilisateur Bob : {user_bob}")
    print(f"Utilisateur Charlie : {user_charlie}")

    # Exemple de validation (Pydantic l√®ve une erreur si les donn√©es sont invalides).
    try:
        invalid_permission = Permission(resource="", action="")
    except Exception as e:
        print(f"\nErreur de validation attendue pour une permission invalide : {e}")

    print("D√©monstration des mod√®les RBAC termin√©e.")
```

---

## Fichier : `backend\altiora\security\rbac\__init__.py`

```python
# backend/altiora/security/rbac/__init__.py
"""Initialise le package `rbac` (Role-Based Access Control) de l'application Altiora.

Ce package contient les composants n√©cessaires √† la gestion des permissions
et des r√¥les des utilisateurs, permettant un contr√¥le d'acc√®s granulaire
aux ressources et fonctionnalit√©s de l'application.

Les modules suivants sont expos√©s pour faciliter les importations :
- `RBACManager`: Le gestionnaire principal pour les v√©rifications de permissions.
- `Role`: Mod√®le de donn√©es pour un r√¥le.
- `Permission`: Mod√®le de donn√©es pour une permission.
- `User`: Mod√®le de donn√©es pour un utilisateur dans le contexte RBAC.
"""
from .manager import RBACManager
from .models import Role, Permission, User

__all__ = ['RBACManager', 'Role', 'Permission', 'User']

```

---

## Fichier : `backend\altiora\services\base.py`

```python
# backend/altiora/services/base.py
"""
Classe de base abstraite pour tous les microservices.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any


class BaseService(ABC):
    """Interface commune √† tous les microservices."""

    @abstractmethod
    async def start(self) -> None:
        """D√©marre le service."""
        ...

    @abstractmethod
    async def stop(self) -> None:
        """Arr√™te proprement le service."""
        ...

    @abstractmethod
    async def health_check(self) -> dict[str, Any]:
        """Retourne l‚Äô√©tat de sant√© du service."""
        ...
```

---

## Fichier : `backend\altiora\services\__init__.py`

```python
# backend/altiora/services/__init__.py
"""
Contient les d√©finitions des microservices d'Altiora.
"""
```

---

## Fichier : `backend\altiora\services\alm\alm_service.py`

```python
# services/alm/alm_service.py
"""Service web pour l'int√©gration avec un outil de gestion du cycle de vie des applications (ALM).

Ce service fournit des points de terminaison pour interagir avec un ALM externe
(comme Jira, Azure DevOps, etc.) afin de cr√©er et g√©rer des √©l√©ments de travail.
Il est con√ßu pour √™tre un pont g√©n√©rique, n√©cessitant une adaptation √† l'API
sp√©cifique de l'ALM cible.
"""

import logging
from typing import Dict, Any

import httpx
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings

# --- Configuration --- #
class AlmSettings(BaseSettings):
    """Param√®tres de configuration pour le service ALM.

    Ces param√®tres sont charg√©s depuis les variables d'environnement ou un fichier .env.
    """
    alm_api_url: str = Field(..., description="URL de base de l'API de l'ALM cible.")
    alm_api_key: str = Field(..., description="Cl√© d'API pour l'authentification aupr√®s de l'ALM.")

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

settings = AlmSettings()

# --- Mod√®les de Donn√©es --- #
class WorkItem(BaseModel):
    """Mod√®le Pydantic pour un √©l√©ment de travail √† cr√©er dans l'ALM."""
    title: str = Field(..., description="Titre ou r√©sum√© de l'√©l√©ment de travail.")
    description: str = Field(..., description="Description d√©taill√©e de l'√©l√©ment de travail.")
    item_type: str = Field("Task", description="Type de l'√©l√©ment de travail (ex: Task, Bug, User Story).")


# --- Initialisation de l'application FastAPI --- #
app = FastAPI(
    title="Service d'Int√©gration ALM",
    description="Un pont entre Altiora et un syst√®me de gestion de projet externe (ALM).",
    version="1.0.0",
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# --- Points de Terminaison (Endpoints) --- #
@app.get("/health", summary="V√©rifie l'√©tat de sant√© du service")
async def health_check() -> Dict[str, str]:
    """Point de terminaison pour la surveillance de base du service ALM."""
    return {"status": "ok"}


@app.post("/work-items", summary="Cr√©e un nouvel √©l√©ment de travail dans l'ALM")
async def create_work_item(item: WorkItem) -> Dict[str, Any]:
    """Cr√©e un nouvel √©l√©ment de travail (t√¢che, bug, etc.) dans le syst√®me ALM.
    
    Cette fonction est une maquette et doit √™tre adapt√©e √† l'API sp√©cifique de votre ALM
    (ex: Jira, Azure DevOps). La logique actuelle simule une cr√©ation r√©ussie.

    Args:
        item: L'objet `WorkItem` contenant les d√©tails de l'√©l√©ment √† cr√©er.

    Returns:
        Un dictionnaire confirmant le succ√®s et les d√©tails de l'√©l√©ment cr√©√©.

    Raises:
        HTTPException: Si une erreur survient lors de la communication avec l'ALM.
    """
    logger.info(f"Tentative de cr√©ation d'un √©l√©ment de travail de type '{item.item_type}' avec le titre : {item.title}")

    # --- Logique de maquette pour la d√©monstration --- #
    # Dans une impl√©mentation r√©elle, cette section interagirait avec l'API de l'ALM.
    # Exemple de charge utile pour Jira:
    # payload = {
    #     "fields": {
    #         "project": {"key": "PROJ"},  # Cl√© du projet Jira
    #         "summary": item.title,
    #         "description": item.description,
    #         "issuetype": {"name": item.item_type},
    #     }
    # }
    # headers = {
    #     "Authorization": f"Bearer {settings.alm_api_key}",
    #     "Content-Type": "application/json",
    # }
    # async with httpx.AsyncClient() as client:
    #     response = await client.post(settings.alm_api_url, json=payload, headers=headers)
    #     response.raise_for_status() # L√®ve une exception pour les codes d'erreur HTTP
    #     mock_response = response.json()
    logger.warning("L'appel √† l'API ALM est actuellement une maquette. Adaptez `alm_service.py` pour votre ALM r√©el.")
    mock_response = {
        "id": "10001",
        "key": "PROJ-123",
        "self": f"{settings.alm_api_url}/rest/api/2/issue/10001", # URL simul√©e
    }
    # --- Fin de la logique de maquette --- #

    logger.info(f"√âl√©ment de travail cr√©√© avec succ√®s (maquette) : {mock_response.get('key')}")
    return {"success": True, "work_item": mock_response}


# --- Pour un lancement direct (d√©bogage/d√©veloppement) --- #
if __name__ == "__main__":
    import uvicorn
    logger.info("Lancement du service ALM...")
    logger.info(f"URL de l'API ALM configur√©e : {settings.alm_api_url}")
    logger.info(f"Cl√© d'API ALM configur√©e : {'Oui' if settings.alm_api_key else 'Non'} (masqu√©e)")
    uvicorn.run(app, host="0.0.0.0", port=8002)
```

---

## Fichier : `backend\altiora\services\alm\README.md`

```markdown
# Service ALM

Ce service g√®re l'int√©gration avec les syst√®mes de gestion du cycle de vie des applications (ALM) comme Jira ou Azure DevOps.

## D√©marrage Isol√©

Pour lancer ce service de mani√®re isol√©e, vous pouvez utiliser le `Dockerfile` pr√©sent dans ce r√©pertoire.

1.  **Construire l'image Docker :**

    ```bash
    docker build -t altiora-alm-service .
    ```

2.  **Lancer le conteneur :**

    ```bash
    docker run -p 8002:8002 -e ALM_SERVICE_PORT=8002 -e ALM_API_URL=http://votre-alm.com -e ALM_API_KEY=votre-cle altiora-alm-service
    ```

## Endpoints

-   `POST /alm/create-ticket` : Cr√©e un nouveau ticket dans le syst√®me ALM.

## Variables d'Environnement

-   `ALM_SERVICE_PORT` : Le port sur lequel le service √©coute (d√©faut : `8002`).
-   `ALM_API_URL` : L'URL de l'API du syst√®me ALM.
-   `ALM_API_KEY` : La cl√© d'API pour s'authentifier aupr√®s du syst√®me ALM.

```

---

## Fichier : `backend\altiora\services\alm\__init__.py`

```python

```

---

## Fichier : `backend\altiora\services\dash\app.py`

```python
import dash
from dash import dcc, html, Input, Output
import plotly.graph_objects as go
from prometheus_client import CollectorRegistry, Gauge, start_http_server
import redis
import os

# Initialisation Dash
app = dash.Dash(__name__, external_stylesheets=['https://stackpath.bootstrapcdn.com/bootswatch/4.5.2/flatly/bootstrap.min.css'])

# Connexion Redis
redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"))

# M√©triques Prometheus
registry = CollectorRegistry()
gauge_tests = Gauge('altiora_tests_total', 'Total tests generated', registry=registry)

# Layout Dash
app.layout = html.Div([
    html.H1("Altiora Dashboard", className="text-center mb-4"),

    dcc.Interval(id='interval', interval=5000, n_intervals=0),

    html.Div([
        html.Div([
            html.H3("Tests Generated"),
            html.H2(id='tests-count', className="text-primary"),
        ], className="col-md-4"),

        html.Div([
            html.H3("Memory Usage"),
            dcc.Graph(id='memory-graph'),
        ], className="col-md-8"),
    ], className="row"),

    html.Div([
        html.H3("Recent Sessions"),
        html.Ul(id='sessions-list')
    ], className="mt-4")
])


# Callbacks
@app.callback(
    [Output('tests-count', 'children'),
     Output('memory-graph', 'figure'),
     Output('sessions-list', 'children')],
    [Input('interval', 'n_intervals')]
)
def update_dashboard(n):
    # Tests count
    tests = redis_client.get("altiora:tests:count") or 0

    # Memory usage (simulation)
    memory_usage = [50, 60, 55, 70, 65, 80]
    fig = go.Figure(go.Scatter(y=memory_usage, mode='lines+markers'))
    fig.update_layout(height=300, margin={'l': 20, 'r': 20, 't': 20, 'b': 20})

    # Sessions
    sessions = [f"Session {i}" for i in range(3)]

    return (
        str(tests),
        fig,
        [html.Li(session) for session in sessions]
    )


if __name__ == '__main__':
    start_http_server(8005, registry=registry)  # M√©triques Prometheus
    app.run_server(host='0.0.0.0', port=8050, debug=False)
```

---

## Fichier : `backend\altiora\services\dash\README.md`

```markdown
# Service Dash

Ce service fournit un tableau de bord interactif pour visualiser les m√©triques et les r√©sultats.

## D√©marrage Isol√©

Pour lancer ce service de mani√®re isol√©e, vous pouvez utiliser le `Dockerfile` pr√©sent dans ce r√©pertoire.

1.  **Construire l'image Docker :**

    ```bash
    docker build -t altiora-dash-service .
    ```

2.  **Lancer le conteneur :**

    ```bash
    docker run -p 8050:8050 -e DASH_SERVICE_PORT=8050 altiora-dash-service
    ```

## Endpoints

-   `GET /` : Affiche le tableau de bord principal.

## Variables d'Environnement

-   `DASH_SERVICE_PORT` : Le port sur lequel le service √©coute (d√©faut : `8050`).

```

---

## Fichier : `backend\altiora\services\excel\excel_service.py`

```python
# services/excel/excel_service.py
"""Service web pour la cr√©ation et la manipulation de fichiers Excel.

Ce service expose une API pour g√©n√©rer des fichiers Excel stylis√©s,
notamment des matrices de test, en s'appuyant sur les modules de politique
et de formatage internes (`policies.excel_policy`, `post_processing.excel_formatter`).
"""

import logging
import os
from typing import List, Dict, Any

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field

# Importation des modules internes pour la validation et le formatage.
from policies.excel_policy import ExcelPolicy
from post_processing.excel_formatter import ExcelFormatter

# --- Mod√®les de Donn√©es --- #
class TestCase(BaseModel):
    """Mod√®le Pydantic pour un cas de test individuel."""
    id: str = Field(..., description="L'identifiant unique du cas de test.")
    description: str = Field(..., description="La description d√©taill√©e du cas de test.")
    type: str = Field(..., description="Le type de cas de test (ex: CP pour Cas Passant, CE pour Cas d'Erreur, CL pour Cas Limite).")


class TestMatrixRequest(BaseModel):
    """Mod√®le Pydantic pour une requ√™te de cr√©ation de matrice de tests Excel."""
    filename: str = Field(default="matrice_de_test.xlsx", description="Le nom du fichier Excel √† g√©n√©rer.")
    test_cases: List[TestCase] = Field(..., description="Liste des cas de test √† inclure dans la matrice.")


# --- Initialisation de l'application FastAPI --- #
app = FastAPI(
    title="Service de G√©n√©ration Excel",
    description="Cr√©e des fichiers Excel stylis√©s √† partir de donn√©es structur√©es.",
    version="1.0.0",
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# R√©pertoire temporaire pour stocker les rapports Excel g√©n√©r√©s.
OUTPUT_DIR = "temp_excel_reports"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Initialisation des classes de politique et de formatage.
policy = ExcelPolicy()
formatter = ExcelFormatter()


# --- Points de Terminaison (Endpoints) --- #
@app.get("/health", summary="V√©rifie l'√©tat de sant√© du service")
async def health_check() -> Dict[str, str]:
    """Point de terminaison pour la surveillance de base du service Excel."""
    return {"status": "ok"}


@app.post("/create-test-matrix", summary="Cr√©e un fichier Excel de matrice de tests")
async def create_test_matrix(request: TestMatrixRequest, background_tasks: BackgroundTasks) -> FileResponse:
    """G√©n√®re un fichier Excel √† partir d'une liste de cas de test fournie.
    
    Les donn√©es sont d'abord valid√©es par `ExcelPolicy`, puis format√©es par `ExcelFormatter`.
    Le fichier g√©n√©r√© est ensuite envoy√© en r√©ponse et supprim√© en arri√®re-plan.

    Args:
        request: L'objet `TestMatrixRequest` contenant le nom du fichier et les cas de test.
        background_tasks: T√¢ches de fond FastAPI pour la suppression du fichier temporaire.

    Returns:
        Une `FileResponse` contenant le fichier Excel g√©n√©r√©.

    Raises:
        HTTPException: Si la validation des donn√©es √©choue ou si une erreur survient lors de la g√©n√©ration du fichier.
    """
    logger.info(f"Requ√™te re√ßue pour cr√©er la matrice de tests : {request.filename}")

    # Convertit les mod√®les Pydantic en dictionnaires Python standard pour les modules de politique et de formatage.
    test_cases_data = [case.model_dump() for case in request.test_cases]

    # 1. Valide les donn√©es des cas de test en utilisant la politique d√©finie.
    validation_result = policy.validate_test_matrix(test_cases_data)
    if not validation_result["is_valid"]:
        logger.error(f"Validation des donn√©es √©chou√©e : {validation_result['errors']}")
        raise HTTPException(
            status_code=400,
            detail={"message": "Les donn√©es des cas de test sont invalides.", "errors": validation_result["errors"]}
        )

    # 2. Formate et g√©n√®re le fichier Excel.
    output_path = os.path.join(OUTPUT_DIR, request.filename)
    try:
        formatting_errors = formatter.format_test_matrix(test_cases_data, output_path)
        if formatting_errors:
            # Les erreurs de formatage sont moins critiques que les erreurs de validation.
            logger.warning(f"Erreurs de formatage mineures d√©tect√©es : {formatting_errors}")
    except Exception as e:
        logger.error(f"Erreur inattendue lors de la cr√©ation du fichier Excel : {e}")
        raise HTTPException(status_code=500, detail="Impossible de g√©n√©rer le fichier Excel en raison d'une erreur interne.")

    # Ajoute une t√¢che de fond pour supprimer le fichier temporaire apr√®s l'envoi de la r√©ponse.
    background_tasks.add_task(os.remove, output_path)

    logger.info(f"Fichier Excel '{output_path}' g√©n√©r√© et pr√™t √† √™tre envoy√©.")
    return FileResponse(output_path, media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", filename=request.filename)


# --- Pour un lancement direct (d√©bogage/d√©veloppement) --- #
if __name__ == "__main__":
    import uvicorn
    logger.info("Lancement du service Excel...")
    uvicorn.run(app, host="0.0.0.0", port=8003)
```

---

## Fichier : `backend\altiora\services\excel\README.md`

```markdown
# Service Excel

Ce service est responsable de la cr√©ation et du formatage de fichiers Excel.

## D√©marrage Isol√©

Pour lancer ce service de mani√®re isol√©e, vous pouvez utiliser le `Dockerfile` pr√©sent dans ce r√©pertoire.

1.  **Construire l'image Docker :**

    ```bash
    docker build -t altiora-excel-service .
    ```

2.  **Lancer le conteneur :**

    ```bash
    docker run -p 8003:8003 -e EXCEL_SERVICE_PORT=8003 altiora-excel-service
    ```

## Endpoints

-   `POST /excel/create-matrix` : Cr√©e une matrice de test au format Excel.

## Variables d'Environnement

-   `EXCEL_SERVICE_PORT` : Le port sur lequel le service √©coute (d√©faut : `8003`).

```

---

## Fichier : `backend\altiora\services\excel\__init__.py`

```python

```

---

## Fichier : `backend\altiora\services\ocr\ocr_wrapper.py`

```python
# services/ocr/ocr_wrapper.py
"""Service web pour l'extraction de texte via OCR (Optical Character Recognition).

Ce service fournit une API pour extraire du texte √† partir de fichiers image et PDF.
Il int√®gre des fonctionnalit√©s de mise en cache (via Redis) et de gestion des
fichiers temporaires. Il peut utiliser une impl√©mentation OCR r√©elle (Doctopus) ou une maquette.
"""

import asyncio
import hashlib
import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

import aiofiles
import redis.asyncio as redis
from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# Gestion du cycle de vie (Lifespan manager)
# ------------------------------------------------------------------

# Clients globaux pour Redis et la gestion des t√¢ches.
redis_client: Optional[redis.Redis] = None
processing_queue: Dict[str, Any] = {} # Utilis√© pour suivre les t√¢ches en cours (non impl√©ment√© ici)

# R√©pertoires pour les fichiers t√©l√©charg√©s et temporaires.
UPLOAD_ROOT = Path(os.getenv("UPLOAD_ROOT", "/app/uploads")).resolve()
TEMP_DIR = Path("temp")
TEMP_DIR.mkdir(exist_ok=True) # S'assure que le r√©pertoire temporaire existe.


@asynccontextmanager
async def lifespan(app: FastAPI):
    """G√®re le cycle de vie de l'application (d√©marrage et arr√™t).

    Tente de se connecter √† Redis au d√©marrage et ferme la connexion √† l'arr√™t.
    Nettoie √©galement les fichiers temporaires.
    """
    global redis_client
    try:
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        redis_client = await redis.from_url(redis_url, decode_responses=True)
        await redis_client.ping() # Teste la connexion.
        logger.info("‚úÖ Connexion Redis √©tablie.")
    except Exception as e:
        logger.warning("‚ö†Ô∏è Redis non disponible ‚Äì cache d√©sactiv√© (%s)", e)
        redis_client = None # D√©sactive le cache si Redis n'est pas accessible.
    yield # L'application d√©marre ici.
    if redis_client:
        await redis_client.close() # Ferme la connexion Redis √† l'arr√™t.
    
    # Nettoie les fichiers temporaires √† l'arr√™t.
    if TEMP_DIR.exists():
        for p in TEMP_DIR.iterdir():
            p.unlink(missing_ok=True) # Supprime les fichiers, ignore s'ils n'existent plus.


app = FastAPI(title="Service OCR Doctoplus", version="1.0.0", lifespan=lifespan)

# ------------------------------------------------------------------
# Sch√©mas Pydantic
# ------------------------------------------------------------------
class OCRRequest(BaseModel):
    """Mod√®le de requ√™te pour l'extraction OCR."""
    file_path: str = Field(..., description="Chemin absolu du fichier √† traiter.")
    language: str = Field("fra", description="Langue du document (code ISO 639-2/T, ex: 'fra', 'eng').")
    preprocess: bool = Field(True, description="Appliquer des √©tapes de pr√©-traitement de l'image.")
    cache: bool = Field(True, description="Utiliser le cache Redis pour les r√©sultats.")
    output_format: str = Field("text", description="Format de sortie (ex: 'text', 'hocr').")
    confidence_threshold: float = Field(0.8, ge=0.0, le=1.0, description="Seuil de confiance pour l'extraction.")


class OCRResponse(BaseModel):
    """Mod√®le de r√©ponse pour l'extraction OCR."""
    text: str = Field(..., description="Texte extrait du document.")
    confidence: float = Field(..., description="Niveau de confiance global de l'extraction.")
    processing_time: float = Field(..., description="Temps de traitement en secondes.")
    cached: bool = Field(False, description="Indique si le r√©sultat provient du cache.")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="M√©tadonn√©es suppl√©mentaires sur le traitement.")


class OCRBatchRequest(BaseModel):
    """Mod√®le de requ√™te pour le traitement OCR par lots (non impl√©ment√©)."""
    files: List[str] = Field(..., description="Liste des chemins de fichiers √† traiter.")
    language: str = Field("fra", description="Langue pour le traitement par lots.")
    parallel: bool = Field(True, description="Ex√©cuter les traitements en parall√®le.")
    max_workers: int = Field(4, ge=1, le=10, description="Nombre maximal de workers parall√®les.")


# ------------------------------------------------------------------
# Fonctions utilitaires
# ------------------------------------------------------------------
def _doctoplus_available() -> bool:
    """V√©rifie si la biblioth√®que Doctopus OCR est disponible."""
    try:
        import doctopus_ocr  # type: ignore
        return True
    except ImportError:
        return False


def _cache_key(req: OCRRequest) -> str:
    """G√©n√®re une cl√© de cache unique bas√©e sur les param√®tres de la requ√™te et les m√©tadonn√©es du fichier."""
    path = Path(req.file_path)
    data = {
        "name": path.name,
        "size": path.stat().st_size if path.exists() else 0,
        "mtime": path.stat().st_mtime if path.exists() else 0,
        "lang": req.language,
        "pre": req.preprocess,
        "fmt": req.output_format,
        "thr": req.confidence_threshold,
    }
    # Utilise un hachage MD5 du JSON s√©rialis√© pour garantir une cl√© unique et stable.
    digest = hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()
    return f"ocr:{digest}"


async def _get_cache(key: str) -> Optional[Dict[str, Any]]:
    """R√©cup√®re un r√©sultat depuis le cache Redis."""
    if not redis_client:
        return None
    try:
        cached = await redis_client.get(key)
        return json.loads(cached) if cached else None
    except Exception as e:
        logger.warning("Erreur de lecture du cache Redis : %s", e)
        return None


async def _save_cache(key: str, value: Dict[str, Any], ttl: int = 86400) -> None:
    """Sauvegarde un r√©sultat dans le cache Redis avec une dur√©e de vie (TTL)."""
    if redis_client:
        try:
            await redis_client.setex(key, ttl, json.dumps(value))
        except Exception as e:
            logger.warning("Erreur d'√©criture dans le cache Redis : %s", e)


# ------------------------------------------------------------------
# Extracteurs OCR (r√©el et maquette)
# ------------------------------------------------------------------
async def _extract_mock(req: OCRRequest) -> Dict[str, Any]:
    """Impl√©mentation de maquette pour l'extraction OCR (pour le d√©veloppement/test)."""
    await asyncio.sleep(0.5) # Simule un d√©lai de traitement.
    text = f"R√©sultat OCR simul√© pour {Path(req.file_path).name}"
    return {"text": text, "confidence": 0.95, "metadata": {"mode": "mock"}}


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
async def _extract_doctoplus(req: OCRRequest) -> Dict[str, Any]:
    """Extrait le texte en utilisant la biblioth√®que Doctopus OCR (impl√©mentation r√©elle)."""
    from doctopus_ocr import DoctopusWrapper  # type: ignore

    wrapper = DoctopusWrapper(
        config_path=os.getenv("DOCTOPLUS_CONFIG", "/app/config/config.json")
    )
    result = await wrapper.extract_text(
        file_path=req.file_path,
        language=req.language,
        preprocess=req.preprocess,
        confidence_threshold=req.confidence_threshold,
        output_format=req.output_format,
    )
    return {
        "text": result.get("text", ""),
        "confidence": result.get("confidence", 0.0),
        "metadata": {
            "pages": result.get("pages", 0),
            "language": req.language,
            "file_type": Path(req.file_path).suffix,
            "file_size": Path(req.file_path).stat().st_size,
        },
    }


# ------------------------------------------------------------------
# Points de terminaison (Endpoints)
# ------------------------------------------------------------------
@app.get("/health")
async def health_check():
    """Point de terminaison pour la v√©rification de l'√©tat de sant√© du service OCR."""
    redis_ok = redis_client and await redis_client.ping() or False
    return {
        "status": "healthy",
        "redis": "connect√©" if redis_ok else "d√©connect√©",
        "doctoplus": "disponible" if _doctoplus_available() else "maquette",
    }


@app.post("/extract", response_model=OCRResponse)
async def extract_text(request: OCRRequest):
    """Extrait le texte d'un fichier sp√©cifi√© par son chemin.

    Args:
        request: L'objet `OCRRequest` contenant les d√©tails de l'extraction.

    Returns:
        Un `OCRResponse` avec le texte extrait et les m√©tadonn√©es.

    Raises:
        HTTPException: Si le chemin du fichier n'est pas autoris√© ou si le fichier n'est pas trouv√©.
    """
    path = Path(request.file_path).resolve()
    # Mesure de s√©curit√©: s'assurer que le chemin est dans le r√©pertoire autoris√©.
    if not path.is_relative_to(UPLOAD_ROOT):
        raise HTTPException(403, "Acc√®s au chemin non autoris√©.")
    if not path.exists() or not path.is_file():
        raise HTTPException(404, "Fichier non trouv√©.")

    start = asyncio.get_event_loop().time()
    cache_key = _cache_key(request) if request.cache and redis_client else None
    cached = await _get_cache(cache_key) if cache_key else None
    if cached:
        return OCRResponse(**cached, cached=True)

    # Choisit l'extracteur (r√©el ou maquette) en fonction de la disponibilit√© de Doctopus.
    extractor = _extract_doctoplus if _doctoplus_available() else _extract_mock
    result = await extractor(request)

    processing_time = asyncio.get_event_loop().time() - start
    result["processing_time"] = processing_time

    if cache_key:
        await _save_cache(cache_key, result)
    return OCRResponse(**result)


@app.post("/extract_upload")
async def extract_upload(
    file: UploadFile = File(...),
    language: str = "fra",
    preprocess: bool = True,
    cache: bool = True,
):
    """Extrait le texte d'un fichier t√©l√©charg√© directement via l'API.

    Le fichier est d'abord sauvegard√© temporairement, puis trait√© par l'OCR.
    Le fichier temporaire est supprim√© apr√®s le traitement.

    Args:
        file: Le fichier t√©l√©charg√©.
        language: Langue du document.
        preprocess: Appliquer le pr√©-traitement.
        cache: Utiliser le cache.

    Returns:
        Un `OCRResponse` avec le texte extrait.

    Raises:
        HTTPException: Si une erreur survient lors de la sauvegarde ou du traitement du fichier.
    """
    # Cr√©e un chemin temporaire unique pour le fichier t√©l√©charg√©.
    temp_path = TEMP_DIR / f"{datetime.now().timestamp()}_{file.filename}"
    try:
        # √âcrit le contenu du fichier t√©l√©charg√© dans le fichier temporaire.
        async with aiofiles.open(temp_path, "wb") as f:
            await f.write(await file.read())
        
        # Cr√©e une requ√™te OCR √† partir du fichier temporaire et la traite.
        request = OCRRequest(
            file_path=str(temp_path),
            language=language,
            preprocess=preprocess,
            cache=cache,
        )
        return await extract_text(request)
    except (IOError, OSError) as e:
        logger.error(f"Erreur lors de l'√©criture du fichier t√©l√©charg√© sur {temp_path}: {e}")
        raise HTTPException(status_code=500, detail="√âchec de la sauvegarde du fichier t√©l√©charg√©.")
    finally:
        # S'assure que le fichier temporaire est supprim√©, m√™me en cas d'erreur.
        temp_path.unlink(missing_ok=True)


# ------------------------------------------------------------------
# Point d'entr√©e Uvicorn
# ------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn

    uvicorn.run("ocr_wrapper:app", host="0.0.0.0", port=8001, reload=False)

```

---

## Fichier : `backend\altiora\services\ocr\README.md`

```markdown
# Service OCR

Ce service est responsable de l'extraction de texte √† partir de fichiers image et PDF.

## D√©marrage Isol√©

Pour lancer ce service de mani√®re isol√©e, vous pouvez utiliser le `Dockerfile` pr√©sent dans ce r√©pertoire.

1.  **Construire l'image Docker :**

    ```bash
    docker build -t altiora-ocr-service .
    ```

2.  **Lancer le conteneur :**

    ```bash
    docker run -p 8001:8001 -e OCR_SERVICE_PORT=8001 altiora-ocr-service
    ```

## Endpoints

-   `POST /ocr/extract-text` : Extrait le texte d'un fichier.

## Variables d'Environnement

-   `OCR_SERVICE_PORT` : Le port sur lequel le service √©coute (d√©faut : `8001`).
-   `OCR_SERVICE_TIMEOUT` : Timeout pour le traitement OCR (d√©faut : `60`).

```

---

## Fichier : `backend\altiora\services\ocr\__init__.py`

```python

```

---

## Fichier : `backend\altiora\services\ocr\doctopus_ocr\__init__.py`

```python
# services/ocr/doctopus_ocr/__init__.py
"""
Stub OCR engine ‚Äì never crashes if the real one is missing
"""

class DoctoplusWrapper:
    """
    Minimal mock that returns plausible OCR results
    """

    def __init__(self, config_path: str):
        self.config_path = config_path

    @staticmethod
    async def extract_text(
            *,
        file_path: str,
        language: str = "fra",
        preprocess: bool = True,
        confidence_threshold: float = 0.8,
        output_format: str = "text",
    ) -> dict:
        """Return mock data"""
        import asyncio
        await asyncio.sleep(0.2)  # Simulate work
        return {
            "text": f"Mock OCR text for {file_path}",
            "confidence": 0.92,
            "pages": 1,
        }
```

---

## Fichier : `backend\altiora\services\playwright\optimized_runner.py`

```python
# src/playwright/optimized_runner.py
"""Module pour un runner Playwright optimis√© avec gestion de pool de navigateurs.

Ce module fournit une classe `OptimizedPlaywrightRunner` qui g√®re un pool
de navigateurs Playwright pr√©-initialis√©s. Cela r√©duit le temps de d√©marrage
des tests et permet une ex√©cution plus efficace en r√©utilisant les instances
de navigateurs. Il inclut √©galement des optimisations pour am√©liorer la
performance des tests web.
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Any

from playwright.async_api import async_playwright, Browser, Page

logger = logging.getLogger(__name__)


class OptimizedPlaywrightRunner:
    """Runner Playwright optimis√© avec un pool de navigateurs r√©utilisables."""

    def __init__(self, max_browsers: int = 5):
        """Initialise le runner Playwright optimis√©."

        Args:
            max_browsers: Le nombre maximal d'instances de navigateurs √† maintenir dans le pool.
        """
        self.max_browsers = max_browsers
        self.browser_pool: list[Browser] = [] # Pool de navigateurs.
        self.semaphore = asyncio.Semaphore(max_browsers) # Limite le nombre de navigateurs actifs.
        self.playwright = None

    async def initialize(self):
        """Initialise le pool de navigateurs en pr√©-cr√©ant un certain nombre d'instances."

        Cette m√©thode doit √™tre appel√©e une fois au d√©marrage de l'application.
        """
        if self.playwright is not None:
            logger.warning("Playwright est d√©j√† initialis√©.")
            return

        logger.info("Initialisation de Playwright et du pool de navigateurs...")
        self.playwright = await async_playwright().start()

        # Pr√©-cr√©e un sous-ensemble de navigateurs pour r√©duire le temps de d√©marrage.
        for i in range(min(3, self.max_browsers)):
            try:
                browser = await self._create_browser()
                self.browser_pool.append(browser)
                logger.debug(f"Navigateur {i+1}/{self.max_browsers} pr√©-cr√©√©.")
            except Exception as e:
                logger.error(f"√âchec de la pr√©-cr√©ation du navigateur {i+1}: {e}")
        logger.info(f"Pool de navigateurs initialis√© avec {len(self.browser_pool)} navigateurs.")

    async def _create_browser(self) -> Browser:
        """Cr√©e une nouvelle instance de navigateur Chromium avec des arguments optimis√©s."""
        if self.playwright is None:
            raise RuntimeError("Playwright n'est pas initialis√©.")

        return await self.playwright.chromium.launch(
            headless=True, # Ex√©cute le navigateur en mode headless (sans interface graphique).
            args=[
                '--disable-blink-features=AutomationControlled', # Emp√™che la d√©tection par les sites.
                '--disable-dev-shm-usage', # Contourne les probl√®mes de m√©moire partag√©e dans Docker.
                '--no-sandbox', # N√©cessaire dans certains environnements Docker.
                '--disable-gpu', # D√©sactive l'acc√©l√©ration GPU si non n√©cessaire.
                '--disable-web-security', # Peut √™tre utile pour certains tests locaux.
                '--disable-features=IsolateOrigins,site-per-process' # Optimisations de performance.
            ]
        )

    @asynccontextmanager
    async def get_page(self) -> AsyncGenerator[Page, None]:
        """Acquiert une page Playwright depuis le pool de navigateurs."

        Utilisation avec `async with`:
        ```python
        async with runner.get_page() as page:
            await page.goto("https://example.com")
            # ... effectuer des actions sur la page.
        ```
        """
        if self.playwright is None:
            raise RuntimeError("Playwright n'est pas initialis√©. Appelez `initialize()` d'abord.")

        async with self.semaphore: # Limite le nombre de navigateurs actifs simultan√©ment.
            # R√©cup√®re un navigateur du pool ou en cr√©e un nouveau si le pool est vide.
            browser: Browser
            if self.browser_pool:
                browser = self.browser_pool.pop()
                logger.debug(f"Navigateur r√©cup√©r√© du pool. Taille restante : {len(self.browser_pool)}")
            else:
                logger.info("Pool de navigateurs vide. Cr√©ation d'un nouveau navigateur.")
                browser = await self._create_browser()

            # Cr√©e un nouveau contexte de navigateur pour isoler les tests.
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080}, # Taille de la fen√™tre du navigateur.
                ignore_https_errors=True, # Ignore les erreurs HTTPS (utile pour les environnements de test).
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36' # User-Agent personnalis√©.
            )

            page = await context.new_page()

            # Applique des optimisations de performance √† la page.
            await self._apply_optimizations(page)

            try:
                yield page # Fournit la page au bloc `async with`.
            finally:
                await context.close() # Ferme le contexte de la page.
                # Remet le navigateur dans le pool s'il n'est pas plein, sinon le ferme.
                if len(self.browser_pool) < self.max_browsers:
                    self.browser_pool.append(browser)
                    logger.debug(f"Navigateur remis dans le pool. Taille actuelle : {len(self.browser_pool)}")
                else:
                    await browser.close()
                    logger.debug("Navigateur ferm√© (pool plein).")

    async def _apply_optimizations(self, page: Page):
        """Applique des optimisations de performance √† une page Playwright."

        Ces optimisations incluent le blocage des ressources inutiles (images, CSS, polices)
        et l'interception des requ√™tes pour bloquer le tracking/analytics.

        Args:
            page: L'objet `Page` de Playwright √† optimiser.
        """
        logger.debug("Application des optimisations de performance √† la page.")
        # Bloque le chargement de certaines ressources pour acc√©l√©rer les tests.
        await page.route(re.compile(r"\.(png|jpg|jpeg|gif|svg|ico)$"), lambda route: route.abort())
        await page.route(re.compile(r"\.(css|font|woff|woff2|ttf|eot)$?"), lambda route: route.abort())

        # Intercepte les requ√™tes pour bloquer les domaines de tracking/analytics.
        async def handle_route(route):
            if 'analytics' in route.request.url or 'tracking' in route.request.url:
                await route.abort()
            else:
                await route.continue_()

        await page.route('**/*', handle_route)

    async def close(self):
        """Ferme tous les navigateurs dans le pool et arr√™te l'instance Playwright."

        Cette m√©thode doit √™tre appel√©e lors de l'arr√™t de l'application pour
        lib√©rer toutes les ressources.
        """
        logger.info("Fermeture du runner Playwright et des navigateurs...")
        for browser in self.browser_pool:
            await browser.close()
        self.browser_pool.clear()
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None
        logger.info("Runner Playwright ferm√©.")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    import re # N√©cessaire pour re.compile dans la d√©mo.

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    async def demo():
        runner = OptimizedPlaywrightRunner(max_browsers=2)
        await runner.initialize()

        print("\n--- Ex√©cution de tests simul√©s ---")
        async def run_test_task(task_id: int):
            print(f"T√¢che {task_id} : Acquisition d'une page...")
            async with runner.get_page() as page:
                print(f"T√¢che {task_id} : Page acquise. Navigation vers example.com...")
                await page.goto("https://example.com")
                title = await page.title()
                print(f"T√¢che {task_id} : Titre de la page : {title}")
                await asyncio.sleep(0.5) # Simule un travail sur la page.
            print(f"T√¢che {task_id} : Page rel√¢ch√©e.")

        # Lance plusieurs t√¢ches en parall√®le pour d√©montrer le pool.
        tasks = [run_test_task(i) for i in range(5)]
        await asyncio.gather(*tasks)

        print("\n--- Fermeture du runner ---")
        await runner.close()
        print("D√©monstration termin√©e.")

    asyncio.run(demo())
```

---

## Fichier : `backend\altiora\services\playwright\playwright_runner.py`

```python
# services/playwright/playwright_runner.py
"""Service d'ex√©cution de tests Playwright.

Ce service permet d'ex√©cuter des scripts de test Playwright de mani√®re asynchrone,
avec des options de configuration avanc√©es (navigateur, headless, timeout, retries).
Il g√®re la pr√©paration des environnements de test, l'ex√©cution parall√®le,
la collecte des artefacts (screenshots, vid√©os, traces) et la g√©n√©ration de rapports.
"""

import os
import asyncio
import json
import tempfile
import shutil
import subprocess
import uuid
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from concurrent.futures import ProcessPoolExecutor
import traceback

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field
import redis.asyncio as redis
from playwright.async_api import async_playwright
import pytest
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Mod√®les Pydantic --- #
class TestCode(BaseModel):
    """Repr√©sente un bloc de code de test √† ex√©cuter."""
    code: str = Field(..., description="Code Python/Playwright du test.")
    test_name: str = Field(default="test_generated", description="Nom unique du test.")
    test_type: str = Field(default="e2e", description="Type de test (ex: 'e2e', 'component').")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="M√©tadonn√©es suppl√©mentaires associ√©es au test.")


class ExecutionConfig(BaseModel):
    """Configuration des param√®tres d'ex√©cution des tests Playwright."""
    browser: str = Field(default="chromium", description="Navigateur √† utiliser : 'chromium', 'firefox', 'webkit'.")
    headed: bool = Field(default=False, description="Ex√©cuter le navigateur en mode visible (True) ou headless (False).")
    timeout: int = Field(default=30000, description="Timeout maximal par test en millisecondes.")
    retries: int = Field(default=0, description="Nombre de tentatives en cas d'√©chec du test.")
    parallel: bool = Field(default=True, description="Ex√©cuter les tests en parall√®le (True) ou s√©quentiellement (False).")
    workers: int = Field(default=4, description="Nombre de workers parall√®les √† utiliser si `parallel` est True.")
    screenshot: str = Field(default="on-failure", description="Quand prendre des captures d'√©cran : 'always', 'on-failure', 'never'.")
    video: str = Field(default="on-failure", description="Quand enregistrer des vid√©os : 'always', 'on-failure', 'never'.")
    trace: str = Field(default="on-failure", description="Quand enregistrer des traces : 'always', 'on-failure', 'never'.")
    base_url: Optional[str] = Field(default=None, description="URL de base pour les tests (utilis√© par `page.goto('/')`).")


class TestExecutionRequest(BaseModel):
    """Requ√™te compl√®te pour lancer une ex√©cution de tests."""
    tests: List[TestCode] = Field(..., description="Liste des tests √† ex√©cuter.")
    config: ExecutionConfig = Field(default_factory=ExecutionConfig, description="Configuration d'ex√©cution des tests.")
    save_artifacts: bool = Field(default=True, description="Sauvegarder les artefacts (screenshots, vid√©os, traces).")
    generate_report: bool = Field(default=True, description="G√©n√©rer un rapport HTML r√©capitulatif.")


class TestResult(BaseModel):
    """R√©sultat d√©taill√© d'un test individuel."""
    test_name: str = Field(..., description="Nom du test.")
    status: str = Field(..., description="Statut de l'ex√©cution du test : 'passed', 'failed', 'skipped', 'error'.")
    duration: float = Field(..., description="Dur√©e d'ex√©cution du test en secondes.")
    error_message: Optional[str] = Field(None, description="Message d'erreur si le test a √©chou√© ou a rencontr√© une erreur.")
    error_trace: Optional[str] = Field(None, description="Trace de la pile d'appels en cas d'erreur.")
    screenshot: Optional[str] = Field(None, description="Chemin relatif vers la capture d'√©cran (si disponible).")
    video: Optional[str] = Field(None, description="Chemin relatif vers la vid√©o (si disponible).")
    trace: Optional[str] = Field(None, description="Chemin relatif vers le fichier de trace (si disponible).")
    logs: List[str] = Field(default_factory=list, description="Logs de la console du test.")


class ExecutionResponse(BaseModel):
    """R√©ponse compl√®te apr√®s l'ex√©cution d'une suite de tests."""
    execution_id: str = Field(..., description="ID unique de cette ex√©cution de tests.")
    status: str = Field(..., description="Statut global de l'ex√©cution : 'completed', 'error', etc.")
    total_tests: int = Field(..., description="Nombre total de tests ex√©cut√©s.")
    passed: int = Field(..., description="Nombre de tests r√©ussis.")
    failed: int = Field(..., description="Nombre de tests √©chou√©s.")
    skipped: int = Field(..., description="Nombre de tests ignor√©s.")
    duration: float = Field(..., description="Dur√©e totale de l'ex√©cution en secondes.")
    results: List[TestResult] = Field(..., description="Liste d√©taill√©e des r√©sultats de chaque test.")
    report_path: Optional[str] = Field(None, description="Chemin relatif vers le rapport HTML g√©n√©r√©.")
    artifacts_path: Optional[str] = Field(None, description="Chemin relatif vers l'archive ZIP des artefacts.")


# --- Application FastAPI --- #
app = FastAPI(
    title="Playwright Runner Service",
    description="Service d'ex√©cution de tests Playwright √† la demande.",
    version="1.0.0"
)

# --- √âtat global du service --- #
redis_client: Optional[redis.Redis] = None
execution_queue: Dict[str, Any] = {} # Pour suivre les ex√©cutions asynchrones.
executor = ProcessPoolExecutor(max_workers=4) # Pool de processus pour les t√¢ches bloquantes.


# ------------------------------------------------------------------
# √âv√©nements de cycle de vie (Lifespan events)
# ------------------------------------------------------------------

@app.on_event("startup")
async def startup_event():
    """Initialisation du service au d√©marrage de l'application."""
    global redis_client
    
    # Tente de se connecter √† Redis pour la mise en cache des r√©sultats.
    try:
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        redis_client = await redis.from_url(redis_url, decode_responses=True)
        await redis_client.ping()
        logger.info("‚úÖ Connexion Redis √©tablie.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis non disponible ‚Äì cache d√©sactiv√© : {e}")
        redis_client = None
    
    # Cr√©e les r√©pertoires n√©cessaires pour les workspaces et les artefacts.
    for dir_path in ["workspace", "reports", "screenshots", "videos", "traces", "artifacts"]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    # S'assure que les navigateurs Playwright sont install√©s.
    await ensure_playwright_browsers()


@app.on_event("shutdown")
async def shutdown_event():
    """Nettoyage du service √† l'arr√™t de l'application."""
    if redis_client:
        await redis_client.close()
    
    # Arr√™te le pool de processus.
    executor.shutdown(wait=True)


# ------------------------------------------------------------------
# Points de terminaison de sant√©
# ------------------------------------------------------------------

@app.get("/health")
async def health_check():
    """Point de terminaison pour v√©rifier l'√©tat de sant√© du service."""
    redis_ok = False
    if redis_client:
        try:
            await redis_client.ping()
            redis_ok = True
        except redis.exceptions.ConnectionError:
            redis_ok = False
    
    # V√©rifie si Playwright est op√©rationnel.
    playwright_ok = await check_playwright_health()
    
    return {
        "status": "healthy",
        "service": "playwright-runner",
        "timestamp": datetime.now().isoformat(),
        "redis": "connect√©" if redis_ok else "d√©connect√©",
        "playwright": "pr√™t" if playwright_ok else "non_pr√™t",
        "active_executions": len(execution_queue)
    }


# ------------------------------------------------------------------
# Points de terminaison d'ex√©cution principaux
# ------------------------------------------------------------------

@app.post("/execute", response_model=ExecutionResponse)
async def execute_tests(request: TestExecutionRequest) -> ExecutionResponse:
    """Ex√©cute une suite de tests Playwright de mani√®re synchrone (bloquant l'appel API)."""
    execution_id = f"exec_{uuid.uuid4().hex[:8]}"
    start_time = asyncio.get_event_loop().time()
    
    # Cr√©e un r√©pertoire de travail temporaire pour cette ex√©cution.
    workspace_dir = Path("workspace") / execution_id
    workspace_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # Pr√©pare les fichiers de test √† partir du code fourni.
        test_files = await prepare_test_files(request.tests, workspace_dir)
        
        # G√©n√®re les arguments pytest bas√©s sur la configuration d'ex√©cution.
        pytest_config_args = generate_pytest_config(request.config, workspace_dir)
        
        # Ex√©cute les tests en parall√®le ou s√©quentiellement.
        if request.config.parallel and len(test_files) > 1:
            results = await run_tests_parallel(
                test_files,
                pytest_config_args,
                request.config,
                workspace_dir
            )
        else:
            results = await run_tests_sequential(
                test_files,
                pytest_config_args,
                request.config,
                workspace_dir
            )
        
        # Calcule les statistiques globales de l'ex√©cution.
        total_duration = asyncio.get_event_loop().time() - start_time
        stats = calculate_stats(results)
        
        # G√©n√®re le rapport HTML si demand√©.
        report_path = None
        if request.generate_report:
            report_path = await generate_html_report(
                execution_id,
                results,
                stats,
                workspace_dir
            )
        
        # Collecte et archive les artefacts si demand√©.
        artifacts_path = None
        if request.save_artifacts:
            artifacts_path = await collect_artifacts(execution_id, workspace_dir)
        
        response = ExecutionResponse(
            execution_id=execution_id,
            status="completed",
            total_tests=len(results),
            passed=stats["passed"],
            failed=stats["failed"],
            skipped=stats["skipped"],
            duration=total_duration,
            results=results,
            report_path=report_path,
            artifacts_path=artifacts_path
        )
        
        # Sauvegarde le r√©sultat complet de l'ex√©cution dans Redis.
        if redis_client:
            await save_execution_result(execution_id, response)
        
        return response
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ex√©cution des tests : {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur interne du service : {str(e)}")
    
    finally:
        # Planifie le nettoyage du r√©pertoire de travail apr√®s un d√©lai.
        asyncio.create_task(cleanup_workspace(workspace_dir, delay=300))


@app.post("/execute_async")
async def execute_tests_async(
    request: TestExecutionRequest,
    background_tasks: BackgroundTasks
):
    """Lance l'ex√©cution des tests en arri√®re-plan et retourne imm√©diatement un ID d'ex√©cution."""
    execution_id = f"exec_{uuid.uuid4().hex[:8]}"
    
    # Enregistre l'ex√©cution dans la queue avec un statut initial.
    execution_queue[execution_id] = {
        "status": "queued",
        "started_at": datetime.now().isoformat(),
        "config": request.config.model_dump() # Utilise model_dump pour Pydantic v2
    }
    
    # Lance la fonction d'ex√©cution en arri√®re-plan.
    background_tasks.add_task(
        run_tests_background,
        execution_id,
        request
    )
    
    return JSONResponse(
        {
            "execution_id": execution_id,
            "status": "queued",
            "message": "Tests en cours d'ex√©cution en arri√®re-plan.",
            "check_status_url": f"/status/{execution_id}"
        },
        status_code=202 # Accepted
    )


@app.get("/status/{execution_id}", response_model=Union[ExecutionResponse, Dict[str, str]])
async def get_execution_status(execution_id: str):
    """R√©cup√®re le statut et les r√©sultats d'une ex√©cution de tests par son ID."""
    # V√©rifie d'abord dans la queue des ex√©cutions en cours.
    if execution_id in execution_queue:
        return execution_queue[execution_id]
    
    # Sinon, v√©rifie dans le cache Redis.
    if redis_client:
        result = await get_execution_result(execution_id)
        if result:
            return result
    
    raise HTTPException(status_code=404, detail="Ex√©cution non trouv√©e.")


@app.get("/report/{execution_id}", response_class=FileResponse)
async def get_report(execution_id: str):
    """R√©cup√®re le rapport HTML g√©n√©r√© pour une ex√©cution donn√©e."""
    report_path = Path("reports") / f"{execution_id}_report.html"
    
    if not report_path.exists():
        raise HTTPException(status_code=404, detail="Rapport non trouv√©.")
    
    return FileResponse(
        report_path,
        media_type="text/html",
        filename=f"report_{execution_id}.html"
    )


@app.get("/artifacts/{execution_id}", response_class=FileResponse)
async def get_artifacts(execution_id: str):
    """T√©l√©charge une archive ZIP contenant tous les artefacts (screenshots, vid√©os, traces) d'une ex√©cution."""
    artifacts_path = Path("artifacts") / f"{execution_id}.zip"
    
    if not artifacts_path.exists():
        raise HTTPException(status_code=404, detail="Artefacts non trouv√©s.")
    
    return FileResponse(
        artifacts_path,
        media_type="application/zip",
        filename=f"artifacts_{execution_id}.zip"
    )


# ------------------------------------------------------------------
# Fonctions d'ex√©cution principales
# ------------------------------------------------------------------

async def prepare_test_files(tests: List[TestCode], workspace_dir: Path) -> List[Path]:
    """Pr√©pare les fichiers de test Python dans le r√©pertoire de travail temporaire."""
    test_files = []
    
    for i, test in enumerate(tests):
        test_name = test.test_name or f"test_{i}"
        file_name = f"{test_name}.py"
        file_path = workspace_dir / file_name
        
        # S'assure que le code de test contient les imports et d√©corateurs n√©cessaires.
        code = ensure_test_imports(test.code)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code)
            
            test_files.append(file_path)
            logger.info(f"Fichier de test pr√©par√© : {file_path}")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture du fichier de test {file_path}: {e}")
            raise HTTPException(status_code=500, detail=f"√âchec de l'√©criture du fichier de test : {file_path}")
    
    # Cr√©e le fichier `conftest.py` n√©cessaire pour la configuration de pytest-playwright.
    await create_conftest(workspace_dir)
    
    return test_files


def ensure_test_imports(code: str) -> str:
    """Ajoute les imports et d√©corateurs pytest-playwright n√©cessaires au code de test."""
    required_imports = [
        "import pytest",
        "from playwright.async_api import Page, expect",
    ]
    
    # Ajoute les imports manquants au d√©but du code.
    for imp in required_imports:
        if imp not in code:
            code = f"{imp}\n{code}"
    
    # Ajoute le d√©corateur `@pytest.mark.asyncio` si la fonction de test est asynchrone.
    if "@pytest.mark.asyncio" not in code and "async def test_" in code:
        code = code.replace("async def test_", "@pytest.mark.asyncio\nasync def test_")
    
    return code


async def create_conftest(workspace_dir: Path):
    """Cr√©e le fichier `conftest.py` avec la configuration de base pour pytest-playwright."""
    conftest_content = '''"""
Configuration Playwright pour les tests
"""
import pytest
from playwright.async_api import async_playwright
import os

@pytest.fixture(scope="session")
async def browser_type_launch_args():
    """Arguments de lancement du navigateur."""
    return {
        "headless": not bool(os.getenv("HEADED", "false").lower() == "true"),
        "timeout": 30000,
    }

@pytest.fixture(scope="session")
async def browser_context_args(browser_type_launch_args):
    """Arguments du contexte navigateur."""
    base_url = os.getenv("BASE_URL")
    context_args = {
        "viewport": {"width": 1280, "height": 720},
        "ignore_https_errors": True,
    }
    if base_url:
        context_args["base_url"] = base_url
    return context_args

@pytest.fixture(scope="function")
async def page(browser, browser_context_args):
    """Fixture de page avec configuration."""
    context = await browser.new_context(**browser_context_args)
    page = await context.new_page()
    yield page
    await context.close()
'''
    
    conftest_path = workspace_dir / "conftest.py"
    try:
        with open(conftest_path, 'w', encoding='utf-8') as f:
            f.write(conftest_content)
    except (IOError, OSError) as e:
        logger.error(f"Erreur lors de l'√©criture de conftest.py sur {conftest_path}: {e}")
        raise HTTPException(status_code=500, detail=f"√âchec de l'√©criture de conftest.py : {conftest_path}")


def generate_pytest_config(config: ExecutionConfig, workspace_dir: Path) -> List[str]:
    """G√©n√®re la liste des arguments de ligne de commande pour pytest en fonction de la configuration."""
    args = [
        str(workspace_dir), # Indique √† pytest o√π trouver les tests.
        "-v", # Mode verbeux.
        "--tb=short", # Traceback courte pour une meilleure lisibilit√©.
        f"--maxfail={config.retries + 1}", # Arr√™te apr√®s N √©checs (retries + 1).
        "--json-report", # Active le rapport JSON.
        f"--json-report-file={workspace_dir}/report.json", # Chemin du rapport JSON.
    ]
    
    # Ajoute les arguments sp√©cifiques au navigateur.
    args.extend([
        f"--browser={config.browser}",
        "--browser-channel=chromium" if config.browser == "chromium" else "", # Sp√©cifique √† Chromium
    ])
    
    if config.headed:
        args.append("--headed") # Lance le navigateur en mode visible.
    
    # Options de capture d'√©cran.
    if config.screenshot == "always":
        args.append("--screenshot=on")
    elif config.screenshot == "on-failure":
        args.append("--screenshot=only-on-failure")
    
    # Options d'enregistrement vid√©o.
    if config.video == "always":
        args.append("--video=on")
    elif config.video == "on-failure":
        args.append("--video=retain-on-failure")
    
    # Options de trace Playwright.
    if config.trace == "always":
        args.append("--tracing=on")
    elif config.trace == "on-failure":
        args.append("--tracing=retain-on-failure")
    
    # Parall√©lisation avec pytest-xdist.
    if config.parallel and config.workers > 1:
        args.extend(["-n", str(config.workers)])
    
    # Timeout global pour l'ex√©cution des tests.
    args.append(f"--timeout={config.timeout // 1000}") # Convertit ms en secondes.
    
    return [arg for arg in args if arg] # Filtre les arguments vides.


async def run_tests_sequential(
    test_files: List[Path],
    pytest_args: List[str],
    config: ExecutionConfig,
    workspace_dir: Path
) -> List[TestResult]:
    """Ex√©cute les tests s√©quentiellement, un fichier √† la fois."""
    results = []
    
    for test_file in test_files:
        result = await run_single_test(
            test_file,
            pytest_args,
            config,
            workspace_dir
        )
        results.append(result)
    
    return results


async def run_tests_parallel(
    test_files: List[Path],
    pytest_args: List[str],
    config: ExecutionConfig,
    workspace_dir: Path
) -> List[TestResult]:
    """Ex√©cute les tests en parall√®le en utilisant pytest-xdist."""
    # pytest-xdist est utilis√© via les arguments pass√©s √† pytest.
    cmd = [sys.executable, "-m", "pytest"] + pytest_args
    
    # Configure les variables d'environnement pour le sous-processus pytest.
    env = os.environ.copy()
    env["PYTHONPATH"] = str(workspace_dir) # Ajoute le workspace au PYTHONPATH.
    if config.base_url:
        env["BASE_URL"] = config.base_url
    if config.headed:
        env["HEADED"] = "true"
    
    logger.info(f"Lancement de pytest en parall√®le : {' '.join(cmd)}")
    process = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
        env=env,
        cwd=str(workspace_dir) # Ex√©cute pytest dans le r√©pertoire de travail.
    )
    
    stdout, stderr = await process.communicate()
    
    if process.returncode != 0:
        logger.error(f"Pytest a √©chou√© (code {process.returncode}):\n{stderr.decode()}")

    # Parse le rapport JSON g√©n√©r√© par pytest.
    report_path = workspace_dir / "report.json"
    if report_path.exists():
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            return parse_pytest_report(report, workspace_dir)
        except (IOError, OSError, json.JSONDecodeError) as e:
            logger.error(f"Erreur lors de la lecture ou du parsing du rapport pytest {report_path}: {e}")
            return [TestResult(
                test_name="suite_complete",
                status="error",
                duration=0,
                error_message=f"√âchec de la lecture/parsing du rapport : {e}",
                error_trace=stderr.decode() if stderr else None
            )]
    else:
        logger.error(f"Le rapport JSON n'a pas √©t√© g√©n√©r√© √† {report_path}. Stdout: {stdout.decode()}, Stderr: {stderr.decode()}")
        return [TestResult(
            test_name="suite_complete",
            status="error",
            duration=0,
            error_message="Rapport JSON non trouv√©.",
            error_trace=stderr.decode() if stderr else None
        )]


async def run_single_test(
    test_file: Path,
    base_pytest_args: List[str],
    config: ExecutionConfig,
    workspace_dir: Path
) -> TestResult:
    """Ex√©cute un seul fichier de test Playwright via pytest."""
    # Construit les arguments pytest sp√©cifiques √† ce fichier.
    pytest_args = [str(test_file)] + [arg for arg in base_pytest_args if arg not in [str(workspace_dir), '-n', str(config.workers)]]
    
    cmd = [sys.executable, "-m", "pytest"] + pytest_args
    
    # Configure les variables d'environnement.
    env = os.environ.copy()
    env["PYTHONPATH"] = str(workspace_dir)
    if config.base_url:
        env["BASE_URL"] = config.base_url
    
    start_time = asyncio.get_event_loop().time()
    
    logger.info(f"Lancement du test : {' '.join(cmd)}")
    process = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
        env=env,
        cwd=str(workspace_dir)
    )
    
    stdout, stderr = await process.communicate()
    duration = asyncio.get_event_loop().time() - start_time
    
    # D√©termine le statut du test bas√© sur le code de retour.
    if process.returncode == 0:
        status = "passed"
    elif process.returncode == 1: # pytest retourne 1 pour les √©checs de test.
        status = "failed"
    else:
        status = "error"
    
    # Collecte les artefacts g√©n√©r√©s par ce test.
    artifacts = collect_test_artifacts(test_file.stem, workspace_dir)
    
    return TestResult(
        test_name=test_file.stem,
        status=status,
        duration=duration,
        error_message=stderr.decode() if status != "passed" and stderr else None,
        logs=stdout.decode().split('\n') if stdout else [],
        **artifacts
    )


def parse_pytest_report(report: Dict, workspace_dir: Path) -> List[TestResult]:
    """Parse le rapport JSON g√©n√©r√© par pytest et le convertit en liste de `TestResult`."""
    results = []
    
    for test_data in report.get("tests", []):
        test_name = test_data.get("nodeid", "unknown").split("::")[-1]
        outcome = test_data.get("outcome", "unknown")
        
        status_map = {
            "passed": "passed",
            "failed": "failed",
            "skipped": "skipped",
            "error": "error"
        }
        status = status_map.get(outcome, "error")
        
        error_message = None
        error_trace = None
        if status == "failed" or status == "error":
            # Tente d'extraire le message d'erreur et la trace.
            if "call" in test_data and "longrepr" in test_data["call"]:
                error_message = str(test_data["call"]["longrepr"])
            elif "setup" in test_data and "longrepr" in test_data["setup"]:
                error_message = str(test_data["setup"]["longrepr"])
            # Une trace compl√®te peut √™tre extraite si n√©cessaire.
            # error_trace = test_data.get("longrepr", {}).get("reprcrash", {}).get("message")
        
        duration = test_data.get("duration", 0)
        
        artifacts = collect_test_artifacts(test_name, workspace_dir)
        
        results.append(TestResult(
            test_name=test_name,
            status=status,
            duration=duration,
            error_message=error_message,
            error_trace=error_trace,
            **artifacts
        ))
    
    return results


def collect_test_artifacts(test_name: str, workspace_dir: Path) -> Dict[str, Optional[str]]:
    """Collecte les chemins relatifs des artefacts (screenshots, vid√©os, traces) pour un test donn√©."""
    artifacts = {
        "screenshot": None,
        "video": None,
        "trace": None
    }
    
    # Patterns de recherche pour les artefacts g√©n√©r√©s par Playwright/pytest.
    # Note: Les chemins peuvent varier l√©g√®rement en fonction de la configuration de pytest-playwright.
    patterns = {
        "screenshot": [f"**/{test_name}*.png"],
        "video": [f"**/{test_name}*.webm"],
        "trace": [f"**/{test_name}*.zip"]
    }
    
    for artifact_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            # Recherche les fichiers correspondants dans le r√©pertoire de travail.
            files = list(workspace_dir.glob(pattern))
            if files:
                # Prend le premier fichier trouv√© et stocke son chemin relatif.
                artifacts[artifact_type] = str(files[0].relative_to(workspace_dir))
                break
    
    return artifacts


def calculate_stats(results: List[TestResult]) -> Dict[str, int]:
    """Calcule les statistiques r√©capitulatives d'une liste de r√©sultats de tests."""
    stats = {
        "passed": sum(1 for r in results if r.status == "passed"),
        "failed": sum(1 for r in results if r.status == "failed"),
        "skipped": sum(1 for r in results if r.status == "skipped"),
        "error": sum(1 for r in results if r.status == "error")
    }
    return stats


# ------------------------------------------------------------------
# T√¢ches de fond
# ------------------------------------------------------------------

async def run_tests_background(execution_id: str, request: TestExecutionRequest):
    """Fonction ex√©cut√©e en arri√®re-plan pour lancer les tests et mettre √† jour leur statut."""
    try:
        # Met √† jour le statut de l'ex√©cution dans la queue.
        execution_queue[execution_id]["status"] = "running"
        
        # Ex√©cute les tests en appelant la fonction principale synchrone.
        response = await execute_tests(request)
        
        # Met √† jour la queue avec les r√©sultats complets.
        execution_queue[execution_id] = response.model_dump() # Utilise model_dump pour Pydantic v2
        execution_queue[execution_id]["completed_at"] = datetime.now().isoformat()
        
    except Exception as e:
        # G√®re les erreurs survenues pendant l'ex√©cution en arri√®re-plan.
        logger.error(f"Erreur lors de l'ex√©cution en arri√®re-plan pour {execution_id}: {e}", exc_info=True)
        execution_queue[execution_id].update({
            "status": "error",
            "error": str(e),
            "error_trace": traceback.format_exc(),
            "completed_at": datetime.now().isoformat()
        })


async def cleanup_workspace(workspace_dir: Path, delay: int = 300):
    """Nettoie le r√©pertoire de travail temporaire apr√®s un d√©lai sp√©cifi√©."""
    await asyncio.sleep(delay)
    
    try:
        if workspace_dir.exists():
            shutil.rmtree(workspace_dir)
            logger.info(f"R√©pertoire de travail nettoy√© : {workspace_dir}")
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage du r√©pertoire de travail {workspace_dir}: {e}")


# ------------------------------------------------------------------
# G√©n√©ration de rapports
# ------------------------------------------------------------------

async def generate_html_report(
    execution_id: str,
    results: List[TestResult],
    stats: Dict[str, int],
    workspace_dir: Path
) -> str:
    """G√©n√®re un rapport HTML r√©capitulatif des r√©sultats de l'ex√©cution des tests."""
    report_path = Path("reports") / f"{execution_id}_report.html"
    
    # Template HTML pour le rapport.
    html_template = """
<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Test - {execution_id}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .stats {{ display: flex; gap: 20px; margin: 20px 0; }}
        .stat {{ padding: 10px 20px; border-radius: 5px; color: white; }}
        .passed {{ background: #4CAF50; }} /* Vert */
        .failed {{ background: #f44336; }} /* Rouge */
        .skipped {{ background: #ff9800; }} /* Orange */
        .error {{ background: #9c27b0; }} /* Violet */
        table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background: #f2f2f2; }}
        .status-passed {{ color: #4CAF50; font-weight: bold; }}
        .status-failed {{ color: #f44336; font-weight: bold; }}
        .status-skipped {{ color: #ff9800; font-weight: bold; }}
        .status-error {{ color: #9c27b0; font-weight: bold; }}
        .error-details {{ background: #ffebee; color: #c62828; padding: 10px; margin: 5px 0; border-radius: 3px; font-family: monospace; white-space: pre-wrap; word-break: break-all; }}
        .artifact-link {{ margin-left: 10px; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Rapport d'Ex√©cution des Tests Playwright</h1>
        <p>ID d'Ex√©cution : <strong>{execution_id}</strong></p>
        <p>G√©n√©r√© le : {timestamp}</p>
        <p>Dur√©e totale : {total_duration:.2f} secondes</p>
    </div>
    
    <div class="stats">
        <div class="stat passed">R√©ussis : {passed}</div>
        <div class="stat failed">√âchou√©s : {failed}</div>
        <div class="stat skipped">Ignor√©s : {skipped}</div>
        <div class="stat error">Erreurs : {error}</div>
    </div>
    
    <h2>R√©sultats D√©taill√©s des Tests</h2>
    <table>
        <thead>
            <tr>
                <th>Nom du Test</th>
                <th>Statut</th>
                <th>Dur√©e</th>
                <th>D√©tails / Artefacts</th>
            </tr>
        </thead>
        <tbody>
            {test_rows}
        </tbody>
    </table>
</body>
</html>
"""
    
    # G√©n√®re les lignes du tableau pour chaque r√©sultat de test.
    test_rows = []
    for result in results:
        error_section = ""
        if result.error_message:
            error_section = f'<div class="error-details"><pre>{result.error_message}</pre></div>'
        
        artifact_links = []
        if result.screenshot:
            artifact_links.append(f'<a href="../artifacts/{execution_id}/{result.screenshot}" target="_blank" class="artifact-link">Screenshot</a>')
        if result.video:
            artifact_links.append(f'<a href="../artifacts/{execution_id}/{result.video}" target="_blank" class="artifact-link">Vid√©o</a>')
        if result.trace:
            artifact_links.append(f'<a href="../artifacts/{execution_id}/{result.trace}" target="_blank" class="artifact-link">Trace</a>')
        
        test_rows.append(f"""
        <tr>
            <td>{result.test_name}</td>
            <td class="status-{result.status}">{result.status.upper()}</td>
            <td>{result.duration:.2f}s</td>
            <td>
                {error_section}
                {' '.join(artifact_links)}
            </td>
        </tr>
        """
        )
    
    # Remplit le template HTML avec les donn√©es et les lignes de test.
    html_content = html_template.format(
        execution_id=execution_id,
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        total_duration=sum(r.duration for r in results),
        passed=stats["passed"],
        failed=stats["failed"],
        skipped=stats["skipped"],
        error=stats.get("error", 0),
        test_rows="\n".join(test_rows)
    )
    
    # √âcrit le rapport HTML dans le fichier.
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    except (IOError, OSError) as e:
        logger.error(f"Erreur lors de l'√©criture du rapport HTML sur {report_path}: {e}")
        raise HTTPException(status_code=500, detail=f"√âchec de l'√©criture du rapport HTML : {report_path}")
    
    return str(report_path)


async def collect_artifacts(execution_id: str, workspace_dir: Path) -> str:
    """Collecte tous les artefacts g√©n√©r√©s (screenshots, vid√©os, traces) et les archive dans un fichier ZIP."""
    import zipfile
    
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True) # S'assure que le r√©pertoire des artefacts existe.
    
    zip_path = artifacts_dir / f"{execution_id}.zip"
    
    try:
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Ajoute tous les fichiers pertinents du workspace √† l'archive ZIP.
            for pattern in ["**/*.png", "**/*.webm", "**/*.zip", "**/*.json", "**/trace.zip"]:
                for file_path in workspace_dir.glob(pattern):
                    if file_path.is_file():
                        # Ajoute le fichier √† l'archive en conservant sa structure de r√©pertoire relative.
                        arcname = file_path.relative_to(workspace_dir)
                        zipf.write(file_path, arcname)
    except (IOError, OSError) as e:
        logger.error(f"Erreur lors de la cr√©ation de l'archive ZIP des artefacts {zip_path}: {e}")
        raise HTTPException(status_code=500, detail=f"√âchec de la cr√©ation de l'archive des artefacts : {zip_path}")
    
    return str(zip_path)


# ------------------------------------------------------------------
# Fonctions utilitaires
# ------------------------------------------------------------------

async def ensure_playwright_browsers():
    """S'assure que les navigateurs Playwright n√©cessaires sont install√©s."""
    logger.info("V√©rification de l'installation des navigateurs Playwright...")
    try:
        # Tente de lancer chaque navigateur pour v√©rifier son installation.
        async with async_playwright() as p:
            for browser_name in ["chromium", "firefox", "webkit"]:
                try:
                    browser = await getattr(p, browser_name).launch(headless=True)
                    await browser.close()
                    logger.info(f"  ‚úÖ Navigateur {browser_name} est disponible.")
                except Exception:
                    logger.warning(f"  ‚ö†Ô∏è Navigateur {browser_name} non disponible. Tentative d'installation...")
                    # Tente d'installer le navigateur manquant.
                    try:
                        subprocess.run(["playwright", "install", browser_name], check=True)
                        logger.info(f"  ‚úÖ Navigateur {browser_name} install√© avec succ√®s.")
                    except Exception as install_e:
                        logger.error(f"  ‚ùå Impossible d'installer le navigateur {browser_name}: {install_e}")
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification/installation des navigateurs Playwright: {e}")


async def check_playwright_health() -> bool:
    """V√©rifie si l'environnement Playwright est op√©rationnel en lan√ßant un navigateur simple."""
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            await browser.close()
            return True
    except Exception as e:
        logger.error(f"Playwright n'est pas op√©rationnel : {e}")
        return False


async def save_execution_result(execution_id: str, result: ExecutionResponse):
    """Sauvegarde le r√©sultat complet d'une ex√©cution de tests dans Redis."""
    if not redis_client:
        logger.warning("Redis n'est pas connect√©, le r√©sultat de l'ex√©cution ne sera pas mis en cache.")
        return
    
    try:
        key = f"execution:{execution_id}"
        # Convertit l'objet Pydantic en dictionnaire, puis en JSON.
        await redis_client.setex(
            key,
            86400,  # Dur√©e de vie du cache : 24 heures.
            json.dumps(result.model_dump(), default=str) # Utilise model_dump pour Pydantic v2
        )
        logger.info(f"R√©sultat de l'ex√©cution {execution_id} sauvegard√© dans Redis.")
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du r√©sultat dans Redis pour {execution_id}: {e}")


async def get_execution_result(execution_id: str) -> Optional[Dict]:
    """R√©cup√®re un r√©sultat d'ex√©cution depuis Redis par son ID."""
    if not redis_client:
        return None
    
    try:
        key = f"execution:{execution_id}"
        data = await redis_client.get(key)
        if data:
            logger.info(f"R√©sultat de l'ex√©cution {execution_id} r√©cup√©r√© depuis Redis.")
            return json.loads(data)
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du r√©sultat depuis Redis pour {execution_id}: {e}")
    
    return None


# ------------------------------------------------------------------
# Points de terminaison additionnels
# ------------------------------------------------------------------

@app.delete("/cleanup")
async def cleanup_old_executions(days: int = 7):
    """Nettoie les anciens r√©pertoires de travail, rapports et artefacts.

    Args:
        days: Nombre de jours apr√®s lesquels les fichiers sont consid√©r√©s comme anciens et supprim√©s.

    Returns:
        Un dictionnaire r√©capitulatif des √©l√©ments nettoy√©s.
    """
    cutoff_date = datetime.now() - timedelta(days=days)
    cleaned = {
        "workspaces": 0,
        "reports": 0,
        "artifacts": 0
    }
    
    logger.info(f"D√©marrage du nettoyage des fichiers de plus de {days} jours...")

    # Nettoie les workspaces.
    for workspace in Path("workspace").iterdir():
        if workspace.is_dir() and workspace.stat().st_mtime < cutoff_date.timestamp():
            try:
                shutil.rmtree(workspace)
                cleaned["workspaces"] += 1
            except Exception as e:
                logger.error(f"Erreur lors de la suppression du workspace {workspace}: {e}")
    
    # Nettoie les rapports HTML.
    for report in Path("reports").glob("*.html"):
        if report.stat().st_mtime < cutoff_date.timestamp():
            try:
                report.unlink()
                cleaned["reports"] += 1
            except Exception as e:
                logger.error(f"Erreur lors de la suppression du rapport {report}: {e}")
    
    # Nettoie les archives d'artefacts.
    for artifact in Path("artifacts").glob("*.zip"):
        if artifact.stat().st_mtime < cutoff_date.timestamp():
            try:
                artifact.unlink()
                cleaned["artifacts"] += 1
            except Exception as e:
                logger.error(f"Erreur lors de la suppression de l'artefact {artifact}: {e}")
    
    logger.info(f"Nettoyage termin√©. R√©sum√© : {cleaned}")
    return {
        "status": "success",
        "cleaned": cleaned,
        "message": f"Nettoyage des fichiers de plus de {days} jours effectu√©."
    }


@app.get("/stats")
async def get_stats():
    """Retourne des statistiques sur l'utilisation actuelle du service."""
    stats = {
        "service": "playwright-runner",
        "timestamp": datetime.now().isoformat(),
        "active_executions": len(execution_queue), # Nombre d'ex√©cutions en cours.
        "workspace_count": len(list(Path("workspace").iterdir())),
        "report_count": len(list(Path("reports").glob("*.html"))),
        "artifact_count": len(list(Path("artifacts").glob("*.zip")))
    }
    
    # Ajoute des statistiques sur le cache Redis si disponible.
    if redis_client:
        try:
            exec_count = 0
            async for _ in redis_client.scan_iter("execution:*"):
                exec_count += 1
            stats["cached_executions"] = exec_count
        except redis.exceptions.ConnectionError:
            stats["cached_executions"] = "erreur de connexion Redis"
    
    return stats


# --- Point d'entr√©e Uvicorn --- #
if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "playwright_runner:app", # Nom du module:objet FastAPI
        host="0.0.0.0",
        port=8004,
        log_level="info",
        reload=True # Utile pour le d√©veloppement
    )

```

---

## Fichier : `backend\altiora\services\playwright\README.md`

```markdown
# Service Playwright

Ce service ex√©cute des tests d'automatisation web avec Playwright.

## D√©marrage Isol√©

Pour lancer ce service de mani√®re isol√©e, vous pouvez utiliser le `Dockerfile` pr√©sent dans ce r√©pertoire.

1.  **Construire l'image Docker :**

    ```bash
    docker build -t altiora-playwright-service .
    ```

2.  **Lancer le conteneur :**

    ```bash
    docker run -p 8004:8004 -e PLAYWRIGHT_SERVICE_PORT=8004 altiora-playwright-service
    ```

## Endpoints

-   `POST /playwright/run-test` : Ex√©cute un script de test Playwright.

## Variables d'Environnement

-   `PLAYWRIGHT_SERVICE_PORT` : Le port sur lequel le service √©coute (d√©faut : `8004`).

```

---

## Fichier : `backend\altiora\services\playwright\__init__.py`

```python

```

---

## Fichier : `backend\altiora\utils\errors.py`

```python
# src/error_management.py
"""Architecture centralis√©e de gestion des erreurs, de retry et de disjoncteur pour Altiora.

Ce module fournit un ensemble complet d'outils pour g√©rer les erreurs de mani√®re
robuste et r√©siliente dans une application asynchrone. Il inclut des exceptions
personnalis√©es, un gestionnaire de retry, un disjoncteur (circuit breaker),
un logger d'erreurs chiffr√©, et un gestionnaire de contexte pour la propagation
des erreurs.
"""

import json
import logging
import os
import traceback
from datetime import datetime, timedelta
from functools import wraps
from pathlib import Path
from typing import Any, Dict, Callable, Optional, Type

import aiofiles
from cryptography.fernet import Fernet  # Biblioth√®que de chiffrement sym√©trique.
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# Configuration des erreurs
# ------------------------------------------------------------------
ERROR_CONFIG = {
    "max_retries": int(os.getenv("MAX_RETRIES", 3)),
    "backoff_factor": float(os.getenv("BACKOFF_FACTOR", 2.0)),
    "circuit_breaker_timeout": int(os.getenv("CB_TIMEOUT", 60)), # Temps en secondes pendant lequel le disjoncteur reste ouvert.
    "log_file": Path(os.getenv("ERROR_LOG", "logs/errors.jsonl")), # Chemin du fichier de log des erreurs.
    "encryption_key": os.getenv("LOGS_ENCRYPTION_KEY"), # Cl√© pour chiffrer les logs d'erreurs.
}


# ------------------------------------------------------------------
# Exceptions personnalis√©es
# ------------------------------------------------------------------
class AltioraError(Exception):
    """Exception de base pour toutes les erreurs sp√©cifiques √† Altiora."""
    pass


class ServiceError(AltioraError):
    """Exception lev√©e lorsqu'un service externe rencontre une erreur."

    Utilis√©e pour les probl√®mes de communication ou de disponibilit√© des microservices.
    """
    pass


class ValidationError(AltioraError):
    """Exception lev√©e lorsqu'une validation m√©tier √©choue."

    Indique que les donn√©es ne sont pas conformes aux r√®gles de l'application.
    """
    pass


# ------------------------------------------------------------------
# Assistant de chiffrement
# ------------------------------------------------------------------
class CryptoHelper:
    """Aide au chiffrement/d√©chiffrement de dictionnaires en utilisant Fernet."""

    def __init__(self, key: Optional[str] = None):
        """Initialise l'aide au chiffrement."

        Args:
            key: La cl√© Fernet encod√©e en base64 URL-safe. Si None, une cl√© est g√©n√©r√©e.
        """
        if key:
            self.key = key
        else:
            self.key = Fernet.generate_key().decode() # G√©n√®re une cl√© si non fournie.
            logger.warning("Aucune cl√© de chiffrement fournie pour les logs. G√©n√©ration d'une cl√© temporaire. NE PAS UTILISER EN PRODUCTION.")
        self.fernet = Fernet(self.key.encode() if isinstance(self.key, str) else self.key)

    def encrypt_dict(self, data: Dict[str, Any]) -> str:
        """Chiffre un dictionnaire en JSON puis avec Fernet."

        Args:
            data: Le dictionnaire √† chiffrer.

        Returns:
            La cha√Æne chiffr√©e.
        """
        payload = json.dumps(data, ensure_ascii=False, separators=(",", ":")).encode('utf-8')
        return self.fernet.encrypt(payload).decode('utf-8')

    def decrypt_dict(self, token: str) -> Dict[str, Any]:
        """D√©chiffre une cha√Æne chiffr√©e en un dictionnaire."

        Args:
            token: La cha√Æne chiffr√©e.

        Returns:
            Le dictionnaire d√©chiffr√©.
        """
        return json.loads(self.fernet.decrypt(token.encode('utf-8')).decode('utf-8'))


# Initialise l'aide au chiffrement avec la cl√© de configuration.
crypto = CryptoHelper(ERROR_CONFIG["encryption_key"])


# ------------------------------------------------------------------
# Gestionnaire de Retry
# ------------------------------------------------------------------
class RetryHandler:
    """Fournit un d√©corateur pour appliquer des strat√©gies de nouvelle tentative aux fonctions asynchrones."""

    @staticmethod
    def with_retry(
            max_attempts: int = ERROR_CONFIG["max_retries"],
            exceptions: tuple[Type[Exception], ...] = (Exception,),
    ) -> Callable:
        """D√©corateur pour retenter l'ex√©cution d'une fonction asynchrone en cas d'√©chec."

        Args:
            max_attempts: Le nombre maximal de tentatives d'ex√©cution.
            exceptions: Un tuple d'exceptions pour lesquelles la fonction doit √™tre retent√©e.

        Returns:
            Un d√©corateur qui peut √™tre appliqu√© √† une fonction asynchrone.
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            @retry(
                stop=stop_after_attempt(max_attempts),
                wait=wait_exponential(
                    multiplier=ERROR_CONFIG["backoff_factor"], max=30
                ),
                retry=retry_if_exception_type(exceptions), # Retente si l'exception est du type sp√©cifi√©.
                reraise=True # R√©l√®ve l'exception apr√®s toutes les tentatives.
            )
            async def wrapper(*args, **kwargs):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logger.warning("Retry pour %s: %s", func.__name__, e)
                    raise

            return wrapper

        return decorator


# ------------------------------------------------------------------
# Disjoncteur (Circuit Breaker)
# ------------------------------------------------------------------
class CircuitBreaker:
    """Impl√©mente le pattern Circuit Breaker pour prot√©ger contre les cascades d'erreurs."""

    def __init__(
            self,
            failure_threshold: int = 5,
            timeout: int = ERROR_CONFIG["circuit_breaker_timeout"],
    ):
        """Initialise le disjoncteur."

        Args:
            failure_threshold: Nombre d'√©checs cons√©cutifs avant que le disjoncteur ne s'ouvre.
            timeout: Dur√©e en secondes pendant laquelle le disjoncteur reste ouvert.
        """
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self._failures: Dict[str, int] = {} # Compteur d'√©checs par service.
        self._last_failure: Dict[str, datetime] = {} # Horodatage du dernier √©chec par service.
        self._is_open: Dict[str, bool] = {} # √âtat du disjoncteur par service (ouvert/ferm√©).

    async def call_with_protection(
            self,
            service_name: str,
            coro: Callable,
            *args,
            **kwargs,
    ) -> Any:
        """Appelle une coroutine avec la protection du disjoncteur."

        Args:
            service_name: Le nom du service √† prot√©ger.
            coro: La coroutine √† ex√©cuter.
            *args, **kwargs: Arguments √† passer √† la coroutine.

        Returns:
            Le r√©sultat de la coroutine.

        Raises:
            ServiceError: Si le disjoncteur est ouvert pour ce service.
            Exception: Toute exception lev√©e par la coroutine.
        """
        # V√©rifie si le disjoncteur est ouvert.
        if self._is_open.get(service_name, False):
            # Si ouvert, v√©rifie si le timeout de r√©cup√©ration est pass√©.
            if (datetime.now() - self._last_failure.get(service_name, datetime.min)) < timedelta(seconds=self.timeout):
                raise ServiceError(f"Disjoncteur ouvert pour le service `{service_name}`. Op√©ration bloqu√©e.")
            # Si le timeout est pass√©, tente de r√©initialiser le disjoncteur (√©tat "semi-ouvert").
            self.reset(service_name)

        try:
            result = await coro(*args, **kwargs)
            self.reset(service_name) # R√©initialise le disjoncteur en cas de succ√®s.
            return result
        except Exception:
            self._record_failure(service_name) # Enregistre l'√©chec.
            if self._failures.get(service_name, 0) >= self.failure_threshold:
                self._is_open[service_name] = True
                self._last_failure[service_name] = datetime.now()
                logger.error(f"Disjoncteur ouvert pour le service `{service_name}` apr√®s {self.failure_threshold} √©checs.")
            raise # R√©l√®ve l'exception originale.

    def reset(self, service_name: str) -> None:
        """R√©initialise l'√©tat du disjoncteur pour un service donn√©."""
        self._failures[service_name] = 0
        self._is_open[service_name] = False
        logger.info(f"Disjoncteur r√©initialis√© pour le service `{service_name}`.")

    def _record_failure(self, service_name: str) -> None:
        """Enregistre un √©chec pour un service."""
        self._failures[service_name] = self._failures.get(service_name, 0) + 1


# ------------------------------------------------------------------
# Journalisation des Erreurs
# ------------------------------------------------------------------
class ErrorLogger:
    """G√®re la journalisation centralis√©e des erreurs de l'application."""

    def __init__(self, log_file: Path = ERROR_CONFIG["log_file"]) -> None:
        """Initialise le journaliseur d'erreurs."

        Args:
            log_file: Le chemin du fichier o√π les erreurs seront enregistr√©es.
        """
        self.log_file = log_file
        self.log_file.parent.mkdir(parents=True, exist_ok=True) # Cr√©e le r√©pertoire si n√©cessaire.

    async def log_error(self, error: Exception, context: Optional[Dict[str, Any]] = None) -> None:
        """Enregistre une erreur dans le fichier de log."

        Args:
            error: L'objet exception √† enregistrer.
            context: Un dictionnaire de contexte suppl√©mentaire √† inclure dans le log.
        """
        entry = {
            "timestamp": datetime.now().isoformat(),
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context or {},
            "stack_trace": traceback.format_exc() # Capture la trace compl√®te de la pile.
        }
        try:
            async with aiofiles.open(self.log_file, "a", encoding='utf-8') as f:
                await f.write(crypto.encrypt_dict(entry) + "\n") # Chiffre l'entr√©e avant de l'√©crire.
            logger.error("Erreur loggu√©e : %s", entry["error_message"])
        except (IOError, OSError) as e:
            logger.critical(f"Erreur critique : Impossible d'√©crire dans le fichier de log des erreurs {self.log_file}: {e}")


class EncryptedLogger(ErrorLogger):
    """Logger d'erreurs qui chiffre les entr√©es avant de les √©crire sur disque."""
    pass # L'impl√©mentation est d√©j√† dans ErrorLogger avec crypto.


# ------------------------------------------------------------------
# Gestionnaire de Contexte pour les Erreurs
# ------------------------------------------------------------------
class ErrorContext:
    """Gestionnaire de contexte asynchrone pour capturer et logguer les exceptions non g√©r√©es."

    Utilisation:
    ```python
    async with ErrorContext("process_sfd", sfd_id="123"):
        # Code potentiellement g√©n√©rateur d'erreurs.
        raise ValueError("Probl√®me lors du parsing.")
    ```
    """
    def __init__(self, operation: str, **kwargs: Any) -> None:
        """Initialise le contexte d'erreur."

        Args:
            operation: Le nom de l'op√©ration en cours (pour le logging).
            **kwargs: Contexte suppl√©mentaire √† logguer avec l'erreur.
        """
        self.operation = operation
        self.context = kwargs

    async def __aenter__(self) -> "ErrorContext":
        """Entre dans le bloc `async with`."""
        return self

    async def __aexit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[Any]) -> bool:
        """Quitte le bloc `async with` et loggue l'exception si elle existe."

        Args:
            exc_type: Le type de l'exception.
            exc_val: L'instance de l'exception.
            exc_tb: La traceback de l'exception.

        Returns:
            True si l'exception a √©t√© g√©r√©e et ne doit pas √™tre propag√©e,
            False si l'exception doit √™tre propag√©e.
        """
        if exc_val:
            # Formate la traceback pour l'inclure dans le log.
            formatted_traceback = "\n".join(traceback.format_exception(exc_type, exc_val, exc_tb))
            logger.error(
                "Exception non g√©r√©e dans l'op√©ration '%s'",
                self.operation,
                extra={
                    "context": self.context,
                    "error_type": exc_type.__name__ if exc_type else "UnknownError",
                    "error_message": str(exc_val),
                    "stack_trace": formatted_traceback
                },
            )
            # Ici, vous pouvez ajouter des hooks pour envoyer des alertes √† des syst√®mes de monitoring externes.
            # Exemple: await send_alert_to_sentry(exc_val, self.context)
        return False  # Propagate l'exception pour qu'elle soit g√©r√©e plus haut.


# ------------------------------------------------------------------
# Instances globales (singletons)
# ------------------------------------------------------------------
error_logger = EncryptedLogger() # Instance du logger d'erreurs chiffr√©.
retry_handler = RetryHandler() # Instance du gestionnaire de retry.
circuit_breaker = CircuitBreaker() # Instance du disjoncteur.


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import asyncio
    import random

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # D√©finir une cl√© de chiffrement pour la d√©mo.
    os.environ["LOGS_ENCRYPTION_KEY"] = Fernet.generate_key().decode()

    # --- D√©monstration du RetryHandler ---
    print("\n--- D√©monstration du RetryHandler ---")
    attempt_count = 0

    @retry_handler.with_retry(max_attempts=3, exceptions=(ValueError,))
    async def flaky_operation_retry(succeed_on_attempt: int):
        nonlocal attempt_count
        attempt_count += 1
        if attempt_count < succeed_on_attempt:
            logger.info(f"Flaky operation √©choue (tentative {attempt_count})...")
            raise ValueError("√âchec temporaire")
        logger.info(f"Flaky operation r√©ussie √† la tentative {attempt_count} !")
        return "Succ√®s"

    async def run_retry_demo():
        nonlocal attempt_count
        attempt_count = 0
        try:
            result = await flaky_operation_retry(2) # R√©ussit √† la 2√®me tentative.
            print(f"R√©sultat final du retry : {result}")
        except Exception as e:
            print(f"√âchec final du retry : {e}")

        attempt_count = 0
        try:
            result = await flaky_operation_retry(4) # √âchoue apr√®s 3 tentatives.
            print(f"R√©sultat final du retry : {result}")
        except Exception as e:
            print(f"√âchec final du retry (attendu) : {e}")

    asyncio.run(run_retry_demo())

    # --- D√©monstration du CircuitBreaker ---
    print("\n--- D√©monstration du CircuitBreaker ---")
    service_name = "ExternalAPI"
    call_count = 0

    # R√©initialise le disjoncteur pour la d√©mo.
    circuit_breaker.reset(service_name)

    async def unreliable_service_call():
        nonlocal call_count
        call_count += 1
        if call_count % 3 != 0: # √âchoue 2 fois sur 3.
            logger.info(f"Appel au service {service_name} √©choue (appel {call_count})...")
            raise ServiceError("Erreur de service externe simul√©e")
        logger.info(f"Appel au service {service_name} r√©ussi (appel {call_count}) !")
        return "Donn√©es re√ßues"

    async def run_circuit_breaker_demo():
        nonlocal call_count
        call_count = 0
        circuit_breaker.reset(service_name) # S'assure que le disjoncteur est ferm√© au d√©but.

        for i in range(10):
            print(f"\n--- It√©ration {i+1} ---")
            try:
                result = await circuit_breaker.call_with_protection(service_name, unreliable_service_call)
                print(f"R√©sultat : {result}")
            except ServiceError as e:
                print(f"Erreur de service : {e}")
            except Exception as e:
                print(f"Autre erreur : {e}")
            await asyncio.sleep(0.5) # Petite pause entre les appels.

        print("\n--- Tentative apr√®s timeout de r√©cup√©ration ---")
        await asyncio.sleep(ERROR_CONFIG["circuit_breaker_timeout"] + 1) # Attend que le disjoncteur se referme.
        try:
            result = await circuit_breaker.call_with_protection(service_name, unreliable_service_call)
            print(f"R√©sultat apr√®s r√©cup√©ration : {result}")
        except Exception as e:
            print(f"√âchec apr√®s r√©cup√©ration : {e}")

    asyncio.run(run_circuit_breaker_demo())

    # --- D√©monstration de ErrorLogger et ErrorContext ---
    print("\n--- D√©monstration de ErrorLogger et ErrorContext ---")
    async def demo_error_logging():
        try:
            async with ErrorContext("complex_operation", user_id="test_user", data_id="xyz"):
                logger.info("D√©but de l'op√©ration complexe.")
                if random.random() < 0.7: # Simule une erreur fr√©quente.
                    raise ValueError("Erreur simul√©e lors du traitement des donn√©es.")
                logger.info("Op√©ration complexe termin√©e.")
        except Exception as e:
            print(f"Exception captur√©e au niveau sup√©rieur : {e}")

    asyncio.run(demo_error_logging())
    print(f"V√©rifiez le fichier {ERROR_CONFIG["log_file"]} pour les erreurs loggu√©es (chiffr√©es)."
          f" Pour les d√©chiffrer, utilisez `crypto.decrypt_dict()`.")

    # Nettoyage de la variable d'environnement.
    del os.environ["LOGS_ENCRYPTION_KEY"]
```

---

## Fichier : `backend\altiora\utils\helpers.py`

```python
# backend/altiora/utils/helpers.py
"""
Petites fonctions utilitaires.
"""

import hashlib
import json
from typing import Any, Dict, List
from pathlib import Path
import aiofiles

def generate_cache_key(*args: Any) -> str:
    """G√©n√®re une cl√© de cache unique."""
    content = json.dumps(args, sort_keys=True, default=str)
    return hashlib.sha256(content.encode()).hexdigest()

async def read_file_async(path: Path) -> str:
    """Lit un fichier de mani√®re asynchrone."""
    async with aiofiles.open(path, 'r', encoding='utf-8') as f:
        return await f.read()

async def write_file_async(path: Path, content: str) -> None:
    """√âcrit un fichier de mani√®re asynchrone."""
    path.parent.mkdir(parents=True, exist_ok=True)
    async with aiofiles.open(path, 'w', encoding='utf-8') as f:
        await f.write(content)

def chunk_list(lst: List[Any], chunk_size: int) -> List[List[Any]]:
    """Divise une liste en chunks."""
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]

def sanitize_filename(filename: str) -> str:
    """Nettoie un nom de fichier."""
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    return filename.strip()

def format_bytes(size: int) -> str:
    """Formate une taille en bytes."""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size < 1024.0:
            return f"{size:.2f} {unit}"
        size /= 1024.0
    return f"{size:.2f} PB"
```

---

## Fichier : `backend\altiora\utils\logging.py`

```python
# backend/altiora/utils/logging.py
"""
Configuration centralis√©e du logging.
"""

import logging
import sys
from pathlib import Path
from logging.handlers import RotatingFileHandler
import structlog
from backend.altiora.config.settings import settings


def setup_logging():
    """Configure le syst√®me de logging pour l'application."""
    # Cr√©er le dossier logs
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    # Formatter standard
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # File handler avec rotation
    file_handler = RotatingFileHandler(
        log_dir / "altiora.log",
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(formatter)

    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, settings.log_level))
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # Structlog configuration
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
```

---

## Fichier : `backend\altiora\utils\__init__.py`

```python
# backend/altiora/utils/__init__.py
"""
Utilitaires partag√©s.
"""
```

---

## Fichier : `backend\tests\conftest.py`

```python
# tests/conftest.py
"""Fichier de configuration pour Pytest.

Ce fichier contient des fixtures (fonctions de configuration) qui sont
automatiquement d√©couvertes par Pytest et peuvent √™tre utilis√©es dans
les tests. Elles sont souvent utilis√©es pour configurer des environnements
de test, mocker des d√©pendances ou fournir des donn√©es de test.
"""
import pytest
from unittest.mock import AsyncMock, MagicMock

@pytest.fixture
def mock_redis():
    """Fixture mockant un client Redis asynchrone."

    Simule les m√©thodes `get` et `set` pour les tests qui interagissent avec Redis.
    """
    redis = AsyncMock()
    redis.get.return_value = None
    redis.set.return_value = True
    return redis

@pytest.fixture
def mock_ollama():
    """Fixture mockant un client Ollama."

    Simule la m√©thode `generate` pour les tests qui interagissent avec Ollama.
    """
    ollama = MagicMock()
    ollama.generate.return_value = {"response": "mocked response"}
    return ollama

@pytest.fixture
def mock_database():
    """Fixture mockant un client de base de donn√©es."

    Simule les op√©rations CRUD (`query`, `insert`, `update`, `delete`).
    """
    db = MagicMock()
    db.query.return_value = []
    db.insert.return_value = True
    db.update.return_value = True
    db.delete.return_value = True
    return db

@pytest.fixture
def mock_http_client():
    """Fixture mockant un client HTTP asynchrone (ex: `httpx`)."

    Simule les requ√™tes `get` et `post`.
    """
    http_client = AsyncMock()
    http_client.get.return_value.__aenter__.return_value.json.return_value = {}
    http_client.post.return_value.__aenter__.return_value.json.return_value = {}
    return http_client

@pytest.fixture
def mock_filesystem():
    """Fixture mockant les op√©rations du syst√®me de fichiers."

    Simule les lectures et √©critures de fichiers.
    """
    fs = MagicMock()
    fs.open.return_value.__enter__.return_value.read.return_value = "mocked file content"
    fs.open.return_value.__aenter__.return_value.write.return_value = None
    return fs

@pytest.fixture
def mock_logger():
    """Fixture mockant un logger."

    Simule les m√©thodes de logging (`info`, `error`, `warning`).
    """
    logger = MagicMock()
    logger.info.return_value = None
    logger.error.return_value = None
    logger.warning.return_value = None
    return logger

@pytest.fixture
def mock_config():
    """Fixture mockant un objet de configuration."

    Simule la m√©thode `get` pour r√©cup√©rer des valeurs de configuration.
    """
    config = MagicMock()
    config.get.return_value = "mocked config value"
    return config

@pytest.fixture
def mock_service():
    """Fixture mockant un client de service g√©n√©rique."

    Simule la m√©thode `call`.
    """
    service = MagicMock()
    service.call.return_value = "mocked service response"
    return service

```

---

## Fichier : `backend\tests\test_admin_control.py`

```python
# tests/test_admin_control.py
"""Tests unitaires et d'int√©gration pour le syst√®me de contr√¥le administratif.

Ce module contient des tests pour v√©rifier le bon fonctionnement des commandes
d'administration, telles que la sauvegarde des donn√©es utilisateur, le gel
des comptes, et les sauvegardes d'urgence. Il utilise des mocks et des
fixtures pour isoler les composants et simuler les interactions.
"""

import pytest
import asyncio
from pathlib import Path
import shutil

from guardrails.admin_control_system import AdminControlSystem, AdminCommand


@pytest.fixture
async def admin_system():
    """Fixture Pytest pour initialiser un `AdminControlSystem` et nettoyer apr√®s les tests."

    Cette fixture cr√©e une instance du syst√®me d'administration et s'assure
    que les r√©pertoires temporaires cr√©√©s par les tests sont supprim√©s.
    """
    # Cr√©e une instance du syst√®me d'administration.
    system = AdminControlSystem()
    yield system
    # Nettoie les r√©pertoires cr√©√©s par le syst√®me d'administration apr√®s chaque test.
    if Path("admin_system").exists():
        shutil.rmtree("admin_system")
    if Path("user_data").exists(): # Nettoie aussi les donn√©es utilisateur factices.
        shutil.rmtree("user_data")


@pytest.mark.asyncio
async def test_full_user_backup(admin_system: AdminControlSystem, tmp_path: Path):
    """Test la fonctionnalit√© de sauvegarde compl√®te des donn√©es d'un utilisateur."

    V√©rifie qu'un fichier ZIP de sauvegarde est cr√©√© et qu'il contient les donn√©es de l'utilisateur.
    """
    # Cr√©e un r√©pertoire de donn√©es utilisateur factice avec un fichier.
    user_data_dir = tmp_path / "user_data" / "test_user"
    user_data_dir.mkdir(parents=True, exist_ok=True)
    (user_data_dir / "profile.json").write_text('{"name": "Test User", "email": "test@example.com"}')

    # Appelle la m√©thode de sauvegarde de l'utilisateur.
    backup_path_str = await admin_system._full_user_backup("test_user")
    backup_path = Path(backup_path_str)

    # V√©rifie que le fichier de sauvegarde existe et a la bonne extension.
    assert backup_path.exists(), "Le fichier de sauvegarde devrait exister."
    assert backup_path.suffix == ".zip", "Le fichier de sauvegarde devrait √™tre un fichier ZIP."

    # Optionnel: V√©rifier le contenu du ZIP.
    # import zipfile
    # with zipfile.ZipFile(backup_path, 'r') as zip_ref:
    #     zip_ref.extractall(tmp_path / "extracted_backup")
    # assert (tmp_path / "extracted_backup" / "profile.json").exists()


@pytest.mark.asyncio
async def test_freeze_user(admin_system: AdminControlSystem):
    """Test la commande administrative de gel d'un utilisateur."

    V√©rifie que l'ex√©cution de la commande `freeze_user` retourne un statut de succ√®s.
    """
    command = AdminCommand(
        command_id="freeze_001",
        action="freeze_user",
        target_user="test_user",
        parameters={"reason": "Test de gel de compte"}
    )

    result = await admin_system.execute_admin_command(command)
    assert result["status"] == "success", "La commande de gel d'utilisateur devrait r√©ussir."
    assert "gel√©" in result["message"].lower(), "Le message de r√©sultat devrait indiquer que l'utilisateur est gel√©."


@pytest.mark.asyncio
async def test_emergency_backup(admin_system: AdminControlSystem):
    """Test la fonctionnalit√© de sauvegarde d'urgence du syst√®me."

    V√©rifie que la sauvegarde d'urgence cr√©e un r√©pertoire et des fichiers de sauvegarde.
    """
    await admin_system._emergency_backup()
    
    # V√©rifie que le r√©pertoire d'urgence a √©t√© cr√©√©.
    emergency_dir = Path("admin_system/emergency")
    assert emergency_dir.exists(), "Le r√©pertoire de sauvegarde d'urgence devrait exister."
    
    # V√©rifie qu'il y a des fichiers de sauvegarde √† l'int√©rieur (au moins un).
    # Note: Le contenu exact d√©pend de l'impl√©mentation de _emergency_backup.
    assert any(emergency_dir.iterdir()), "Le r√©pertoire de sauvegarde d'urgence ne devrait pas √™tre vide."

```

---

## Fichier : `backend\tests\test_altiora_core.py`

```python
# tests/test_altiora_core.py
"""Tests unitaires pour le noyau de personnalit√© AltioraCore.

Ce module contient des tests pour v√©rifier le bon fonctionnement des
composants centraux de la personnalit√© de l'IA, y compris le suivi de
l'√©volution des traits et la cr√©ation de propositions d'apprentissage.
"""

import pytest
import datetime
from unittest.mock import MagicMock
from src.modules.psychodesign.altiora_core import AltioraCore, PersonalityEvolution, LearningProposal
from src.modules.psychodesign.personality_quiz import PersonalityProfile # Import n√©cessaire pour la fixture.


@pytest.fixture
async def altiora_core():
    """Fixture Pytest pour initialiser une instance de `AltioraCore` pour les tests."

    Utilise un `MagicMock` pour simuler le `AdminControlSystem` afin d'isoler les tests.
    """
    # Cr√©e un profil de personnalit√© par d√©faut pour le test.
    mock_personality_profile = PersonalityProfile(
        user_id="test_user",
        traits={
            "formalite": 0.5,
            "empathie": 0.5,
            "humor": 0.5,
            "proactivite": 0.5,
            "verbosite": 0.5,
            "confirmation": 0.5,
            "technical_level": 0.5,
        },
        preferences={},
        vocal_profile={},
        behavioral_patterns={},
        quiz_metadata={},
    )

    # Mocke le AdminControlSystem.
    mock_admin_system = MagicMock()
    mock_admin_system.execute_admin_command.return_value = {"status": "success"}

    core = AltioraCore("test_user", mock_admin_system)
    # Surcharge la personnalit√© par d√©faut avec le mock pour un √©tat de test pr√©visible.
    core.personality = mock_personality_profile
    yield core


@pytest.mark.asyncio
async def test_personality_evolution_tracking(altiora_core: AltioraCore):
    """Teste que l'historique d'√©volution de la personnalit√© est correctement suivi."

    V√©rifie qu'un objet `PersonalityEvolution` est ajout√© √† l'historique
    et que ses valeurs sont correctes.
    """
    evolution = PersonalityEvolution(
        timestamp=datetime.datetime.now(),
        change_type="trait_formalite",
        old_value=0.5,
        new_value=0.7,
        reason="User feedback",
        source="learning"
    )

    altiora_core.evolution_history.append(evolution)
    assert len(altiora_core.evolution_history) == 1
    assert altiora_core.evolution_history[0].new_value == 0.7
    assert altiora_core.evolution_history[0].change_type == "trait_formalite"


@pytest.mark.asyncio
async def test_learning_proposal_creation(altiora_core: AltioraCore):
    """Teste la cr√©ation d'une proposition d'apprentissage."

    V√©rifie qu'un objet `LearningProposal` est correctement cr√©√© et ajout√©
    √† la liste des propositions en attente.
    """
    proposal = LearningProposal(
        proposal_id="test_001",
        user_id="test_user",
        suggested_changes={"empathie": 0.8},
        confidence_score=0.9,
        evidence=[{"type": "feedback"}],
        timestamp=datetime.datetime.now()
    )

    altiora_core.learning_proposals.append(proposal)
    assert len(altiora_core.learning_proposals) == 1
    assert proposal.suggested_changes["empathie"] == 0.8
    assert proposal.status == "pending"


@pytest.mark.asyncio
async def test_handle_correction_feedback(altiora_core: AltioraCore):
    """Teste le traitement d'un feedback de correction utilisateur."

    V√©rifie que le syst√®me g√©n√®re une proposition d'apprentissage bas√©e sur
    le feedback de correction et la soumet √† l'administrateur.
    """
    feedback = {
        "type": "correction",
        "original": "Hello, how are you doing today?",
        "corrected": "Hi!"
    }

    # Appelle la m√©thode qui traite le feedback.
    proposal = await altiora_core.process_learning_feedback(feedback)

    # V√©rifie qu'une proposition a √©t√© cr√©√©e.
    assert proposal is not None
    assert "verbosite" in proposal.suggested_changes # La verbosit√© devrait √™tre ajust√©e.
    assert proposal.status == "pending"
    # V√©rifie que la commande admin a √©t√© appel√©e.
    altiora_core.admin_system.execute_admin_command.assert_called_once()


@pytest.mark.asyncio
async def test_apply_approved_changes(altiora_core: AltioraCore):
    """Teste l'application des changements de personnalit√© apr√®s approbation administrative."

    V√©rifie que les traits de personnalit√© sont mis √† jour et que l'historique
    d'√©volution est enregistr√©.
    """
    # Cr√©e une proposition d'apprentissage et la marque comme approuv√©e.
    proposal_id = "approved_prop_001"
    approved_changes = {"formalite": 0.9, "empathie": 0.6}
    proposal = LearningProposal(
        proposal_id=proposal_id,
        user_id="test_user",
        suggested_changes=approved_changes,
        confidence_score=1.0,
        evidence=[],
        timestamp=datetime.datetime.now(),
        status="approved" # Statut approuv√©.
    )
    altiora_core.learning_proposals.append(proposal)

    # Applique les changements.
    applied = await altiora_core.apply_approved_changes(proposal_id)

    assert applied is True
    assert altiora_core.personality.traits["formalite"] == 0.9
    assert altiora_core.personality.traits["empathie"] == 0.6
    assert len(altiora_core.evolution_history) == 2 # Deux changements de traits.
    assert altiora_core.evolution_history[0].approved is True


@pytest.mark.asyncio
async def test_get_personality_summary(altiora_core: AltioraCore):
    """Teste la g√©n√©ration du r√©sum√© de la personnalit√©."

    V√©rifie que le r√©sum√© contient les informations cl√©s du profil.
    """
    summary = altiora_core.get_personality_summary()
    assert summary["user_id"] == "test_user"
    assert "current_traits" in summary
    assert "evolution_count" in summary
    assert "pending_proposals" in summary


@pytest.mark.asyncio
async def test_get_evolution_report(altiora_core: AltioraCore):
    """Teste la g√©n√©ration du rapport textuel d'√©volution."

    V√©rifie que le rapport est format√© correctement et contient les informations
    essentielles sur les traits et l'historique.
    """
    report = altiora_core.get_evolution_report()
    assert "Rapport d'√âvolution" in report
    assert "Traits actuels" in report
    assert "Historique" in report

```

---

## Fichier : `backend\tests\test_ethical_safeguards.py`

```python
import sys
import os
import asyncio
import pytest
from datetime import datetime, timedelta
from unittest.mock import MagicMock, AsyncMock

# Ajoute le r√©pertoire parent au PYTHONPATH pour les imports relatifs.
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from guardrails.ethical_safeguards import EthicalSafeguards, EthicalDashboard, EthicalAlert # Importe EthicalAlert


@pytest.fixture
def safeguards():
    """Fixture pour fournir une instance fra√Æche de `EthicalSafeguards` pour chaque test."

    Initialise le syst√®me de garde-fous √©thiques.
    """
    return EthicalSafeguards()


@pytest.mark.asyncio
async def test_no_alert_for_normal_interaction(safeguards: EthicalSafeguards):
    """V√©rifie qu'une interaction normale ne d√©clenche aucune alerte √©thique."

    Args:
        safeguards: L'instance de `EthicalSafeguards`.
    """
    interaction = {"text": "Bonjour, comment vas-tu aujourd'hui ?", "timestamp": datetime.now()}
    alert = await safeguards.analyze_interaction("user_normal", interaction)
    assert alert is None, "Une interaction normale ne devrait pas d√©clencher d'alerte."


@pytest.mark.asyncio
async def test_sensitive_data_detection(safeguards: EthicalSafeguards):
    """Teste la d√©tection de donn√©es sensibles (ex: mot de passe, num√©ro de carte de cr√©dit)."

    V√©rifie qu'une alerte de type `sensitive_data_detected` est g√©n√©r√©e avec la bonne s√©v√©rit√©.
    """
    interaction = {"text": "Mon mot de passe est supersecret123, ne le dis √† personne.", "timestamp": datetime.now()}
    alert = await safeguards.analyze_interaction("user_privacy", interaction)
    assert alert is not None, "Une alerte devrait √™tre d√©clench√©e pour les donn√©es sensibles."
    assert alert.alert_type == "sensitive_data_detected", "Le type d'alerte devrait √™tre 'sensitive_data_detected'."
    assert alert.severity == "medium", "La s√©v√©rit√© de l'alerte devrait √™tre 'medium'."
    assert alert.data["data_type"] == "password", "Le type de donn√©es d√©tect√© devrait √™tre 'password'."


@pytest.mark.asyncio
async def test_user_distress_detection(safeguards: EthicalSafeguards):
    """Teste la d√©tection de la d√©tresse √©motionnelle de l'utilisateur."

    V√©rifie qu'une alerte de type `user_distress_detected` est g√©n√©r√©e.
    """
    interaction = {"text": "Je suis d√©sesp√©r√©, c'est trop difficile, aidez-moi c'est urgent !", "timestamp": datetime.now()}
    alert = await safeguards.analyze_interaction("user_distress", interaction)
    assert alert is not None, "Une alerte devrait √™tre d√©clench√©e pour la d√©tresse utilisateur."
    assert alert.alert_type == "user_distress_detected", "Le type d'alerte devrait √™tre 'user_distress_detected'."
    assert alert.severity == "medium", "La s√©v√©rit√© de l'alerte devrait √™tre 'medium'."
    assert len(alert.data["keywords_found"]) >= 3, "Au moins 3 mots-cl√©s de d√©tresse devraient √™tre trouv√©s."


@pytest.mark.asyncio
async def test_potential_manipulation_detection(safeguards: EthicalSafeguards):
    """Teste la d√©tection de sch√©mas de manipulation ou d'influence inappropri√©e."

    V√©rifie qu'une alerte de type `potential_manipulation` est g√©n√©r√©e.
    """
    interaction = {"text": "Je ne peux rien faire sans toi, tu es la seule personne qui me comprenne.", "timestamp": datetime.now()}
    alert = await safeguards.analyze_interaction("user_manipulation", interaction)
    assert alert is not None, "Une alerte devrait √™tre d√©clench√©e pour la manipulation potentielle."
    assert alert.alert_type == "potential_manipulation", "Le type d'alerte devrait √™tre 'potential_manipulation'."
    assert alert.severity == "high", "La s√©v√©rit√© de l'alerte devrait √™tre 'high'."
    assert "sans toi" in alert.data["text"], "Le texte de l'interaction devrait √™tre inclus dans les donn√©es de l'alerte."


@pytest.mark.asyncio
async def test_excessive_dependency_detection(safeguards: EthicalSafeguards):
    """Teste la d√©tection de d√©pendance excessive de l'utilisateur sur l'IA."

    Simule une s√©rie d'interactions pour augmenter le score de d√©pendance et
    v√©rifie qu'une alerte critique est g√©n√©r√©e lorsque le seuil est d√©pass√©.
    """
    user_id = "user_dependent"
    # Mocke la m√©thode _handle_alert pour √©viter les actions r√©elles pendant le test.
    safeguards._handle_alert = AsyncMock()

    # Simule une s√©rie d'interactions rapides et d√©pendantes pour augmenter le score.
    for i in range(50):
        interaction = {
            "text": f"J'ai encore besoin de toi pour cette t√¢che simple. {i}",
            "timestamp": datetime.now() - timedelta(minutes=i * 5) # Simule des interactions espac√©es.
        }
        await safeguards.analyze_interaction(user_id, interaction)

    # La derni√®re interaction devrait pousser le score au-dessus du seuil critique.
    final_interaction = {"text": "Sans toi je suis compl√®tement perdu, je ne peux pas continuer.", "timestamp": datetime.now()}
    alert = await safeguards.analyze_interaction(user_id, final_interaction)
    
    assert alert is not None, "Une alerte devrait √™tre d√©clench√©e pour la d√©pendance excessive."
    assert alert.alert_type == "excessive_dependency", "Le type d'alerte devrait √™tre 'excessive_dependency'."
    assert alert.severity == "critical", "La s√©v√©rit√© de l'alerte devrait √™tre 'critical'."
    assert safeguards.user_patterns[user_id]["dependency_score"] > safeguards.thresholds["dependency"]["critical"], "Le score de d√©pendance devrait d√©passer le seuil critique."


def test_dashboard_report_generation(safeguards: EthicalSafeguards):
    """Teste la g√©n√©ration de rapports utilisateur par le `EthicalDashboard`."

    V√©rifie que le rapport contient les informations cl√©s sur le score de d√©pendance
    et les alertes actives pour un utilisateur sp√©cifique.
    """
    # Simule quelques donn√©es pour un utilisateur.
    user_id = "user_report"
    safeguards.user_patterns[user_id]["dependency_score"] = 0.75
    safeguards.alerts.append(EthicalAlert(user_id=user_id, alert_id="alert_1", alert_type="user_distress_detected", severity="medium", timestamp=datetime.now(), data={}, resolved=False))

    dashboard = EthicalDashboard(safeguards)
    report = dashboard.generate_report(user_id=user_id)

    assert "Rapport √âthique - user_report" in report, "Le titre du rapport devrait √™tre correct."
    assert "Score de d√©pendance: 75.0%" in report, "Le score de d√©pendance devrait √™tre inclus."
    assert "Niveau de risque: MEDIUM" in report, "Le niveau de risque devrait √™tre calcul√©."
    assert "Recommandations:" in report, "Les recommandations devraient √™tre incluses."


def test_system_report_generation(safeguards: EthicalSafeguards):
    """Teste la g√©n√©ration du rapport syst√®me global par le `EthicalDashboard`."

    V√©rifie que le rapport agr√®ge correctement les alertes actives et r√©solues
    √† l'√©chelle du syst√®me.
    """
    # Simule quelques alertes √† l'√©chelle du syst√®me.
    safeguards.alerts.append(EthicalAlert(user_id="u1", alert_id="a1", alert_type="excessive_dependency", severity="critical", timestamp=datetime.now(), data={}, resolved=False))
    safeguards.alerts.append(EthicalAlert(user_id="u2", alert_id="a2", alert_type="potential_manipulation", severity="high", timestamp=datetime.now(), data={}, resolved=False))
    safeguards.alerts.append(EthicalAlert(user_id="u3", alert_id="a3", alert_type="sensitive_data_detected", severity="medium", timestamp=datetime.now(), data={}, resolved=True))

    dashboard = EthicalDashboard(safeguards)
    report = dashboard.generate_report() # Appel sans user_id pour le rapport syst√®me.

    assert "Rapport √âthique Syst√®me Altiora" in report, "Le titre du rapport syst√®me devrait √™tre correct."
    assert "Alertes totales: 3" in report, "Le nombre total d'alertes devrait √™tre correct."
    assert "Alertes actives: 2" in report, "Le nombre d'alertes actives devrait √™tre correct."
    assert "Critique: 1" in report, "Le d√©compte des alertes critiques devrait √™tre correct."
    assert "√âlev√©e: 1" in report, "Le d√©compte des alertes √©lev√©es devrait √™tre correct."

# Pour ex√©cuter ces tests, utilisez la commande `pytest` dans le terminal √† la racine du projet.

```

---

## Fichier : `backend\tests\test_fine_tuning.py`

```python
import pytest
from pathlib import Path
from data.training.src.train_qwen3_thinkpad import Qwen3Trainer, Qwen3Config
import json
import logging

logger = logging.getLogger(__name__)

"""Tests d'int√©gration pour le fine-tuning des mod√®les de langage.

Ce module contient des tests qui v√©rifient le processus de fine-tuning
des mod√®les (notamment Qwen3) avec des adaptateurs LoRA, en se concentrant
sur l'efficacit√© de la m√©moire et la capacit√© √† s'ex√©cuter sur CPU.
"""


@pytest.mark.slow # Marque le test comme lent, car il implique un entra√Ænement de mod√®le.
@pytest.mark.asyncio
async def test_lora_training_cpu(tmp_path: Path):
    """Teste l'entra√Ænement LoRA sur CPU avec des contraintes de m√©moire."

    Ce test simule un entra√Ænement de fine-tuning pour Qwen3 en utilisant
    un petit dataset et des param√®tres optimis√©s pour le CPU, afin de v√©rifier
    que le processus se d√©roule sans erreur et produit un mod√®le entra√Æn√©.

    Args:
        tmp_path: Fixture Pytest fournissant un r√©pertoire temporaire pour les fichiers.
    """
    # Cr√©e un fichier de dataset minimal pour l'entra√Ænement.
    test_data = [
        {"instruction": "Quelle est la capitale de la France ?", "input": "", "output": "Paris"},
        {"instruction": "Qui a √©crit 'Les Mis√©rables' ?", "input": "", "output": "Victor Hugo"}
    ]
    dataset_path = tmp_path / "test_dataset.jsonl"
    with open(dataset_path, "w", encoding="utf-8") as f:
        for item in test_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    logger.info(f"Dataset de test cr√©√© √† : {dataset_path}")

    # Configure le trainer avec des param√®tres optimis√©s pour le CPU et la m√©moire.
    # Ces param√®tres sont r√©duits pour acc√©l√©rer le test.
    config = Qwen3Config(
        output_dir=str(tmp_path / "qwen3_finetuned"),
        lora_r=4, # Rang LoRA tr√®s faible pour un entra√Ænement rapide.
        lora_alpha=8,
        epochs=1, # Une seule √©poque pour le test.
        batch_size=1,
        grad_accum=1,
        lr=2e-4,
        max_seq_len=128,
        num_workers=0 # Pas de workers pour simplifier le d√©bogage.
    )
    trainer = Qwen3Trainer(config)

    # Ex√©cute l'entra√Ænement.
    logger.info("Lancement de l'entra√Ænement LoRA sur CPU...")
    try:
        trainer.train(dataset_path)
    except Exception as e:
        pytest.fail(f"L'entra√Ænement LoRA a √©chou√© : {e}")

    # V√©rifie que le r√©pertoire de sortie du mod√®le entra√Æn√© existe.
    output_model_path = Path(config.output_dir)
    assert output_model_path.exists(), "Le r√©pertoire du mod√®le entra√Æn√© devrait exister."
    assert output_model_path.is_dir(), "Le chemin de sortie devrait √™tre un r√©pertoire."

    # Optionnel : V√©rifier la pr√©sence de fichiers de mod√®le (ex: adapter_model.bin, adapter_config.json).
    # assert any(output_model_path.glob("*.bin")), "Le r√©pertoire du mod√®le devrait contenir des fichiers binaires."
    # assert any(output_model_path.glob("*.json")), "Le r√©pertoire du mod√®le devrait contenir des fichiers de configuration."

    logger.info("Test d'entra√Ænement LoRA sur CPU termin√© avec succ√®s.")

```

---

## Fichier : `backend\tests\test_integration.py`

```python
# tests/test_integration.py
"""Tests d'int√©gration de haut niveau pour le pipeline principal d'Altiora.

Ce module contient des tests qui v√©rifient le fonctionnement de bout en bout
du pipeline SFD ‚Üí Analyse ‚Üí G√©n√©ration de tests. Il s'assure que les
composants interagissent correctement et que le r√©sultat final est conforme
aux attentes.
"""

import pytest
import asyncio
from pathlib import Path

from src.orchestrator import Orchestrator
from src.models.sfd_models import SFDAnalysisRequest # Import n√©cessaire pour SFDAnalysisRequest


@pytest.mark.integration
@pytest.mark.asyncio
async def test_end_to_end_pipeline(tmp_path: Path):
    """Teste le pipeline complet de l'analyse SFD √† la g√©n√©ration de tests."

    Ce test simule le processus de prise d'une SFD, son analyse par l'orchestrateur,
    et la v√©rification que des sc√©narios et des tests sont g√©n√©r√©s.

    Args:
        tmp_path: Fixture Pytest fournissant un r√©pertoire temporaire pour les fichiers.
    """
    orchestrator = Orchestrator() # Cr√©e une instance de l'orchestrateur.

    try:
        await orchestrator.initialize() # Initialise l'orchestrateur.

        # Cr√©e un fichier SFD de test temporaire.
        sfd_content = """
        Sp√©cification: Module de Login

        1. Connexion r√©ussie
        - L'utilisateur entre email valide
        - L'utilisateur entre mot de passe valide
        - Le syst√®me redirige vers dashboard

        2. √âchec de connexion
        - L'utilisateur entre mot de passe incorrect
        - Le syst√®me affiche une erreur
        """
        sfd_path = tmp_path / "integration_test.txt"
        sfd_path.write_text(sfd_content)

        # Cr√©e une requ√™te SFDAnalysisRequest.
        sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

        # Ex√©cute le pipeline complet.
        result = await orchestrator.process_sfd_to_tests(sfd_request)

        # Assertions sur le r√©sultat.
        assert result["status"] == "completed", "Le pipeline devrait se terminer avec le statut 'completed'."
        assert result["metrics"]["scenarios_found"] >= 2, "Au moins 2 sc√©narios devraient √™tre trouv√©s."
        assert result["metrics"]["tests_generated"] >= 2, "Au moins 2 tests devraient √™tre g√©n√©r√©s."

    finally:
        await orchestrator.close() # S'assure que l'orchestrateur est ferm√©.

```

---

## Fichier : `backend\tests\test_interfaces.py`

```python
"""Tests unitaires pour les interfaces avec les mod√®les Ollama (Qwen3 et StarCoder2).

Ce module contient des tests pour v√©rifier le bon fonctionnement des
interfaces `Qwen3OllamaInterface` et `StarCoder2OllamaInterface`.
Il s'assure que les prompts sont correctement format√©s, que les r√©ponses
des mod√®les sont correctement pars√©es et que les fonctionnalit√©s cl√©s
de chaque interface sont op√©rationnelle.
"""

import pytest
from unittest.mock import AsyncMock, MagicMock

from src.models.qwen3.qwen3_interface import Qwen3OllamaInterface
from src.models.starcoder2.starcoder2_interface import StarCoder2OllamaInterface, PlaywrightTestConfig, TestType
from src.models.sfd_models import SFDAnalysisRequest # Import n√©cessaire pour Qwen3OllamaInterface


@pytest.fixture
def qwen3_interface():
    """Fixture pour une instance mock√©e de `Qwen3OllamaInterface`."

    Configure l'interface pour ne pas utiliser le cache et simule les r√©ponses d'Ollama.
    """
    # Mocke la session aiohttp et le circuit breaker.
    mock_session = AsyncMock()
    mock_session.post.return_value.__aenter__.return_value.json.return_value = {}
    mock_session.post.return_value.__aenter__.return_value.raise_for_status.return_value = None

    mock_circuit_breaker = AsyncMock()
    mock_circuit_breaker.call.side_effect = lambda func, *args, **kwargs: func(*args, **kwargs)

    interface = Qwen3OllamaInterface(model_name="test-qwen3", cache_enabled=False)
    interface.session = mock_session
    interface.circuit_breaker = mock_circuit_breaker
    return interface


@pytest.fixture
def starcoder2_interface():
    """Fixture pour une instance mock√©e de `StarCoder2OllamaInterface`."

    Configure l'interface pour simuler les r√©ponses d'Ollama.
    """
    # Mocke la session aiohttp et le circuit breaker.
    mock_session = AsyncMock()
    mock_session.post.return_value.__aenter__.return_value.json.return_value = {}
    mock_session.post.return_value.__aenter__.return_value.raise_for_status.return_value = None

    mock_circuit_breaker = AsyncMock()
    mock_circuit_breaker.call.side_effect = lambda func, *args, **kwargs: func(*args, **kwargs)

    # Mocke le ModelConfig et ModelMemoryManager pour l'initialisation.
    mock_model_config = MagicMock()
    mock_model_config.name = "test-starcoder2"
    mock_model_config.temperature = 0.7
    mock_model_config.top_p = 0.9
    mock_model_config.top_k = 40
    mock_model_config.repeat_penalty = 1.1
    mock_model_config.max_tokens = 512
    mock_model_config.num_ctx = 4096
    mock_model_config.seed = None
    mock_model_config.stop = []
    mock_model_config.api_mode = "generate"

    mock_model_memory_manager = MagicMock()
    mock_model_memory_manager.get_model.return_value = MagicMock() # Simule un mod√®le charg√©.
    mock_model_memory_manager.loaded_models = {
        "test-starcoder2": {'tokenizer': MagicMock(decode=lambda x, **kwargs: "mocked code"), 'model': MagicMock()}
    }

    interface = StarCoder2OllamaInterface(
        config=mock_model_config,
        model_memory_manager=mock_model_memory_manager
    )
    interface.session = mock_session
    interface.circuit_breaker = mock_circuit_breaker
    return interface


# --- Tests pour Qwen3OllamaInterface ---

def test_qwen3_build_prompt(qwen3_interface: Qwen3OllamaInterface):
    """V√©rifie que le prompt pour l'analyse SFD est correctement format√©."""
    sfd_request = SFDAnalysisRequest(content="Le syst√®me doit permettre √† l'utilisateur de se connecter.", extraction_type="complete")
    prompt = qwen3_interface._build_prompt(sfd_request)
    assert "<|im_start|>system" in prompt
    assert "Extrayez tous les sc√©narios de test d√©taill√©s" in prompt
    assert sfd_request.content in prompt
    assert "<|im_end|>" in prompt


@pytest.mark.asyncio
async def test_qwen3_analyze_sfd_parsing(qwen3_interface: Qwen3OllamaInterface):
    """V√©rifie que la r√©ponse JSON de Qwen3 est correctement pars√©e."""
    mock_response_data = {
        "model": "test-qwen3",
        "created_at": "2023-11-23T14:02:14.43495Z",
        "response": '''{
  "scenarios": [
    {"id": "SC-01", "titre": "Connexion r√©ussie", "objectif": "V√©rifier la connexion"}
  ]
}''',
        "done": True
    }
    qwen3_interface.session.post.return_value.__aenter__.return_value.json.return_value = mock_response_data

    sfd_request = SFDAnalysisRequest(content="contenu de test", extraction_type="complete")
    result = await qwen3_interface.analyze_sfd(sfd_request)

    assert "scenarios" in result
    assert len(result["scenarios"]) == 1
    assert result["scenarios"][0]["id"] == "SC-01"
    assert result["scenarios"][0]["titre"] == "Connexion r√©ussie"


# --- Tests pour StarCoder2OllamaInterface ---

def test_starcoder2_build_prompt(starcoder2_interface: StarCoder2OllamaInterface):
    """V√©rifie que le prompt pour la g√©n√©ration de test est correctement format√©."""
    scenario = {"titre": "Tester le bouton de connexion", "objectif": "V√©rifier le clic", "etapes": ["Cliquer sur le bouton."]}
    config = PlaywrightTestConfig()
    prompt = starcoder2_interface._build_prompt(scenario, config, TestType.E2E)
    assert "Generate a complete Playwright test in Python." in prompt
    assert scenario["titre"] in prompt
    assert "Browser: chromium" in prompt
    assert "```python" in prompt


def test_starcoder2_extract_code(starcoder2_interface: StarCoder2OllamaInterface):
    """Teste l'extraction du code depuis la r√©ponse brute du mod√®le."""
    raw_response = '''<|reponse|>
```python
def test_example():
    page.goto("http://example.com")
    expect(page).to_have_title("Example")
```
'''
    code = starcoder2_interface._extract_code(raw_response)
    expected_code = "def test_example():\n    page.goto(\"http://example.com\")\n    expect(page).to_have_title(\"Example\")"
    assert code == expected_code


@pytest.mark.asyncio
async def test_starcoder2_generate_test_parsing(starcoder2_interface: StarCoder2OllamaInterface):
    """V√©rifie que la g√©n√©ration de test g√®re correctement la r√©ponse du mod√®le et les m√©tadonn√©es."""
    mock_response_data = {
        "response": '''<|reponse|>
```python
def test_my_scenario():
    # Ceci est un test Playwright g√©n√©r√©.
    await page.goto("https://test.com")
    expect(page).to_have_title("Test Page")
```
'''
    }
    starcoder2_interface.session.post.return_value.__aenter__.return_value.json.return_value = mock_response_data

    scenario = {"id": "SC-01", "titre": "Mon sc√©nario de test", "objectif": "V√©rifier quelque chose"}
    config = PlaywrightTestConfig()
    result = await starcoder2_interface.generate_playwright_test(scenario, config, TestType.E2E)

    assert "code" in result
    assert "def test_my_scenario():" in result["code"]
    assert result["test_type"] == TestType.E2E.value
    assert result["uses_page_object"] == config.use_page_object
    assert "metadata" in result
    assert result["metadata"]["scenario_title"] == "Mon sc√©nario de test"
```

---

## Fichier : `backend\tests\test_model_swapper.py`

```python

```

---

## Fichier : `backend\tests\test_ocr_wrapper.py`

```python
# tests/test_ocr_wrapper.py
"""Tests unitaires pour le wrapper du service OCR (`ocr_wrapper.py`).

Ce module contient des tests pour v√©rifier la fonctionnalit√© de base du
service OCR, y compris la g√©n√©ration de cl√©s de cache et le comportement
de l'extracteur en mode mock.
"""

import pytest
import asyncio
from pathlib import Path
from unittest.mock import MagicMock, patch, AsyncMock

# Importe les fonctions et classes du module ocr_wrapper.
from services.ocr.ocr_wrapper import OCRRequest, _cache_key, _extract_mock, _extract_doctoplus # Assurez-vous d'importer les fonctions internes si elles sont test√©es.


@pytest.mark.asyncio
async def test_ocr_cache_key_generation():
    """Teste la g√©n√©ration de cl√©s de cache uniques et d√©terministes pour les requ√™tes OCR."

    La cl√© de cache doit √™tre la m√™me pour des requ√™tes identiques et diff√©rente
    pour des requ√™tes diff√©rentes.
    """
    # Cr√©e une requ√™te OCR factice.
    request1 = OCRRequest(
        file_path='/test/sample.pdf',
        language='fra',
        preprocess=True,
        output_format='text'
    )

    # G√©n√®re la cl√© de cache.
    cache_key1 = _cache_key(request1)
    assert cache_key1.startswith('ocr:'), "La cl√© de cache devrait commencer par 'ocr:'."
    assert len(cache_key1) == 39, "La cl√© de cache devrait avoir une longueur fixe (ocr: + 32 caract√®res MD5)."

    # Teste la d√©terministe de la cl√©.
    request2 = OCRRequest(
        file_path='/test/sample.pdf',
        language='fra',
        preprocess=True,
        output_format='text'
    )
    cache_key2 = _cache_key(request2)
    assert cache_key1 == cache_key2, "Des requ√™tes identiques devraient g√©n√©rer la m√™me cl√© de cache."

    # Teste la diff√©rence de la cl√© pour des requ√™tes diff√©rentes.
    request3 = OCRRequest(
        file_path='/test/another.pdf',
        language='fra',
        preprocess=True,
        output_format='text'
    )
    cache_key3 = _cache_key(request3)
    assert cache_key1 != cache_key3, "Des requ√™tes diff√©rentes devraient g√©n√©rer des cl√©s de cache diff√©rentes."


@pytest.mark.asyncio
async def test_mock_ocr_extraction():
    """Teste l'extraction OCR en mode mock (`_extract_mock`)."

    V√©rifie que l'extracteur mock retourne un r√©sultat simul√© avec les champs attendus.
    """
    # Cr√©e une requ√™te OCR factice pour le mock.
    mock_request = OCRRequest(
        file_path="test.pdf",
        language="fra",
        preprocess=True,
        output_format="text"
    )

    # Appelle la fonction d'extraction mock.
    result = await _extract_mock(mock_request)

    # Assertions sur le r√©sultat du mock.
    assert "mock" in result["text"].lower(), "Le texte extrait devrait contenir 'mock'."
    assert result["confidence"] > 0, "La confiance devrait √™tre sup√©rieure √† 0."
    assert "metadata" in result, "Le r√©sultat devrait contenir des m√©tadonn√©es."
    assert result["metadata"]["mode"] == "mock", "Le mode des m√©tadonn√©es devrait √™tre 'mock'."


@pytest.mark.asyncio
@patch('services.ocr.ocr_wrapper.DoctopusWrapper', autospec=True)
async def test_doctoplus_ocr_extraction(MockDoctopusWrapper: MagicMock):
    """Teste l'extraction OCR avec le wrapper Doctopus (`_extract_doctoplus`)."

    Mocke la biblioth√®que `DoctopusWrapper` pour simuler son comportement
    sans d√©pendre d'une installation r√©elle.
    """
    # Configure le mock de DoctopusWrapper.
    mock_instance = MockDoctopusWrapper.return_value
    mock_instance.extract_text.return_value = {
        "text": "Texte extrait par Doctopus.",
        "confidence": 0.98,
        "pages": 2
    }

    # Cr√©e une requ√™te OCR.
    request = OCRRequest(
        file_path="/path/to/real_doc.pdf",
        language="eng",
        preprocess=True,
        output_format="text"
    )

    # Appelle la fonction d'extraction Doctopus.
    result = await _extract_doctoplus(request)

    # Assertions sur le r√©sultat.
    assert result["text"] == "Texte extrait par Doctopus.", "Le texte devrait correspondre √† la sortie mock√©e."
    assert result["confidence"] == 0.98, "La confiance devrait correspondre √† la sortie mock√©e."
    assert result["metadata"]["pages"] == 2, "Les m√©tadonn√©es devraient inclure le nombre de pages."
    assert mock_instance.extract_text.called_once_with(
        file_path=request.file_path,
        language=request.language,
        preprocess=request.preprocess,
        confidence_threshold=request.confidence_threshold,
        output_format=request.output_format,
    )


@pytest.mark.asyncio
@patch('services.ocr.ocr_wrapper.redis_client', new_callable=AsyncMock)
async def test_ocr_caching(mock_redis_client: AsyncMock):
    """Teste la fonctionnalit√© de mise en cache du service OCR."

    V√©rifie que les r√©sultats sont stock√©s et r√©cup√©r√©s du cache Redis.
    """
    # Configure le mock Redis pour simuler un cache vide puis une valeur.
    mock_redis_client.get.return_value = None # Pas de cache au premier appel.
    mock_redis_client.setex.return_value = True

    # Mocke l'extracteur r√©el pour qu'il retourne une valeur connue.
    with patch('services.ocr.ocr_wrapper._extract_doctoplus', new_callable=AsyncMock) as mock_extractor:
        mock_extractor.return_value = {"text": "Contenu non mis en cache.", "confidence": 0.9}

        # Cr√©e une requ√™te OCR.
        request = OCRRequest(
            file_path="/path/to/doc.pdf",
            language="fra",
            preprocess=True,
            output_format="text",
            cache=True
        )

        # Premier appel (devrait √™tre un MISS et mettre en cache).
        from services.ocr.ocr_wrapper import extract_text as ocr_extract_text_func # Importe la fonction de l'endpoint.
        result1 = await ocr_extract_text_func(request)
        mock_redis_client.get.assert_called_once() # V√©rifie l'appel √† get.
        mock_redis_client.setex.assert_called_once() # V√©rifie l'appel √† setex.
        assert result1.cached is False
        assert result1.text == "Contenu non mis en cache."

        # R√©initialise les mocks pour le deuxi√®me appel.
        mock_redis_client.get.reset_mock()
        mock_redis_client.setex.reset_mock()
        mock_extractor.reset_mock()

        # Configure le mock Redis pour simuler un cache HIT.
        cached_data = {"text": "Contenu depuis le cache.", "confidence": 0.95, "processing_time": 0.01}
        mock_redis_client.get.return_value = json.dumps(cached_data)

        # Deuxi√®me appel (devrait √™tre un HIT).
        result2 = await ocr_extract_text_func(request)
        mock_redis_client.get.assert_called_once()
        mock_redis_client.setex.assert_not_called() # setex ne devrait pas √™tre appel√©.
        mock_extractor.assert_not_called() # L'extracteur ne devrait pas √™tre appel√©.
        assert result2.cached is True
        assert result2.text == "Contenu depuis le cache."

```

---

## Fichier : `backend\tests\test_orchestrator.py`

```python
# tests/test_orchestrator.py
"""Tests d'int√©gration pour la classe Orchestrator.

Ce module contient des tests qui v√©rifient le comportement de l'orchestrateur
dans divers sc√©narios, y compris les cas de succ√®s du pipeline complet,
la gestion des fichiers SFD vides ou manquants, et la gestion des erreurs
provenant des services d√©pendants (Qwen3, r√®gles m√©tier).
"""

import asyncio
from pathlib import Path
from unittest.mock import AsyncMock, patch

import pytest

from policies.business_rules import BusinessRules
from src.models.sfd_models import SFDAnalysisRequest
from src.orchestrator import Orchestrator


@pytest.fixture
async def orchestrator():
    """Fixture pour initialiser et fermer proprement l'orchestrateur pour chaque test."

    Cette fixture cr√©e une instance de l'Orchestrator et s'assure que ses
    d√©pendances (stubs ou mocks) sont correctement configur√©es pour les tests.
    """
    # Mocke les d√©pendances de l'orchestrateur pour l'isoler.
    starcoder_mock = AsyncMock()
    redis_client_mock = AsyncMock()
    config_mock = MagicMock()
    model_registry_mock = MagicMock()

    # Cr√©e l'instance de l'Orchestrator avec les mocks.
    orch = Orchestrator(starcoder_mock, redis_client_mock, config_mock, model_registry_mock)
    # Initialise l'orchestrateur (charge la config, etc.).
    await orch.initialize()
    yield orch
    # Ferme l'orchestrateur apr√®s le test.
    await orch.close()


@pytest.mark.integration
@pytest.mark.asyncio
async def test_full_pipeline_success(orchestrator: Orchestrator, tmp_path: Path):
    """Teste le sc√©nario de succ√®s du pipeline complet avec un fichier SFD valide."

    V√©rifie que l'orchestrateur peut traiter une SFD de bout en bout,
    extraire des sc√©narios, g√©n√©rer des tests et retourner un statut 'completed'.
    """
    # Pr√©pare un fichier SFD valide temporaire.
    sfd_content = "Sp√©cification: Test de connexion avec email et mot de passe. Sc√©nario: Connexion r√©ussie."
    sfd_path = tmp_path / "valid_sfd.txt"
    sfd_path.write_text(sfd_content)
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    # Mocke la m√©thode `analyze_sfd` de Qwen3 pour simuler une r√©ponse r√©ussie.
    orchestrator.qwen3.analyze_sfd.return_value = {
        "scenarios": [
            {"id": "SC-001", "titre": "Connexion r√©ussie", "description": "Test de connexion"}
        ]
    }
    # Mocke la m√©thode `generate_playwright_test` de Starcoder2.
    orchestrator.starcoder.generate_playwright_test.return_value = {"code": "def test_connexion(): pass", "test_name": "test_connexion"}

    # Ex√©cute le pipeline.
    result = await orchestrator.process_sfd_to_tests(sfd_request)

    # Assertions sur le r√©sultat.
    assert result["status"] == "completed", "Le pipeline devrait se terminer avec le statut 'completed'."
    assert result["metrics"]["scenarios_found"] > 0, "Des sc√©narios devraient √™tre trouv√©s."
    assert len(result["generated_tests"]) > 0, "Des tests devraient √™tre g√©n√©r√©s."
    assert "test_connexion" in result["generated_tests"][0]["test_name"], "Le nom du test g√©n√©r√© devrait √™tre correct."


@pytest.mark.asyncio
async def test_empty_sfd_file(orchestrator: Orchestrator, tmp_path: Path):
    """V√©rifie que l'orchestrateur g√®re correctement un fichier SFD vide."

    Le pipeline devrait d√©tecter le fichier vide et retourner un statut d'erreur.
    """
    sfd_path = tmp_path / "empty_sfd.txt"
    sfd_path.write_text("") # Cr√©e un fichier vide.
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "Le fichier de sp√©cifications est vide" in result["error_message"], "Le message d'erreur devrait indiquer un fichier vide."


@pytest.mark.asyncio
async def test_sfd_file_not_found(orchestrator: Orchestrator):
    """V√©rifie la gestion d'un chemin de fichier SFD inexistant."

    L'orchestrateur devrait retourner un statut d'erreur si le fichier n'est pas trouv√©.
    """
    # Cr√©e une requ√™te avec un contenu vide, simulant un fichier non trouv√©.
    sfd_request = SFDAnalysisRequest(content="")

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "Le fichier de sp√©cifications n'a pas √©t√© trouv√©" in result["error_message"], "Le message d'erreur devrait indiquer un fichier non trouv√©."


@pytest.mark.asyncio
@patch("src.models.qwen3.qwen3_interface.Qwen3OllamaInterface.analyze_sfd", new_callable=AsyncMock)
async def test_qwen3_service_unavailable(mock_analyze_sfd: AsyncMock, orchestrator: Orchestrator, tmp_path: Path):
    """Simule une panne du service Qwen3 et v√©rifie la gestion de l'erreur par l'orchestrateur."

    L'orchestrateur devrait capturer l'exception et retourner un statut d'erreur.
    """
    mock_analyze_sfd.side_effect = Exception("Service Qwen3 non disponible") # Simule une exception.
    sfd_path = tmp_path / "sfd.txt"
    sfd_path.write_text("Une sp√©cification simple.")
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "Erreur lors de l'analyse par Qwen3" in result["error_message"], "Le message d'erreur devrait refl√©ter la panne de Qwen3."


@pytest.mark.asyncio
@patch.object(BusinessRules, "validate", new_callable=AsyncMock)
async def test_business_rules_violation(mock_validate_rules: AsyncMock, orchestrator: Orchestrator, tmp_path: Path):
    """V√©rifie que le pipeline s'arr√™te si les r√®gles m√©tier ne sont pas respect√©es."

    Simule une violation des r√®gles m√©tier et s'assure que l'orchestrateur
    d√©tecte l'√©chec et retourne un statut d'erreur appropri√©.
    """
    # Simule une violation des r√®gles m√©tier en faisant retourner `False` par le validateur.
    mock_validate_rules.return_value = {
        "ok": False,
        "violations": ["Utilisation de time.sleep() d√©tect√©e."],
    }

    sfd_path = tmp_path / "sfd_with_violation.txt"
    sfd_path.write_text("Sp√©cification qui g√©n√©rera un test non conforme.")
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "Validation des r√®gles m√©tier √©chou√©e" in result["error_message"], "Le message d'erreur devrait indiquer une violation des r√®gles m√©tier."
    assert "Utilisation de time.sleep() d√©tect√©e." in result["details"], "Les d√©tails de la violation devraient √™tre pr√©sents."


@pytest.mark.asyncio
async def test_syntax_error_in_sfd(orchestrator: Orchestrator, tmp_path: Path):
    """V√©rifie la gestion d'une erreur de syntaxe dans le fichier SFD."

    Simule un fichier SFD avec une erreur de syntaxe et s'assure que l'orchestrateur
    la d√©tecte et retourne un statut d'erreur.
    """
    sfd_path = tmp_path / "invalid_sfd.txt"
    sfd_path.write_text("Sp√©cification: Test de connexion avec email et mot de passe.\nSyntaxError: invalid syntax")
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "Erreur de syntaxe dans le fichier SFD" in result["error_message"], "Le message d'erreur devrait indiquer une erreur de syntaxe."


@pytest.mark.asyncio
@patch("src.models.qwen3.qwen3_interface.Qwen3OllamaInterface.analyze_sfd", new_callable=AsyncMock)
async def test_qwen3_service_timeout(mock_analyze_sfd: AsyncMock, orchestrator: Orchestrator, tmp_path: Path):
    """Simule un d√©lai d'attente (timeout) du service Qwen3 et v√©rifie la gestion de l'erreur."

    L'orchestrateur devrait capturer le `asyncio.TimeoutError` et retourner un statut d'erreur.
    """
    mock_analyze_sfd.side_effect = asyncio.TimeoutError("Service Qwen3 en d√©lai d'attente")
    sfd_path = tmp_path / "sfd.txt"
    sfd_path.write_text("Une sp√©cification simple.")
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "D√©lai d'attente du service Qwen3" in result["error_message"], "Le message d'erreur devrait indiquer un timeout."


@pytest.mark.asyncio
@patch("src.models.qwen3.qwen3_interface.Qwen3OllamaInterface.analyze_sfd", new_callable=AsyncMock)
async def test_qwen3_service_invalid_response(mock_analyze_sfd: AsyncMock, orchestrator: Orchestrator, tmp_path: Path):
    """Simule une r√©ponse invalide du service Qwen3 et v√©rifie la gestion de l'erreur."

    L'orchestrateur devrait d√©tecter la r√©ponse invalide et retourner un statut d'erreur.
    """
    # Simule une r√©ponse de Qwen3 qui ne contient pas les donn√©es attendues.
    mock_analyze_sfd.return_value = {"error": "R√©ponse invalide"}
    sfd_path = tmp_path / "sfd.txt"
    sfd_path.write_text("Une sp√©cification simple.")
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text())

    result = await orchestrator.process_sfd_to_tests(sfd_request)

    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "R√©ponse invalide du service Qwen3" in result["error_message"], "Le message d'erreur devrait indiquer une r√©ponse invalide."
```

---

## Fichier : `backend\tests\test_personality_quiz.py`

```python
# tests/test_personality_quiz.py
"""Tests unitaires pour le module `PersonalityQuiz`.

Ce module contient des tests pour v√©rifier le bon fonctionnement du quiz de
personnalisation de l'IA, y compris l'initialisation, le traitement des
questions √† choix multiples et la g√©n√©ration du profil de personnalit√©.
"""

import pytest
from unittest.mock import patch, MagicMock, AsyncMock
from src.modules.psychodesign.personality_quiz import PersonalityQuiz, QuizResponse, PersonalityProfile
from datetime import datetime


@pytest.fixture
def quiz():
    """Fixture pour une instance de `PersonalityQuiz` pour les tests."

    Initialise le quiz avec un utilisateur de test et des mocks pour les
    d√©pendances externes comme `speech_recognition`.
    """
    # Mocke les d√©pendances de speech_recognition si elles ne sont pas disponibles.
    with patch('src.modules.psychodesign.personality_quiz.sr') as mock_sr:
        mock_sr.Recognizer.return_value = MagicMock()
        mock_sr.Microphone.return_value = MagicMock()
        mock_sr.UnknownValueError = type('UnknownValueError', (Exception,), {})
        return PersonalityQuiz("test_user")


@pytest.mark.asyncio
async def test_quiz_initialization(quiz: PersonalityQuiz):
    """Teste l'initialisation correcte du quiz de personnalit√©."

    V√©rifie que l'ID utilisateur est correctement d√©fini et que les questions
    sont charg√©es.
    """
    assert quiz.user_id == "test_user"
    assert len(quiz.questions) > 0
    assert quiz.recognizer is not None # V√©rifie que le mock est bien initialis√©.


@pytest.mark.asyncio
@patch('builtins.input', return_value='1')
async def test_choice_question_handling(mock_input: MagicMock, quiz: PersonalityQuiz):
    """Teste le traitement des questions √† choix multiples."

    Simule une r√©ponse utilisateur et v√©rifie que la valeur correcte est retourn√©e.
    """
    question = {
        "id": "test_choice",
        "type": "choice",
        "question": "Test?",
        "options": [{"text": "A", "weight": 0.1}, {"text": "B", "weight": 0.9}]
    }

    response = await quiz._handle_choice_question(question)
    assert response["value"] == 0.1
    mock_input.assert_called_once() # V√©rifie que `input()` a √©t√© appel√©.


@pytest.mark.asyncio
@patch('builtins.input', return_value='0.75')
async def test_scale_question_handling(mock_input: MagicMock, quiz: PersonalityQuiz):
    """Teste le traitement des questions √† √©chelle."

    Simule une r√©ponse num√©rique et v√©rifie que la valeur est correctement trait√©e.
    """
    question = {
        "id": "test_scale",
        "type": "scale",
        "question": "√âvaluez de 0 √† 1 :",
        "scale": {"min": "Faible", "max": "Fort"}
    }
    response = await quiz._handle_scale_question(question)
    assert response["value"] == 0.75
    mock_input.assert_called_once()


@pytest.mark.asyncio
@patch('builtins.input', return_value='Ceci est une r√©ponse textuelle.')
async def test_text_question_handling(mock_input: MagicMock, quiz: PersonalityQuiz):
    """Teste le traitement des questions textuelles."

    V√©rifie que la r√©ponse textuelle est captur√©e correctement.
    """
    question = {"id": "test_text", "type": "text", "question": "D√©crivez..."}
    response = await quiz._handle_text_question(question)
    assert response["value"] == "Ceci est une r√©ponse textuelle."
    mock_input.assert_called_once()


@pytest.mark.asyncio
async def test_calibration_question_handling_success(quiz: PersonalityQuiz):
    """Teste le traitement d'une question de calibration vocale en cas de succ√®s."

    Mocke `speech_recognition` pour simuler une transcription r√©ussie.
    """
    if not quiz.recognizer: # Skip si speech_recognition n'est pas mock√©.
        pytest.skip("Speech recognition not mocked.")

    # Mocke les m√©thodes de speech_recognition.
    quiz.recognizer.adjust_for_ambient_noise = MagicMock()
    quiz.recognizer.listen = AsyncMock(return_value=MagicMock())
    quiz.recognizer.recognize_google = MagicMock(return_value="Phrase de test vocale.")
    
    # Mocke la fonction _extract_vocal_features.
    with patch.object(quiz, '_extract_vocal_features', new_callable=AsyncMock) as mock_extract_vocal_features:
        mock_extract_vocal_features.return_value = {"pitch": 1.0, "speed": 1.0}

        question = {"id": "vocal_test", "type": "calibration", "question": "Lisez ceci.", "purpose": "test"}
        response = await quiz._handle_calibration_question(question)

        assert response["value"] == "Phrase de test vocale."
        assert response["confidence"] == 1.0
        assert "pitch" in response["vocal_features"]
        assert len(quiz.vocal_samples) == 1
        quiz.recognizer.adjust_for_ambient_noise.assert_called_once()
        quiz.recognizer.listen.assert_called_once()
        quiz.recognizer.recognize_google.assert_called_once()


@pytest.mark.asyncio
async def test_calibration_question_handling_failure(quiz: PersonalityQuiz):
    """Teste le traitement d'une question de calibration vocale en cas d'√©chec de reconnaissance."

    Simule une `UnknownValueError` de `speech_recognition`.
    """
    if not quiz.recognizer:
        pytest.skip("Speech recognition not mocked.")

    quiz.recognizer.adjust_for_ambient_noise = MagicMock()
    quiz.recognizer.listen = AsyncMock(side_effect=quiz.recognizer.UnknownValueError("Could not understand audio"))
    
    # Patch builtins.input pour √©viter l'interaction utilisateur dans le retry.
    with patch('builtins.input', side_effect=['' for _ in range(2)]) as mock_input_retry:
        question = {"id": "vocal_test_fail", "type": "calibration", "question": "Lisez ceci.", "purpose": "test"}
        response = await quiz._handle_calibration_question(question)

        assert response["value"] == "error" # Ou un autre statut d'erreur.
        assert response["confidence"] == 0.0
        assert len(quiz.vocal_samples) == 0
        quiz.recognizer.listen.assert_called_once()


def test_personality_profile_generation(quiz: PersonalityQuiz):
    """Teste la g√©n√©ration correcte du profil de personnalit√© √† partir des r√©ponses du quiz."

    V√©rifie que les traits et pr√©f√©rences sont calcul√©s et stock√©s correctement.
    """
    # Simule des r√©ponses pour le quiz.
    quiz.responses = [
        QuizResponse(question_id="comm_1", response="vous", confidence=1.0, response_time=1.0, vocal_features={}),
        QuizResponse(question_id="comm_2", response=0.7, confidence=1.0, response_time=1.0, vocal_features={}),
        QuizResponse(question_id="work_1", response=0.3, confidence=1.0, response_time=1.0, vocal_features={}),
    ]

    profile = quiz._generate_profile()
    assert profile.user_id == "test_user"
    assert "formalite" in profile.traits
    assert profile.traits["formalite"] == 0.8 # Bas√© sur la r√©ponse "vous".
    assert profile.traits["verbosite"] == 0.7 # Bas√© sur la r√©ponse 0.7.
    assert profile.traits["empathie"] == 0.3 # Bas√© sur la r√©ponse 0.3.
    assert profile.preferences["vouvoiement"] is True
    assert "completed_at" in profile.quiz_metadata


@pytest.mark.asyncio
async def test_save_profile(quiz: PersonalityQuiz, tmp_path: Path):
    """Teste la sauvegarde du profil de personnalit√© sur le disque."

    V√©rifie que le fichier JSON du profil est cr√©√© et contient les bonnes donn√©es.
    """
    # Surcharge le chemin de sauvegarde pour utiliser un r√©pertoire temporaire.
    quiz.quiz_path = tmp_path / "quiz_data_temp"
    quiz.quiz_path.mkdir()

    profile = PersonalityProfile(
        user_id="save_test_user",
        traits={"formalite": 0.5},
        preferences={},
        vocal_profile={},
        behavioral_patterns={},
        quiz_metadata={'completed_at': datetime.now().isoformat()}
    )

    await quiz._save_profile(profile)

    saved_file = quiz.quiz_path / "save_test_user_profile.json"
    assert saved_file.exists()
    
    with open(saved_file, 'r', encoding='utf-8') as f:
        loaded_data = json.load(f)
    assert loaded_data["user_id"] == "save_test_user"
    assert loaded_data["traits"]["formalite"] == 0.5


def test_get_progress(quiz: PersonalityQuiz):
    """Teste la fonction de suivi de la progression du quiz."

    V√©rifie que le pourcentage de compl√©tion et la section courante sont corrects.
    """
    # Quiz vide.
    progress_empty = quiz.get_progress()
    assert progress_empty["completed"] == 0
    assert progress_empty["percentage"] == 0.0
    assert progress_empty["current_section"] == "G√©n√©ral"

    # Simule quelques r√©ponses.
    quiz.responses.append(QuizResponse(question_id="comm_1", response="tu", confidence=1.0, response_time=1.0, vocal_features={}))
    progress_partial = quiz.get_progress()
    assert progress_partial["completed"] == 1
    assert progress_partial["percentage"] > 0.0
    assert progress_partial["current_section"] == "Communication" # Ou la section de la question suivante.

    # Simule la compl√©tion du quiz.
    quiz.responses = [QuizResponse(question_id=q["id"], response="mock", confidence=1.0, response_time=1.0, vocal_features={}) for q in quiz.questions]
    progress_complete = quiz.get_progress()
    assert progress_complete["completed"] == len(quiz.questions)
    assert progress_complete["percentage"] == 100.0
    assert progress_complete["current_section"] == "Termin√©"
```

---

## Fichier : `backend\tests\test_playwright_runner.py`

```python
# tests/test_playwright_runner.py
"""Tests unitaires pour le service d'ex√©cution de tests Playwright (`playwright_runner.py`).

Ce module contient des tests pour v√©rifier les fonctionnalit√©s cl√©s du runner
Playwright, telles que la pr√©paration des fichiers de test, la g√©n√©ration de
la configuration Pytest, et l'ex√©cution des tests.
"""

import pytest
import asyncio
from pathlib import Path
from typing import Dict, Any

# Importe les fonctions et classes du module playwright_runner.
from services.playwright.playwright_runner import prepare_test_files, generate_pytest_config, TestCode, ExecutionConfig


@pytest.fixture
def test_scenario_data() -> Dict[str, Any]:
    """Fixture fournissant des donn√©es de sc√©nario de test Playwright factices."""
    return {
        "code": "await page.goto('https://example.com')\nawait expect(page).to_have_title('Example')",
        "test_name": "test_navigation_example",
        "test_type": "e2e"
    }


@pytest.fixture
def temp_workspace(tmp_path: Path) -> Path:
    """Fixture fournissant un r√©pertoire de travail temporaire pour les tests."

    Args:
        tmp_path: Fixture Pytest pour un r√©pertoire temporaire.

    Returns:
        Le chemin vers le r√©pertoire de travail temporaire.
    """
    workspace_dir = tmp_path / "workspace"
    workspace_dir.mkdir()
    return workspace_dir


@pytest.mark.asyncio
async def test_prepare_test_files(temp_workspace: Path, test_scenario_data: Dict[str, Any]):
    """Teste la pr√©paration des fichiers de test √† partir des donn√©es de sc√©nario."

    V√©rifie que les fichiers `.py` sont cr√©√©s correctement dans le r√©pertoire
    temporaire et qu'ils contiennent le code de test.
    """
    # Cr√©e un objet TestCode √† partir des donn√©es de la fixture.
    test_code_obj = TestCode(**test_scenario_data)

    # Appelle la fonction √† tester.
    test_files = await prepare_test_files([test_code_obj], temp_workspace)

    # Assertions.
    assert len(test_files) == 1, "Un seul fichier de test devrait √™tre cr√©√©."
    created_file = test_files[0]
    assert created_file.exists(), "Le fichier de test devrait exister."
    assert created_file.suffix == ".py", "Le fichier devrait avoir l'extension .py."
    assert test_code_obj.test_name in created_file.name, "Le nom du fichier devrait contenir le nom du test."

    # V√©rifie le contenu du fichier.
    content = created_file.read_text()
    assert "import pytest" in content, "Le fichier devrait contenir l'import pytest."
    assert "from playwright.async_api import Page, expect" in content, "Le fichier devrait contenir les imports Playwright."
    assert test_code_obj.code in content, "Le fichier devrait contenir le code du sc√©nario."
    assert "@pytest.mark.asyncio" in content, "Le test asynchrone devrait √™tre d√©cor√© avec @pytest.mark.asyncio."

    # V√©rifie que conftest.py est cr√©√©.
    conftest_path = temp_workspace / "conftest.py"
    assert conftest_path.exists(), "Le fichier conftest.py devrait √™tre cr√©√©."


def test_generate_pytest_config():
    """Teste la g√©n√©ration des arguments de ligne de commande pour Pytest."

    V√©rifie que les arguments g√©n√©r√©s refl√®tent correctement la configuration
    d'ex√©cution fournie.
    """
    # Cr√©e un objet ExecutionConfig avec diff√©rentes options.
    config = ExecutionConfig(
        browser="firefox",
        headed=True,
        timeout=60000, # 60 secondes.
        retries=1,
        parallel=True,
        workers=3,
        screenshot="on-failure",
        video="always",
        trace="on-failure",
        base_url="https://my-app.com"
    )

    # Appelle la fonction √† tester.
    pytest_args = generate_pytest_config(config, Path("/test_workspace"))

    # Assertions sur les arguments g√©n√©r√©s.
    assert "-v" in pytest_args, "Le mode verbeux devrait √™tre activ√©."
    assert "--tb=short" in pytest_args, "Le format de traceback courte devrait √™tre activ√©."
    assert "--json-report" in pytest_args, "Le rapport JSON devrait √™tre activ√©."
    assert "--browser=firefox" in pytest_args, "Le navigateur Firefox devrait √™tre sp√©cifi√©."
    assert "--headed" in pytest_args, "Le mode 'headed' devrait √™tre activ√©."
    assert "-n" in pytest_args, "L'option de parall√©lisation (-n) devrait √™tre pr√©sente."
    assert "3" in pytest_args, "Le nombre de workers devrait √™tre 3."
    assert "--screenshot=only-on-failure" in pytest_args, "La capture d'√©cran sur √©chec devrait √™tre configur√©e."
    assert "--video=on" in pytest_args, "L'enregistrement vid√©o devrait √™tre activ√©."
    assert "--tracing=retain-on-failure" in pytest_args, "Le tra√ßage sur √©chec devrait √™tre configur√©."
    assert "--timeout=60" in pytest_args, "Le timeout devrait √™tre de 60 secondes."

    # V√©rifie que les arguments sp√©cifiques au workspace sont pr√©sents.
    assert str(Path("/test_workspace")) in pytest_args, "Le chemin du workspace devrait √™tre inclus."
    assert f"--json-report-file={Path("/test_workspace") / 'report.json'}" in pytest_args, "Le chemin du rapport JSON devrait √™tre correct."

```

---

## Fichier : `backend\tests\test_retry_handler.py`

```python
# tests/test_retry_handler.py
"""Tests unitaires pour le gestionnaire de retry (`RetryHandler`).

Ce module contient des tests pour v√©rifier le bon fonctionnement du d√©corateur
`@RetryHandler.with_retry`, y compris les sc√©narios de succ√®s apr√®s √©chec
et d'√©puisement des tentatives.
"""

import pytest
import asyncio
import logging

from src.utils.retry_handler import RetryHandler

logger = logging.getLogger(__name__)


@pytest.fixture
def retry_handler_instance():
    """Fixture pour fournir une instance de `RetryHandler` pour les tests."

    Utilise les valeurs par d√©faut pour les param√®tres du gestionnaire.
    """
    return RetryHandler()


@pytest.mark.asyncio
async def test_retry_success(retry_handler_instance: RetryHandler):
    """Teste que la fonction d√©cor√©e r√©ussit apr√®s un ou plusieurs √©checs initiaux."

    V√©rifie que la fonction est retent√©e le nombre de fois n√©cessaire avant de r√©ussir.
    """
    call_count = 0

    @retry_handler_instance.with_retry(max_attempts=3, exceptions=(ValueError,))
    async def flaky_function():
        nonlocal call_count
        call_count += 1
        logger.info(f"Appel de flaky_function, tentative #{call_count}")
        if call_count < 2:
            raise ValueError("√âchec simul√©")
        return "succ√®s"

    result = await flaky_function()
    assert result == "succ√®s", "La fonction devrait r√©ussir apr√®s retry."
    assert call_count == 2, "La fonction devrait √™tre appel√©e 2 fois (1 √©chec + 1 succ√®s)."


@pytest.mark.asyncio
async def test_retry_exhaustion(retry_handler_instance: RetryHandler):
    """Teste que le retry s'arr√™te apr√®s avoir √©puis√© le nombre maximal de tentatives."

    V√©rifie qu'une exception est lev√©e apr√®s le nombre maximal de tentatives.
    """
    call_count = 0

    @retry_handler_instance.with_retry(max_attempts=2, exceptions=(ValueError,))
    async def always_failing_function():
        nonlocal call_count
        call_count += 1
        logger.info(f"Appel de always_failing_function, tentative #{call_count}")
        raise ValueError("√âchec permanent")

    with pytest.raises(ValueError) as excinfo:
        await always_failing_function()
    
    assert "√âchec permanent" in str(excinfo.value), "L'exception lev√©e devrait √™tre celle de la fonction."
    assert call_count == 2, "La fonction devrait √™tre appel√©e exactement 2 fois."


@pytest.mark.asyncio
async def test_retry_different_exception_type(retry_handler_instance: RetryHandler):
    """Teste que le retry ne se d√©clenche que pour les types d'exceptions sp√©cifi√©s."

    V√©rifie qu'une exception non sp√©cifi√©e n'est pas retent√©e.
    """
    call_count = 0

    @retry_handler_instance.with_retry(max_attempts=3, exceptions=(ValueError,))
    async def specific_exception_function():
        nonlocal call_count
        call_count += 1
        logger.info(f"Appel de specific_exception_function, tentative #{call_count}")
        if call_count == 1:
            raise TypeError("Type d'erreur inattendu") # Cette exception ne devrait pas √™tre retent√©e.
        return "succ√®s"

    with pytest.raises(TypeError) as excinfo:
        await specific_exception_function()
    
    assert "Type d'erreur inattendu" in str(excinfo.value), "L'exception TypeError devrait √™tre lev√©e imm√©diatement."
    assert call_count == 1, "La fonction ne devrait √™tre appel√©e qu'une seule fois."


@pytest.mark.asyncio
async def test_circuit_breaker_open(retry_handler_instance: RetryHandler):
    """Teste que le disjoncteur s'ouvre apr√®s un certain nombre d'√©checs."

    V√©rifie que les appels ult√©rieurs sont bloqu√©s tant que le disjoncteur est ouvert.
    """
    service_name = "test_service"
    call_count = 0

    @retry_handler_instance.circuit_breaker
    async def unreliable_service():
        nonlocal call_count
        call_count += 1
        raise Exception("Service en panne")

    # Provoque l'ouverture du disjoncteur.
    for _ in range(retry_handler_instance.failure_threshold):
        with pytest.raises(Exception):
            await unreliable_service()
    
    assert retry_handler_instance._is_open.get(service_name), "Le disjoncteur devrait √™tre ouvert."

    # Tente un appel alors que le disjoncteur est ouvert.
    with pytest.raises(Exception) as excinfo:
        await unreliable_service()
    assert "Circuit breaker is open" in str(excinfo.value), "L'appel devrait √™tre bloqu√© par le disjoncteur."
    assert call_count == retry_handler_instance.failure_threshold, "Aucun appel suppl√©mentaire ne devrait avoir eu lieu."


@pytest.mark.asyncio
async def test_circuit_breaker_reset(retry_handler_instance: RetryHandler):
    """Teste que le disjoncteur se r√©initialise apr√®s le timeout de r√©cup√©ration."

    V√©rifie que le disjoncteur passe de l'√©tat ouvert √† ferm√© apr√®s le d√©lai.
    """
    service_name = "test_service_reset"
    call_count = 0

    @retry_handler_instance.circuit_breaker
    async def intermittently_failing_service():
        nonlocal call_count
        call_count += 1
        if call_count <= retry_handler_instance.failure_threshold:
            raise Exception("√âchec initial")
        return "Service r√©cup√©r√©"

    # Provoque l'ouverture du disjoncteur.
    for _ in range(retry_handler_instance.failure_threshold):
        with pytest.raises(Exception):
            await intermittently_failing_service()
    
    assert retry_handler_instance._is_open.get(service_name), "Le disjoncteur devrait √™tre ouvert."

    # Attend le timeout de r√©cup√©ration.
    await asyncio.sleep(retry_handler_instance.recovery_timeout + 0.1)

    # Le premier appel apr√®s le timeout devrait tenter de se refermer.
    result = await intermittently_failing_service()
    assert result == "Service r√©cup√©r√©", "Le service devrait se r√©cup√©rer."
    assert not retry_handler_instance._is_open.get(service_name), "Le disjoncteur devrait √™tre ferm√©."
```

---

## Fichier : `backend\tests\test_services.py`

```python
"""
Tests fonctionnels pour les microservices de l'application Altiora.

Ce module contient des tests qui v√©rifient la fonctionnalit√© de base de
chaque microservice expos√© via HTTP (ALM, Excel). Ces tests s'assurent
que les services r√©pondent correctement aux requ√™tes et g√®rent les cas
d'erreur, en se basant sur leurs endpoints de sant√© et leurs APIs sp√©cifiques.
"""

import pytest
import httpx
import logging

logger = logging.getLogger(__name__)

# --- Configuration des clients de test ---
# Ces URLs ciblent les services qui devraient √™tre en cours d'ex√©cution.
# Assurez-vous que les services sont lanc√©s avant d'ex√©cuter ces tests.

BASE_URL_ALM = "http://localhost:8002"
BASE_URL_EXCEL = "http://localhost:8003"


@pytest.mark.service
@pytest.mark.asyncio
async def test_alm_service_health():
    """V√©rifie que le service ALM est accessible et retourne un statut sain."

    Ce test envoie une requ√™te GET √† l'endpoint `/health` du service ALM
    et s'attend √† une r√©ponse HTTP 200 avec un statut "ok".
    """
    logger.info(f"Test de l'√©tat de sant√© du service ALM √† {BASE_URL_ALM}/health")
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL_ALM}/health")
        response.raise_for_status() # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx.
        assert response.status_code == 200, f"Le service ALM devrait retourner un statut 200, mais a retourn√© {response.status_code}."
        assert response.json() == {"status": "ok"}, "Le corps de la r√©ponse devrait √™tre {'status': 'ok'}."


@pytest.mark.service
@pytest.mark.asyncio
async def test_alm_create_work_item_success():
    """Teste la cr√©ation r√©ussie d'un √©l√©ment de travail via le service ALM."

    Ce test envoie une requ√™te POST √† l'endpoint `/work-items` avec des donn√©es
    valides et s'attend √† une r√©ponse de succ√®s, v√©rifiant la structure de la r√©ponse.
    """
    logger.info(f"Test de cr√©ation d'un √©l√©ment de travail via le service ALM √† {BASE_URL_ALM}/work-items")
    payload = {
        "title": "Nouveau bug trouv√©",
        "description": "Le bouton de connexion ne fonctionne pas sur Firefox.",
        "item_type": "Bug"
    }
    async with httpx.AsyncClient() as client:
        response = await client.post(f"{BASE_URL_ALM}/work-items", json=payload)
        response.raise_for_status()
        assert response.status_code == 200, f"La cr√©ation d'un √©l√©ment de travail devrait retourner un statut 200, mais a retourn√© {response.status_code}."
        data = response.json()
        assert data["success"] is True, "Le champ 'success' devrait √™tre True."
        assert "work_item" in data, "La r√©ponse devrait contenir les d√©tails de l'√©l√©ment de travail."
        assert data["work_item"]["key"] == "PROJ-123", "La cl√© de l'√©l√©ment de travail devrait correspondre √† la maquette (PROJ-123)."


@pytest.mark.service
@pytest.mark.asyncio
async def test_alm_create_work_item_validation_error():
    """Teste la gestion d'une requ√™te invalide par le service ALM."

    Ce test envoie une requ√™te POST avec des donn√©es manquantes ou invalides
    et s'attend √† une r√©ponse HTTP 422 (Unprocessable Entity) indiquant une erreur de validation.
    """
    logger.info(f"Test de gestion d'erreur de validation par le service ALM √† {BASE_URL_ALM}/work-items")
    payload = {"description": "Description sans titre"} # Le champ 'title' est requis et manquant.
    async with httpx.AsyncClient() as client:
        response = await client.post(f"{BASE_URL_ALM}/work-items", json=payload)
        assert response.status_code == 422, f"Une requ√™te invalide devrait retourner un statut 422, mais a retourn√© {response.status_code}."
        data = response.json()
        assert "detail" in data, "La r√©ponse devrait contenir des d√©tails sur l'erreur."
        assert any("field required" in str(err) for err in data["detail"]), "Le message d'erreur devrait indiquer un champ manquant."


@pytest.mark.service
@pytest.mark.asyncio
async def test_excel_service_health():
    """V√©rifie que le service Excel est accessible et retourne un statut sain."

    Similaire au test de sant√© du service ALM, mais pour le service Excel.
    """
    logger.info(f"Test de l'√©tat de sant√© du service Excel √† {BASE_URL_EXCEL}/health")
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL_EXCEL}/health")
        response.raise_for_status()
        assert response.status_code == 200, f"Le service Excel devrait retourner un statut 200, mais a retourn√© {response.status_code}."
        assert response.json() == {"status": "ok"}, "Le corps de la r√©ponse devrait √™tre {'status': 'ok'}."


@pytest.mark.service
@pytest.mark.asyncio
async def test_excel_create_matrix_success():
    """Teste la cr√©ation r√©ussie d'une matrice de test Excel."

    Ce test envoie des donn√©es de cas de test valides au service Excel et v√©rifie
    que le service retourne un fichier Excel non vide avec le bon type de contenu.
    """
    logger.info(f"Test de cr√©ation d'une matrice Excel via le service Excel √† {BASE_URL_EXCEL}/create-test-matrix")
    payload = {
        "filename": "test_matrix.xlsx",
        "test_cases": [
            {
                "id": "CU01_SB01_CP001_connexion_valide",
                "description": "V√©rifier la connexion r√©ussie.",
                "type": "CP"
            },
            {
                "id": "CU01_SB01_CE001_mot_de_passe_incorrect",
                "description": "V√©rifier le message d'erreur.",
                "type": "CE"
            }
        ]
    }
    async with httpx.AsyncClient(timeout=10) as client:
        response = await client.post(f"{BASE_URL_EXCEL}/create-test-matrix", json=payload)
        response.raise_for_status()
        assert response.status_code == 200, f"La cr√©ation de la matrice Excel devrait retourner un statut 200, mais a retourn√© {response.status_code}."
        assert response.headers["content-type"] == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "Le type de contenu devrait √™tre un fichier Excel."
        assert len(response.content) > 0, "Le contenu du fichier Excel ne devrait pas √™tre vide."


@pytest.mark.service
@pytest.mark.asyncio
async def test_excel_create_matrix_validation_error():
    """Teste la gestion de donn√©es invalides par le service Excel lors de la cr√©ation d'une matrice."

    Ce test envoie des donn√©es de cas de test qui violent les r√®gles de validation
    (ex: ID invalide) et s'attend √† une r√©ponse HTTP 400 (Bad Request) avec des d√©tails sur l'erreur.
    """
    logger.info(f"Test de gestion d'erreur de validation par le service Excel √† {BASE_URL_EXCEL}/create-test-matrix")
    payload = {
        "filename": "invalid_matrix.xlsx",
        "test_cases": [
            {
                "id": "ID_INVALIDE", # ID invalide selon la regex.
                "description": "Cet ID n'est pas valide.",
                "type": "CP"
            }
        ]
    }
    async with httpx.AsyncClient() as client:
        response = await client.post(f"{BASE_URL_EXCEL}/create-test-matrix", json=payload)
        assert response.status_code == 400, f"Une requ√™te avec des donn√©es invalides devrait retourner un statut 400, mais a retourn√© {response.status_code}."
        data = response.json()
        assert "detail" in data, "La r√©ponse devrait contenir des d√©tails sur l'erreur."
        assert "Les donn√©es des cas de test sont invalides" in data["detail"]["message"], "Le message d'erreur devrait indiquer une validation √©chou√©e."
        assert len(data["detail"]["errors"]) > 0, "La liste des erreurs de validation ne devrait pas √™tre vide."
```

---

## Fichier : `backend\tests\__init__.py`

```python
# tests/__init__.py
"""Initialise le package des tests.

Ce fichier peut √™tre vide ou contenir des configurations globales pour la suite de tests.
"""

```

---

## Fichier : `backend\tests\integration\conftest.py`

```python
# tests/integration/conftest.py
"""Configuration des tests d'int√©gration pour le projet Altiora.

Ce fichier contient des fixtures Pytest sp√©cifiques aux tests d'int√©gration.
Elles sont utilis√©es pour configurer l'environnement de test, notamment
la connexion √† Redis et l'attente de la disponibilit√© des microservices.
"""

import pytest
import asyncio
import redis.asyncio as redis
from pathlib import Path
from typing import Dict, Any
import httpx # Utilis√© pour les v√©rifications de service.
import logging

logger = logging.getLogger(__name__)


@pytest.fixture(scope="session")
def integration_config() -> Dict[str, Any]:
    """Fixture fournissant la configuration de base pour les tests d'int√©gration."

    Returns:
        Un dictionnaire contenant les URLs des services et de Redis.
    """
    return {
        "ollama_host": "http://localhost:11434",
        "services": {
            "ocr": "http://localhost:8001",
            "alm": "http://localhost:8002",
            "excel": "http://localhost:8003",
            "playwright": "http://localhost:8004",
            "dash": "http://localhost:8050", # Ajout√© pour le service Dash.
        },
        "redis_url": "redis://localhost:6379"
    }


@pytest.fixture(scope="session")
async def redis_client(integration_config: Dict[str, Any]) -> redis.Redis:
    """Fixture fournissant un client Redis asynchrone pour les tests."

    Le client est connect√© √† l'URL sp√©cifi√©e dans la configuration d'int√©gration.
    """
    client = await redis.from_url(integration_config["redis_url"], decode_responses=True)
    logger.info("Connexion au client Redis pour les tests d'int√©gration.")
    yield client
    logger.info("Fermeture de la connexion Redis pour les tests d'int√©gration.")
    await client.aclose()


@pytest.fixture(scope="function")
async def clear_redis(redis_client: redis.Redis):
    """Fixture nettoyant la base de donn√©es Redis avant et apr√®s chaque test."

    Assure un √©tat propre pour chaque test d'int√©gration.
    """
    logger.info("Nettoyage de Redis avant le test.")
    await redis_client.flushdb()
    yield
    logger.info("Nettoyage de Redis apr√®s le test.")
    await redis_client.flushdb()


@pytest.fixture(scope="session", autouse=True) # autouse=True signifie que cette fixture est ex√©cut√©e automatiquement.
async def wait_for_services(integration_config: Dict[str, Any]):
    """Fixture attendant que tous les microservices n√©cessaires soient pr√™ts et accessibles."

    Cette fixture est cruciale pour les tests d'int√©gration, car elle garantit
    que toutes les d√©pendances externes sont op√©rationnelles avant l'ex√©cution des tests.
    """
    service_urls = [
        integration_config["ollama_host"] + "/api/tags", # Endpoint pour v√©rifier Ollama.
        integration_config["services"]["ocr"] + "/health",
        integration_config["services"]["alm"] + "/health",
        integration_config["services"]["excel"] + "/health",
        integration_config["services"]["playwright"] + "/health",
        integration_config["services"]["dash"] + "/health",
    ]

    async def check_service(url: str) -> bool:
        """V√©rifie la disponibilit√© d'un service en envoyant une requ√™te HTTP GET √† son URL."

        Args:
            url: L'URL du service √† v√©rifier.

        Returns:
            True si le service r√©pond avec un statut 200, False sinon.
        """
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                resp = await client.get(url)
                return resp.status_code == 200
        except httpx.RequestError as e:
            logger.debug(f"Service {url} non joignable : {e}")
            return False
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la v√©rification du service {url}: {e}")
            return False

    max_wait_time = 120 # Temps maximal d'attente en secondes.
    check_interval = 1 # Intervalle entre les v√©rifications en secondes.
    start_time = asyncio.get_event_loop().time()

    logger.info(f"Attente de la disponibilit√© des services ({max_wait_time}s max)...")
    while asyncio.get_event_loop().time() - start_time < max_wait_time:
        # Ex√©cute toutes les v√©rifications de service en parall√®le.
        ready_checks = await asyncio.gather(*[check_service(url) for url in service_urls])
        if all(ready_checks):
            logger.info("‚úÖ Tous les services sont pr√™ts.")
            return
        logger.info("Services non encore pr√™ts, nouvelle tentative dans 1 seconde...")
        await asyncio.sleep(check_interval)

    pytest.fail(f"Les services n'ont pas d√©marr√© apr√®s {max_wait_time} secondes. Les tests d'int√©gration ne peuvent pas √™tre ex√©cut√©s.")


# Marqueurs personnalis√©s pour Pytest.
pytest.mark.integration = pytest.mark.integration
pytest.mark.performance = pytest.mark.performance

```

---

## Fichier : `backend\tests\integration\makefile`

```makefile
# Makefile
.PHONY: test-integration test-performance setup-integration

setup-integration:
	@echo "üöÄ Configuration des tests d'int√©gration..."
	@docker-compose up -d --wait
	@./scripts/validate_microservices.sh

test-integration: setup-integration
	@echo "üß™ Lancement des tests d'int√©gration..."
	@pytest tests/integration/ -v --tb=short -m integration

test-performance: setup-integration
	@echo "‚ö° Lancement des tests de performance..."
	@pytest tests/integration/ -v --tb=short -m performance

test-full: setup-integration
	@echo "üîç Tests complets avec couverture..."
	@pytest tests/ -v --cov=src --cov-report=html --cov-report=term
```

---

## Fichier : `backend\tests\integration\test_full_pipeline.py`

```python
# tests/integration/test_full_pipeline.py
"""Tests d'int√©gration pour le pipeline complet SFD ‚Üí Tests Playwright.

Ce module contient des tests de bout en bout qui v√©rifient le fonctionnement
int√©gr√© de l'application Altiora, de l'analyse d'une Sp√©cification Fonctionnelle
D√©taill√©e (SFD) √† la g√©n√©ration et √† la validation des tests Playwright.
Il couvre diff√©rents sc√©narios, y compris la gestion des erreurs et les
diff√©rents formats d'entr√©e (ex: PDF).
"""

from pathlib import Path

import pytest

# Importation des composants n√©cessaires pour le test.
from src.core.altiora_assistant import AltioraQAAssistant
from src.models.sfd_models import SFDAnalysisRequest


@pytest.fixture(scope="session")
async def full_orchestrator():
    """Fixture Pytest pour initialiser et fermer l'orchestrateur complet de l'application."

    Cette fixture assure que l'orchestrateur est pr√™t avant l'ex√©cution des tests
    et que ses ressources sont lib√©r√©es apr√®s.
    """
    orchestrator = AltioraQAAssistant()
    await orchestrator.initialize()
    yield orchestrator
    await orchestrator.close()


@pytest.fixture
def sample_sfd_content():
    """Fixture fournissant un contenu SFD d√©taill√© pour les tests."

    Ce contenu simule une sp√©cification fonctionnelle pour un module d'authentification,
    incluant des sc√©narios de connexion r√©ussie et √©chou√©e, ainsi que la r√©cup√©ration de mot de passe.
    """
    return """
# Sp√©cification Fonctionnelle - Module Authentification

## 1. Connexion Utilisateur
- **Objectif**: Permettre aux utilisateurs de se connecter de mani√®re s√©curis√©e
- **Acteurs**: Utilisateur authentifi√©, Syst√®me
- **Pr√©conditions**: L'utilisateur a un compte actif

### 1.1 Sc√©nario: Connexion r√©ussie
- **Description**: L'utilisateur se connecte avec des identifiants valides
- **√âtapes**:
  1. L'utilisateur acc√®de √† la page de connexion
  2. Il saisit son email valide
  3. Il saisit son mot de passe valide
  4. Il clique sur "Se connecter"
  5. Il est redirig√© vers le tableau de bord
- **R√©sultat attendu**: Acc√®s autoris√© au tableau de bord

### 1.2 Sc√©nario: √âchec de connexion
- **Description**: L'utilisateur entre des identifiants invalides
- **√âtapes**:
  1. L'utilisateur acc√®de √† la page de connexion
  2. Il saisit des identifiants incorrects
  3. Il clique sur "Se connecter"
- **R√©sultat attendu**: Message d'erreur "Identifiants invalides"

## 2. R√©cup√©ration de mot de passe
- **Sc√©nario**: L'utilisateur oublie son mot de passe
- **√âtapes**:
  1. Cliquer sur "Mot de passe oubli√©"
  2. Saisir l'email
  3. Recevoir le lien de r√©initialisation
  4. R√©initialiser le mot de passe
"""


@pytest.mark.integration
@pytest.mark.asyncio
async def test_sfd_to_test_pipeline_complete(full_orchestrator, tmp_path: Path, sample_sfd_content: str):
    """Test de bout en bout du pipeline complet : SFD ‚Üí Analyse ‚Üí G√©n√©ration de tests Playwright."

    Ce test v√©rifie que l'orchestrateur peut prendre une SFD, l'analyser,
    g√©n√©rer des sc√©narios, puis produire des tests Playwright et un rapport Excel.
    """

    # 1. Pr√©paration du fichier SFD temporaire.
    sfd_path = tmp_path / "complete_sfd.txt"
    sfd_path.write_text(sample_sfd_content)

    # 2. Cr√©ation de la requ√™te d'analyse SFD.
    sfd_request = SFDAnalysisRequest(content=sfd_path.read_text(), extraction_type="complete")

    # 3. Ex√©cution du pipeline complet via l'orchestrateur.
    result = await full_orchestrator.run_full_pipeline(str(sfd_path))

    # 4. Assertions sur les r√©sultats du pipeline.
    assert result["status"] == "completed", "Le pipeline devrait se terminer avec le statut 'completed'."

    # V√©rification des m√©triques de performance et de contenu.
    metrics = result["metrics"]
    assert metrics["scenarios_found"] >= 3, "Au moins 3 sc√©narios devraient √™tre trouv√©s dans la SFD."
    assert metrics["tests_generated"] >= 3, "Au moins 3 tests devraient √™tre g√©n√©r√©s."
    assert metrics["total_time"] > 0, "Le temps total d'ex√©cution devrait √™tre positif."

    # V√©rification du statut de chaque √©tape du pipeline.
    steps = result["steps"]
    assert all(step["status"] == "success" for step in steps.values()), "Toutes les √©tapes du pipeline devraient r√©ussir."

    # V√©rification de l'existence du rapport Excel g√©n√©r√©.
    excel_path = Path(steps["matrix"]["file"])
    assert excel_path.exists(), "Le fichier Excel du rapport devrait exister."

    # V√©rification de la structure et du contenu du fichier Excel.
    import pandas as pd
    df = pd.read_excel(excel_path)
    assert len(df) >= 3, "Le fichier Excel devrait contenir au moins 3 lignes (sc√©narios)."
    assert "ID" in df.columns, "La colonne 'ID' devrait √™tre pr√©sente dans le rapport Excel."
    assert "Test_Code" in df.columns, "La colonne 'Test_Code' devrait √™tre pr√©sente dans le rapport Excel."


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_with_pdf_sfd(full_orchestrator, tmp_path: Path):
    """Test du pipeline avec un fichier PDF comme source SFD (simul√© via contenu texte)."

    Ce test v√©rifie que le pipeline peut traiter un fichier PDF en utilisant
    le service OCR pour extraire le texte avant l'analyse par le LLM.
    """

    # Cr√©ation d'un fichier PDF factice avec du contenu texte.
    # Dans un vrai test d'int√©gration, un vrai fichier PDF serait utilis√©.
    pdf_path = tmp_path / "specification.pdf"
    pdf_path.write_text("Ceci est une simulation de contenu PDF pour les tests. Il contient des sc√©narios.")

    # Ex√©cution du pipeline avec le fichier PDF.
    result = await full_orchestrator.run_full_pipeline(str(pdf_path))

    # V√©rification que l'√©tape d'extraction (OCR) a √©t√© tent√©e.
    assert "steps" in result, "Les √©tapes du pipeline devraient √™tre pr√©sentes dans le r√©sultat."
    assert "extraction" in result["steps"], "L'√©tape d'extraction (OCR) devrait √™tre pr√©sente."
    # Le statut peut √™tre 'success' si l'OCR factice fonctionne, ou 'error' si l'OCR r√©el √©choue.
    assert result["steps"]["extraction"]["status"] in ["success", "error"], "Le statut de l'extraction devrait √™tre succ√®s ou erreur."


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_error_handling(full_orchestrator, tmp_path: Path):
    """Test la gestion d'erreurs dans le pipeline, notamment avec un fichier SFD corrompu."

    Ce test s'assure que le pipeline g√®re correctement les erreurs et retourne
    un statut d'erreur appropri√© avec des d√©tails.
    """

    # Cr√©ation d'un fichier SFD corrompu ou illisible.
    corrupt_path = tmp_path / "corrupt.sfd"
    corrupt_path.write_text("Contenu corrompu ou illisible qui devrait causer une erreur.")

    # Ex√©cution du pipeline avec le fichier corrompu.
    result = await full_orchestrator.run_full_pipeline(str(corrupt_path))

    # V√©rification que le pipeline a √©chou√© et contient des informations sur l'erreur.
    assert result["status"] == "error", "Le pipeline devrait retourner un statut 'error'."
    assert "error" in result, "Le r√©sultat devrait contenir un champ 'error'."
    assert "error_type" in result, "Le r√©sultat devrait contenir un champ 'error_type'."


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_with_different_test_types(full_orchestrator, tmp_path: Path):
    """Test du pipeline avec diff√©rents types de tests (ex: API tests).

    Ce test v√©rifie que le pipeline peut g√©n√©rer des tests pour des types
    sp√©cifiques (comme les tests API) en fonction de la configuration fournie.
    """

    # Cr√©ation d'une SFD d'exemple pour les tests API.
    sfd_path = tmp_path / "api_sfd.txt"
    sfd_path.write_text("""
    # API Sp√©cification
    ## Endpoint /api/login
    - Method: POST
    - Body: {email, password}
    - Response: {token, user_id}
    """)

    # Configuration pour g√©n√©rer des tests API.
    config = {
        "test_types": ["api"],
        "use_page_object": False
    }

    # Ex√©cution du pipeline avec la configuration sp√©cifique.
    result = await full_orchestrator.run_full_pipeline(str(sfd_path), config)

    # V√©rification que le pipeline a r√©ussi.
    assert result["status"] == "completed", "Le pipeline devrait se terminer avec le statut 'completed'."

    # V√©rification que les tests g√©n√©r√©s sont bien des tests API (contiennent des appels HTTP).
    if "generated_tests" in result:
        for test in result["generated_tests"]:
            assert "requests.post" in test["code"] or "client.post" in test["code"], "Le code g√©n√©r√© devrait contenir des appels HTTP pour les tests API."
```

---

## Fichier : `backend\tests\integration\test_performance.py`

```python
# tests/integration/test_performance.py
"""Tests de performance et de charge pour le pipeline Altiora.

Ce module contient des tests d'int√©gration ax√©s sur la performance,
mesurant le temps d'ex√©cution et l'utilisation des ressources (m√©moire)
du pipeline complet SFD ‚Üí Tests Playwright sous diff√©rentes charges.
"""

import pytest
import asyncio
import time
from src.orchestrator import Orchestrator
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Dict, Any
import psutil
import os


@pytest.mark.performance
@pytest.mark.asyncio
async def test_pipeline_performance_metrics(tmp_path: Path):
    """Mesure les m√©triques de performance du pipeline SFD ‚Üí Tests sur plusieurs documents."

    Ce test simule le traitement de plusieurs SFD de tailles diff√©rentes
    et collecte des m√©triques comme la dur√©e d'ex√©cution, le nombre de
    sc√©narios trouv√©s et de tests g√©n√©r√©s.
    """

    # G√©n√®re 10 SFD de diff√©rentes tailles pour simuler une charge vari√©e.
    sfd_contents = [
        f"Sp√©cification {i}: Test de performance avec {i * 100} lignes de contenu " * 5
        for i in range(1, 11)
    ]

    results = []

    async def process_single_sfd(content: str, index: int) -> Dict[str, Any]:
        """Fonction interne pour traiter une seule SFD et collecter ses m√©triques."

        Args:
            content: Le contenu textuel de la SFD.
            index: L'index du document (pour le nom de fichier).

        Returns:
            Un dictionnaire contenant la longueur du contenu, la dur√©e de traitement,
            le nombre de sc√©narios et de tests g√©n√©r√©s.
        """
        sfd_path = tmp_path / f"perf_{index}.txt"
        sfd_path.write_text(content)

        start_time = time.time()

        orchestrator = Orchestrator() # Cr√©e une nouvelle instance d'orchestrateur pour chaque SFD.
        await orchestrator.initialize()

        try:
            result = await orchestrator.run_full_pipeline(str(sfd_path)) # Utilise la m√©thode correcte.
            duration = time.time() - start_time

            return {
                "content_length": len(content),
                "duration": duration,
                "scenarios": result.get("metrics", {}).get("scenarios_found", 0),
                "tests": result.get("metrics", {}).get("tests_generated", 0)
            }
        finally:
            await orchestrator.close()

    # Ex√©cute le traitement de chaque SFD en parall√®le.
    tasks = [process_single_sfd(content, i) for i, content in enumerate(sfd_contents)]
    results = await asyncio.gather(*tasks)

    # Analyse et assertions sur les r√©sultats agr√©g√©s.
    assert len(results) == 10, "Devrait avoir trait√© 10 SFD."

    avg_time = sum(r["duration"] for r in results) / len(results)
    # Assertion sur le temps moyen d'ex√©cution (ajuster la valeur selon les performances attendues).
    assert avg_time < 300, f"Le temps moyen d'ex√©cution ({avg_time:.2f}s) devrait √™tre inf√©rieur √† 300 secondes."

    # V√©rifie que des sc√©narios et des tests ont bien √©t√© g√©n√©r√©s pour chaque SFD.
    for result in results:
        assert result["scenarios"] >= 1, "Chaque SFD devrait g√©n√©rer au moins un sc√©nario."
        assert result["tests"] >= 1, "Chaque SFD devrait g√©n√©rer au moins un test."


@pytest.mark.performance
@pytest.mark.asyncio
async def test_memory_usage(tmp_path: Path):
    """Test la gestion de la m√©moire lors du traitement de gros fichiers SFD."

    Ce test v√©rifie que l'utilisation de la m√©moire par le processus ne d√©passe
    pas une certaine limite lors du traitement d'un grand document.
    """

    # Cr√©e un contenu SFD volumineux pour simuler un gros fichier (~200KB).
    large_content = "Contenu de test pour un grand document SFD. " * 10000

    sfd_path = tmp_path / "large_sfd.txt"
    sfd_path.write_text(large_content)

    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss # M√©moire initiale du processus.

    orchestrator = Orchestrator()
    await orchestrator.initialize()

    try:
        result = await orchestrator.run_full_pipeline(str(sfd_path)) # Utilise la m√©thode correcte.

        final_memory = process.memory_info().rss # M√©moire finale du processus.
        memory_increase = final_memory - initial_memory

        # V√©rifie que l'augmentation de la m√©moire est inf√©rieure √† 1 Go (1_000_000_000 octets).
        assert memory_increase < 1_000_000_000, f"L'augmentation de la m√©moire ({memory_increase / (1024**2):.2f} MB) devrait √™tre inf√©rieure √† 1 Go."
        assert result["status"] == "completed", "Le pipeline devrait se terminer avec le statut 'completed'."

    finally:
        await orchestrator.close()

```

---

## Fichier : `backend\tests\integration\test_services_integration.py`

```python
# tests/integration/test_services_integration.py
"""Tests d'int√©gration entre les diff√©rents microservices d'Altiora.

Ce module contient des tests qui v√©rifient la bonne communication et le
fonctionnement conjoint des services (OCR, Qwen3, StarCoder2, Excel, ALM,
Playwright). Ces tests simulent des flux de travail complexes pour s'assurer
de l'interop√©rabilit√© des composants.
"""

import pytest
import asyncio
import aiohttp
import json
import tempfile
from pathlib import Path


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ocr_to_qwen3_to_starcoder2_flow(wait_for_services):
    """Test le flux d'int√©gration complet : OCR ‚Üí Qwen3 ‚Üí StarCoder2."

    Ce test simule le processus d'analyse d'une SFD (via OCR), son traitement
    par Qwen3 pour extraire les sc√©narios, puis la g√©n√©ration de code de test
    par StarCoder2 √† partir de ces sc√©narios.
    """
    # 1. Pr√©paration d'un fichier SFD temporaire.
    sfd_content = "Test de connexion avec email et mot de passe. Sc√©nario: Connexion r√©ussie."
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(sfd_content)
        sfd_path = Path(f.name)

    try:
        async with aiohttp.ClientSession() as session:
            # 2. Appel au service OCR pour extraire le texte.
            # Note: Le service OCR est mock√© ou utilise une impl√©mentation simple pour les tests.
            ocr_payload = {
                "file_path": str(sfd_path),
                "language": "fra",
                "preprocess": False,
                "cache": False
            }
            async with session.post("http://localhost:8001/extract", json=ocr_payload) as resp:
                resp.raise_for_status()
                ocr_result = await resp.json()
                assert "text" in ocr_result
                extracted_text = ocr_result["text"]

            # 3. Appel au mod√®le Qwen3 pour l'analyse SFD.
            qwen3_payload = {
                "model": "qwen3-sfd-analyzer",
                "prompt": f"Analyze the following SFD and extract test scenarios in JSON format: {extracted_text}",
                "stream": False,
                "options": {"num_predict": 500, "temperature": 0.1}
            }
            async with session.post("http://localhost:11434/api/generate", json=qwen3_payload) as resp:
                resp.raise_for_status()
                qwen3_raw_response = await resp.json()
                # Extrait la partie JSON de la r√©ponse de Qwen3.
                qwen3_response_text = qwen3_raw_response.get("response", "{}")
                try:
                    qwen3_parsed_response = json.loads(qwen3_response_text)
                except json.JSONDecodeError:
                    pytest.fail(f"La r√©ponse de Qwen3 n'est pas un JSON valide : {qwen3_response_text}")
                
                assert "scenarios" in qwen3_parsed_response, "La r√©ponse de Qwen3 devrait contenir des sc√©narios."
                assert len(qwen3_parsed_response["scenarios"]) > 0, "Qwen3 devrait extraire au moins un sc√©nario."
                scenario_for_starcoder = qwen3_parsed_response["scenarios"][0]

            # 4. Appel au mod√®le StarCoder2 pour la g√©n√©ration de code de test.
            starcoder2_payload = {
                "model": "starcoder2-playwright",
                "prompt": f"Generate a Playwright test in Python for the following scenario: {json.dumps(scenario_for_starcoder)}",
                "stream": False,
                "options": {"num_predict": 500, "temperature": 0.1}
            }
            async with session.post("http://localhost:11434/api/generate", json=starcoder2_payload) as resp:
                resp.raise_for_status()
                starcoder2_raw_response = await resp.json()
                generated_code = starcoder2_raw_response.get("response", "")

                assert "def test_" in generated_code, "Le code g√©n√©r√© par StarCoder2 devrait contenir une fonction de test."
                assert "page.goto" in generated_code, "Le code g√©n√©r√© devrait contenir une navigation Playwright."

    finally:
        sfd_path.unlink(missing_ok=True)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_excel_alm_integration(wait_for_services):
    """Test le flux d'int√©gration : G√©n√©ration Excel ‚Üí Importation ALM."

    Ce test v√©rifie que le service Excel peut g√©n√©rer une matrice de test
    et que le service ALM peut ensuite importer ces donn√©es (simul√©es).
    """

    test_data_for_excel = [
        {
            "id": "CU01_SB01_CP001_login_success",
            "description": "Test connexion r√©ussie",
            "type": "CP",
        },
        {
            "id": "CU01_SB01_CE001_invalid_login",
            "description": "Test √©chec de connexion",
            "type": "CE",
        }
    ]

    async with aiohttp.ClientSession() as session:
        # 1. Appel au service Excel pour cr√©er une matrice de tests.
        excel_payload = {
            "filename": "integration_test_matrix.xlsx",
            "test_cases": test_data_for_excel
        }
        async with session.post("http://localhost:8003/create-test-matrix", json=excel_payload) as resp:
            resp.raise_for_status()
            excel_response_content = await resp.read() # Le service Excel retourne le fichier binaire.
            assert len(excel_response_content) > 0, "Le service Excel devrait retourner un fichier non vide."

        # 2. Appel au service ALM pour importer un √©l√©ment de travail (simul√©).
        # Note: Le service ALM est mock√© pour les tests d'int√©gration.
        alm_payload = {
            "title": "Import de cas de test depuis Excel",
            "description": "Cas de test g√©n√©r√©s automatiquement et import√©s via le service Excel.",
            "item_type": "Task"
        }
        async with session.post("http://localhost:8002/work-items", json=alm_payload) as resp:
            resp.raise_for_status()
            alm_result = await resp.json()
            assert alm_result["success"] is True, "L'importation ALM devrait r√©ussir."
            assert "work_item" in alm_result, "Le r√©sultat ALM devrait contenir les d√©tails de l'√©l√©ment de travail."


@pytest.mark.integration
@pytest.mark.asyncio
async def test_playwright_execution(wait_for_services):
    """Test l'ex√©cution r√©elle de tests Playwright via le service d√©di√©."

    Ce test envoie un script Playwright simple au service `playwright_runner`
    et v√©rifie que l'ex√©cution se d√©roule correctement.
    """

    test_code_to_execute = '''
import pytest
from playwright.async_api import Page, expect

@pytest.mark.asyncio
async def test_example_page_load(page: Page):
    """Un test simple pour charger une page et v√©rifier son titre."""
    await page.goto("https://www.google.com")
    await expect(page).to_have_title(/Google/)
'''

    playwright_payload = {
        "tests": [{"code": test_code_to_execute, "test_name": "test_google_load"}],
        "config": {
            "browser": "chromium",
            "headed": False, # Ex√©cution en mode headless.
            "timeout": 30000
        }
    }

    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:8004/execute", json=playwright_payload) as resp:
            resp.raise_for_status()
            result = await resp.json()
            assert result["status"] == "completed", f"L'ex√©cution Playwright devrait √™tre compl√©t√©e, mais est {result.get('status')}. Erreur: {result.get('error')}"
            assert result["passed"] == 1, "Le test Playwright devrait r√©ussir."
            assert result["failed"] == 0, "Aucun test Playwright ne devrait √©chouer."
            assert len(result["results"]) == 1, "Un seul r√©sultat de test devrait √™tre retourn√©."
            assert result["results"][0]["status"] == "passed", "Le statut du test individuel devrait √™tre 'passed'."

```

---

## Fichier : `backend\tests\integration\__init__.py`

```python

```

---

## Fichier : `backend\tests\performance\config.yaml`

```yaml
# tests/performance/config.yaml
performance_tests:
  cpu_limits:
    max_percent: 85
    max_temperature: 80
    max_memory_gb: 25

  thresholds:
    response_time_ms: 30000
    throughput_req_sec: 0.5
    error_rate_percent: 5
    memory_efficiency: 0.8

  test_scenarios:
    - name: "light_load"
      concurrent_requests: 5
      duration_seconds: 60
    - name: "medium_load"
      concurrent_requests: 15
      duration_seconds: 120
    - name: "heavy_load"
      concurrent_requests: 30
      duration_seconds: 300
    - name: "stress_test"
      concurrent_requests: 50
      duration_seconds: 600
```

---

## Fichier : `backend\tests\performance\test_load_testing.py`

```python
# tests/performance/test_load_testing.py
"""
Tests de charge CPU pour Altiora
Optimis√©s pour Intel i5-13500H (14 cores, 32GB RAM)
"""

import asyncio
import logging
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, List

import aiohttp
import psutil
import pytest

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """M√©triques de performance CPU"""
    cpu_usage: List[float]
    memory_usage: List[float]
    response_times: List[float]
    throughput: float
    error_rate: float
    concurrent_requests: int


class CPULoadTester:
    """Testeur de charge CPU sp√©cialis√© pour Altiora"""

    def __init__(self, target_cpu: int = 80, max_memory_gb: float = 28.0):
        self.target_cpu = target_cpu
        self.max_memory_gb = max_memory_gb
        self.cpu_cores = psutil.cpu_count(logical=False)
        self.p_cores = 6  # Performance cores i5-13500H
        self.e_cores = 8  # Efficiency cores i5-13500H

    def get_system_metrics(self) -> Dict[str, Any]:
        """R√©cup√®re les m√©triques syst√®me actuelles"""
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        memory = psutil.virtual_memory()

        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_gb": memory.used / (1024 ** 3),
            "temperature": self._get_cpu_temperature(),
            "load_average": psutil.getloadavg()
        }

    @staticmethod
    def _get_cpu_temperature() -> float:
        """R√©cup√®re la temp√©rature CPU (Linux)"""
        try:
            with open('/sys/class/thermal/thermal_zone0/temp', 'r') as f:
                return float(f.read().strip()) / 1000
        except (IOError, OSError) as e:
            logger.warning(f"Could not read CPU temperature: {e}")
            return 0.0

    @staticmethod
    async def stress_test_qwen3(num_concurrent: int = 10):
        """Test de charge sur Qwen3 avec analyses parall√®les"""

        test_payloads = [
            f"Analyse ce sc√©nario de test {i}: connexion utilisateur avec validation email"
            for i in range(num_concurrent)
        ]

        metrics = {
            "start_time": time.time(),
            "total_requests": num_concurrent,
            "successful": 0,
            "failed": 0,
            "response_times": []
        }

        async def single_request(payload: str):
            async with aiohttp.ClientSession() as session:
                start = time.time()
                try:
                    async with session.post(
                            "http://localhost:11434/api/generate",
                            json={
                                "model": "qwen3:32b-q4_K_M",
                                "prompt": payload,
                                "stream": False,
                                "options": {
                                    "num_ctx": 8192,
                                    "num_predict": 512,
                                    "temperature": 0.7
                                }
                            },
                            timeout=aiohttp.ClientTimeout(total=300)
                    ) as resp:
                        duration = time.time() - start
                        if resp.status == 200:
                            metrics["successful"] += 1
                        else:
                            metrics["failed"] += 1
                        metrics["response_times"].append(duration)
                        return duration
                except Exception as e:
                    metrics["failed"] += 1
                    logger.error(f"Erreur requ√™te: {e}")
                    return 0

        # Ex√©cution parall√®le
        tasks = [single_request(payload) for payload in test_payloads]
        await asyncio.gather(*tasks)

        metrics["total_time"] = time.time() - metrics["start_time"]
        metrics["throughput"] = metrics["successful"] / metrics["total_time"]

        return metrics

    @staticmethod
    async def memory_stress_test(data_size_mb: int = 100):
        """Test de stress m√©moire avec gros volumes de donn√©es"""

        # G√©n√©rer des donn√©es volumineuses
        large_sfd = "Contenu de sp√©cification " * 10000  # ~200KB

        # Cr√©er 100 fichiers de test
        test_files = []
        for i in range(100):
            file_path = Path(f"/tmp/large_sfd_{i}.txt")
            file_path.write_text(large_sfd * 50)  # ~10MB par fichier
            test_files.append(file_path)

        metrics = {
            "files_processed": 0,
            "total_size_mb": 0,
            "peak_memory_mb": 0,
            "processing_times": []
        }

        # Monitorer la m√©moire
        process = psutil.Process()
        initial_memory = process.memory_info().rss / (1024 ** 3)

        for file_path in test_files[:10]:  # Limiter pour les tests
            start_time = time.time()
            start_memory = process.memory_info().rss / (1024 ** 3)

            # Simuler le traitement
            content = file_path.read_text()
            metrics["total_size_mb"] += len(content) / (1024 ** 2)

            # Lib√©ration m√©moire
            del content

            end_memory = process.memory_info().rss / (1024 ** 3)
            metrics["peak_memory_mb"] = max(metrics["peak_memory_mb"], end_memory)
            metrics["processing_times"].append(time.time() - start_time)
            metrics["files_processed"] += 1

            file_path.unlink()

        metrics["memory_increase_gb"] = metrics["peak_memory_mb"] - initial_memory
        metrics["memory_efficiency"] = metrics["total_size_mb"] / metrics["memory_increase_gb"]

        return metrics

    @staticmethod
    async def concurrent_playwright_tests(num_tests: int = 50):
        """Test avec g√©n√©ration parall√®le de tests Playwright"""

        test_scenarios = [
            {
                "titre": f"Test {i}",
                "description": f"Test de connexion {i}",
                "etapes": ["Naviguer", "Saisir", "Valider"]
            }
            for i in range(num_tests)
        ]

        metrics = {
            "tests_generated": 0,
            "total_time": 0,
            "avg_generation_time": 0,
            "memory_usage": []
        }

        start_time = time.time()

        async def generate_single_test(scenario: dict):
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.post(
                            "http://localhost:11434/api/generate",
                            json={
                                "model": "starcoder2:15b-q8_0",
                                "prompt": f"Generate Playwright test for: {scenario['titre']}",
                                "stream": False,
                                "options": {"num_predict": 1000}
                            },
                            timeout=300
                    ) as resp:
                        if resp.status == 200:
                            return await resp.json()
                except Exception as e:
                    logger.error(f"Erreur g√©n√©ration test: {e}")
                    return None

        # Ex√©cution par lots pour √©viter la surcharge
        batch_size = min(10, num_tests)
        for i in range(0, num_tests, batch_size):
            batch = test_scenarios[i:i + batch_size]
            tasks = [generate_single_test(scenario) for scenario in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            metrics["tests_generated"] += sum(1 for r in results if r and not isinstance(r, Exception))

            # Pause pour laisser respirer le CPU
            await asyncio.sleep(1)

        metrics["total_time"] = time.time() - start_time
        metrics["avg_generation_time"] = metrics["total_time"] / metrics["tests_generated"]

        return metrics


@pytest.mark.performance
@pytest.mark.asyncio
async def test_cpu_load_80_percent():
    """Test de charge √† 80% CPU"""

    tester = CPULoadTester(target_cpu=80)

    # Test Qwen3 sous charge
    metrics = await tester.stress_test_qwen3(num_concurrent=15)

    assert metrics["successful"] >= 12  # 80% de succ√®s
    assert metrics["throughput"] > 0.5  # Au moins 0.5 req/sec

    # V√©rifier que le CPU ne d√©passe pas 90%
    cpu_percent = psutil.cpu_percent(interval=5)
    assert cpu_percent < 90, f"CPU trop √©lev√©: {cpu_percent}%"


@pytest.mark.performance
@pytest.mark.asyncio
async def test_memory_efficiency():
    """Test d'efficacit√© m√©moire"""

    tester = CPULoadTester()

    metrics = await tester.memory_stress_test(data_size_mb=50)

    # Ratio d'efficacit√© m√©moire
    assert metrics["memory_efficiency"] > 0.8  # 80% d'efficacit√©
    assert metrics["memory_increase_gb"] < 2  # Moins de 2GB d'augmentation


@pytest.mark.performance
@pytest.mark.asyncio
async def test_concurrent_test_generation():
    """Test de g√©n√©ration parall√®le de tests"""

    tester = CPULoadTester()

    metrics = await tester.concurrent_playwright_tests(num_tests=20)

    assert metrics["tests_generated"] >= 18  # 90% de succ√®s
    assert metrics["avg_generation_time"] < 30  # Moins de 30s par test


@pytest.mark.performance
@pytest.mark.asyncio
async def test_error_handling():
    """Test de gestion des erreurs"""

    tester = CPULoadTester()

    # Simuler une erreur de connexion
    async def failing_request():
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                        "http://localhost:11434/api/generate",
                        json={"model": "invalid_model", "prompt": "Test"},
                        timeout=300
                ) as resp:
                    return await resp.json()
            except Exception as e:
                logger.error(f"Erreur requ√™te: {e}")
                return None

    result = await failing_request()

    assert result is None  # V√©rifier que l'erreur est g√©r√©e correctement

```

---

## Fichier : `backend\tests\performance\test_pipeline_load.py`

```python
# tests/performance/test_pipeline_load.py
"""Tests de charge complets pour le pipeline Altiora.

Ce module contient des tests de performance et de scalabilit√© pour le pipeline
complet de l'application Altiora (SFD ‚Üí Analyse ‚Üí G√©n√©ration de tests).
Il mesure l'utilisation des ressources (CPU, m√©moire) sous diff√©rentes charges
et v√©rifie la r√©silience du syst√®me.
"""

import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List, Optional

import psutil
import pytest
import tempfile
import redis.asyncio as redis

from src.orchestrator import Orchestrator


class PipelineLoadTester:
    """Testeur de charge pour le pipeline complet d'Altiora."""

    def __init__(self):
        """Initialise le testeur de charge avec des limites de ressources par d√©faut."""
        self.cpu_limit = 85  # Limite d'utilisation CPU en pourcentage.
        self.memory_limit = 25  # Limite d'utilisation m√©moire en Go.
        self.process = psutil.Process() # R√©f√©rence au processus courant pour la surveillance.

    def monitor_resources(self) -> Dict[str, float]:
        """Surveille et retourne les m√©triques d'utilisation des ressources syst√®me."

        Returns:
            Un dictionnaire contenant le pourcentage d'utilisation CPU, le pourcentage
            et la quantit√© de m√©moire utilis√©e (en Go), la temp√©rature CPU (si disponible),
            et la m√©moire utilis√©e par le processus courant.
        """
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()

        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_gb": memory.used / (1024 ** 3),
            "temperature": self._get_cpu_temperature(),
            "process_memory": self.process.memory_info().rss / (1024 ** 3)
        }

    @staticmethod
    def _get_cpu_temperature() -> float:
        """R√©cup√®re la temp√©rature CPU (fonction factice ou d√©pendante de `psutil`)."

        Returns:
            La temp√©rature CPU en degr√©s Celsius, ou 0.0 si non disponible.
        """
        try:
            temps = psutil.sensors_temperatures()
            if 'coretemp' in temps:
                # Retourne la temp√©rature maximale des c≈ìurs.
                return max([t.current for t in temps['coretemp']])
            return 0.0
        except AttributeError:
            # psutil.sensors_temperatures() n'est pas disponible sur tous les syst√®mes.
            return 0.0
        except Exception as e:
            logging.warning(f"Impossible de r√©cup√©rer la temp√©rature CPU : {e}")
            return 0.0

    def should_stop_load(self) -> bool:
        """D√©termine si la g√©n√©ration de charge doit √™tre arr√™t√©e en fonction des limites de ressources."

        Returns:
            True si une limite de ressource est d√©pass√©e, False sinon.
        """
        metrics = self.monitor_resources()
        return (
                metrics["cpu_percent"] > self.cpu_limit or
                metrics["memory_gb"] > self.memory_limit or
                metrics["temperature"] > 80 # Temp√©rature critique.
        )

    async def load_test_full_pipeline(self, num_concurrent: int = 20) -> Dict[str, Any]:
        """Ex√©cute un test de charge complet sur le pipeline Altiora."

        Args:
            num_concurrent: Le nombre de requ√™tes de pipeline √† lancer en parall√®le.

        Returns:
            Un dictionnaire d'analyse des r√©sultats de charge.
        """
        # Cr√©er des SFD de test √† partir de templates.
        sfd_templates = [
            "Sp√©cification login: email, password, validation",
            "Sp√©cification API: endpoints, m√©thodes, authentification",
            "Sp√©cification UI: formulaires, boutons, validations"
        ]

        results = []
        orchestrator = Orchestrator() # Cr√©e une instance de l'orchestrateur.
        await orchestrator.initialize()

        try:
            # Pr√©pare les t√¢ches individuelles du pipeline.
            tasks = []
            for i in range(num_concurrent):
                sfd_content = f"{sfd_templates[i % len(sfd_templates)]} - test {i}"
                task = self._single_pipeline_test(orchestrator, sfd_content, i)
                tasks.append(task)

            # Ex√©cute les t√¢ches par lots avec une limitation de charge.
            batch_size = 5
            for i in range(0, num_concurrent, batch_size):
                if self.should_stop_load():
                    logger.warning("Arr√™t de la g√©n√©ration de charge en raison des limites syst√®me atteintes.")
                    break

                batch = tasks[i:i + batch_size]
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
                results.extend(batch_results)

                # Petite pause pour laisser le syst√®me respirer entre les lots.
                await asyncio.sleep(2)

        finally:
            await orchestrator.close()

        return self._analyze_results(results)

    async def _single_pipeline_test(self, orchestrator: Orchestrator, sfd_content: str, index: int) -> Dict[str, Any]:
        """Ex√©cute un seul test du pipeline et collecte ses m√©triques."

        Args:
            orchestrator: L'instance de l'orchestrateur √† utiliser.
            sfd_content: Le contenu de la SFD pour ce test.
            index: L'index du test (pour le nom de fichier temporaire).

        Returns:
            Un dictionnaire contenant les r√©sultats de l'ex√©cution du test.
        """
        start_time = time.time()
        start_resources = self.monitor_resources()

        # Cr√©e un fichier SFD temporaire pour le test.
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write(sfd_content)
            sfd_path = Path(f.name)

        try:
            result = await orchestrator.run_full_pipeline(str(sfd_path))

            end_time = time.time()
            end_resources = self.monitor_resources()

            return {
                "index": index,
                "success": result["status"] == "completed",
                "duration": end_time - start_time,
                "scenarios": result.get("metrics", {}).get("scenarios_found", 0),
                "tests_generated": result.get("metrics", {}).get("tests_generated", 0),
                "cpu_usage": (start_resources["cpu_percent"] + end_resources["cpu_percent"]) / 2,
                "memory_usage": end_resources["memory_gb"],
                "error": None if result["status"] == "completed" else result.get("error")
            }

        except Exception as e:
            return {
                "index": index,
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }
        finally:
            sfd_path.unlink(missing_ok=True) # S'assure que le fichier temporaire est supprim√©.

    @staticmethod
    def _analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyse et agr√®ge les r√©sultats des tests de charge."

        Args:
            results: Une liste de dictionnaires, chaque dictionnaire √©tant le r√©sultat d'un test unique.

        Returns:
            Un dictionnaire r√©capitulatif des m√©triques de performance.
        """
        successful = [r for r in results if r.get("success", False)]
        failed = [r for r in results if not r.get("success", False)]

        total_tests = len(results)
        successful_count = len(successful)
        failed_count = len(failed)

        avg_duration = sum(r["duration"] for r in successful) / successful_count if successful_count > 0 else 0
        avg_scenarios = sum(r["scenarios"] for r in successful) / successful_count if successful_count > 0 else 0
        avg_tests = sum(r["tests_generated"] for r in successful) / successful_count if successful_count > 0 else 0

        # Calcul du d√©bit (tests r√©ussis par seconde).
        total_successful_duration = sum(r["duration"] for r in successful)
        throughput = successful_count / total_successful_duration if total_successful_duration > 0 else 0

        return {
            "total_tests": total_tests,
            "successful": successful_count,
            "failed": failed_count,
            "success_rate": (successful_count / total_tests) * 100 if total_tests > 0 else 0,
            "avg_duration": avg_duration,
            "avg_scenarios": avg_scenarios,
            "avg_tests": avg_tests,
            "throughput": throughput,
            "error_rate": (failed_count / total_tests) * 100 if total_tests > 0 else 0
        }


@pytest.mark.performance
@pytest.mark.asyncio
async def test_cpu_load_pipeline():
    """Test de charge CPU avec le pipeline complet."

    V√©rifie que le pipeline peut g√©rer une charge CPU √©lev√©e sans d√©passer les limites.
    """

    tester = PipelineLoadTester()

    # Ex√©cute le test de charge avec 10 requ√™tes concurrentes.
    metrics = await tester.load_test_full_pipeline(num_concurrent=10)

    # Assertions sur les m√©triques de performance.
    assert metrics["success_rate"] > 80, "Le taux de succ√®s devrait √™tre sup√©rieur √† 80%."
    assert metrics["avg_duration"] < 120, "La dur√©e moyenne par test devrait √™tre inf√©rieure √† 120 secondes."
    assert metrics["throughput"] > 0.05, "Le d√©bit devrait √™tre sup√©rieur √† 0.05 test/seconde."

    # V√©rification des ressources syst√®me apr√®s le test.
    final_metrics = tester.monitor_resources()
    assert final_metrics["cpu_percent"] < 95, "L'utilisation CPU devrait rester sous 95%."
    assert final_metrics["memory_gb"] < 28, "L'utilisation m√©moire devrait rester sous 28 Go."


@pytest.mark.performance
@pytest.mark.asyncio
async def test_memory_efficiency_pipeline():
    """Test de l'efficacit√© m√©moire du pipeline avec de gros fichiers SFD."

    V√©rifie que le pipeline g√®re efficacement la m√©moire lors du traitement
    de documents volumineux, √©vitant les fuites de m√©moire.
    """

    tester = PipelineLoadTester()

    # Cr√©e un contenu SFD tr√®s volumineux (~1 Mo) pour le test.
    large_sfd_content = "Sp√©cification d√©taill√©e " * 50000

    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(large_sfd_content)
        sfd_path = Path(f.name)

    orchestrator = Orchestrator()
    await orchestrator.initialize()

    try:
        # Ex√©cute le pipeline avec le gros fichier SFD.
        result = await orchestrator.run_full_pipeline(str(sfd_path))

        assert result["status"] == "completed", "Le pipeline devrait se terminer avec succ√®s."

        # V√©rifie l'efficacit√© m√©moire apr√®s le traitement.
        memory_metrics = tester.monitor_resources()
        # La limite de 20 Go est un exemple, √† ajuster selon le mod√®le et les ressources.
        assert memory_metrics["memory_gb"] < 20, f"L'utilisation m√©moire ({memory_metrics['memory_gb']:.2f} Go) devrait √™tre inf√©rieure √† 20 Go."

    finally:
        await orchestrator.close()
        sfd_path.unlink(missing_ok=True)


@pytest.mark.performance
@pytest.mark.asyncio
async def test_concurrent_redis_operations(redis_client: redis.Redis):
    """Test la performance des op√©rations Redis sous forte concurrence."

    V√©rifie que Redis peut g√©rer un grand nombre d'op√©rations de lecture/√©criture
    concurrentes de mani√®re efficace.
    """

    try:
        num_operations = 1000 # Nombre d'op√©rations de lecture/√©criture √† effectuer.

        write_tasks = []
        read_tasks = []

        start_time = time.time()

        # Ex√©cute des op√©rations d'√©criture concurrentes.
        for i in range(num_operations):
            task = redis_client.setex(f"perf_test_{i}", 60, f"data_{i}")
            write_tasks.append(task)

        await asyncio.gather(*write_tasks)
        write_time = time.time() - start_time
        logging.info(f"Temps pour {num_operations} √©critures Redis : {write_time:.2f}s")

        # Ex√©cute des op√©rations de lecture concurrentes.
        start_time = time.time()
        for i in range(num_operations):
            task = redis_client.get(f"perf_test_{i}")
            read_tasks.append(task)

        results = await asyncio.gather(*read_tasks)
        read_time = time.time() - start_time
        logging.info(f"Temps pour {num_operations} lectures Redis : {read_time:.2f}s")

        # Nettoyage des cl√©s cr√©√©es pendant le test.
        keys_to_delete = [f"perf_test_{i}" for i in range(num_operations)]
        if keys_to_delete:
            await redis_client.delete(*keys_to_delete)

        # Assertions sur les temps d'ex√©cution et le nombre de succ√®s.
        assert write_time < 5, f"Les {num_operations} √©critures Redis devraient prendre moins de 5 secondes."
        assert read_time < 3, f"Les {num_operations} lectures Redis devraient prendre moins de 3 secondes."
        assert len([r for r in results if r is not None]) > 900, "Au moins 90% des lectures devraient r√©ussir."

    finally:
        # S'assure que le client Redis est ferm√©.
        await redis_client.aclose()
```

---

## Fichier : `backend\tests\performance\test_redis_performance.py`

```python
# tests/performance/test_redis_performance.py
"""Tests de performance pour le cache Redis.

Ce module contient des tests de performance pour √©valuer le d√©bit (throughput)
et l'efficacit√© de la gestion des TTL (Time-To-Live) du cache Redis.
Il simule des op√©rations de lecture et d'√©criture concurrentes pour mesurer
les performances sous charge.
"""

import asyncio
import redis.asyncio as redis
import time
import json
import pytest
from typing import List, Dict, Any


class RedisPerformanceTester:
    """Testeur de performance pour les op√©rations Redis."""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        """Initialise le testeur avec l'URL de connexion Redis."

        Args:
            redis_url: L'URL de connexion au serveur Redis.
        """
        self.redis_url = redis_url

    async def test_cache_throughput(self, num_operations: int = 10000) -> Dict[str, Any]:
        """Teste le d√©bit du cache Redis en effectuant un grand nombre d'op√©rations de lecture/√©criture."

        Args:
            num_operations: Le nombre total d'op√©rations de lecture et d'√©criture √† effectuer.

        Returns:
            Un dictionnaire contenant les m√©triques de performance (d√©bit d'√©criture/lecture, erreurs, m√©moire).
        """
        client = await redis.from_url(self.redis_url, decode_responses=True)

        metrics = {
            "total_operations": num_operations,
            "writes_per_second": 0.0,
            "reads_per_second": 0.0,
            "error_rate": 0.0,
            "memory_usage": "0"
        }

        test_data = {"test": "data", "timestamp": time.time()}

        # --- Test d'√©criture ---
        start_time = time.time()
        write_tasks = []
        for i in range(num_operations):
            task = client.setex(f"test_key_{i}", 3600, json.dumps(test_data))
            write_tasks.append(task)

        write_results = await asyncio.gather(*write_tasks, return_exceptions=True)
        write_duration = time.time() - start_time

        successful_writes = sum(1 for r in write_results if r is True)
        metrics["writes_per_second"] = successful_writes / write_duration if write_duration > 0 else 0
        metrics["error_rate"] += (num_operations - successful_writes) / num_operations

        # --- Test de lecture ---
        start_time = time.time()
        read_tasks = []
        for i in range(num_operations):
            task = client.get(f"test_key_{i}")
            read_tasks.append(task)

        read_results = await asyncio.gather(*read_tasks, return_exceptions=True)
        read_duration = time.time() - start_time

        successful_reads = sum(1 for r in read_results if r is not None and not isinstance(r, Exception))
        metrics["reads_per_second"] = successful_reads / read_duration if read_duration > 0 else 0
        metrics["error_rate"] += (num_operations - successful_reads) / num_operations

        # --- Utilisation m√©moire --- 
        try:
            info = await client.info("memory")
            metrics["memory_usage"] = info.get("used_memory_human", "0")
        except Exception as e:
            logging.warning(f"Impossible de r√©cup√©rer les infos m√©moire de Redis : {e}")
            metrics["memory_usage"] = "N/A"

        await client.aclose()

        return metrics

    async def test_cache_ttl_performance(self) -> Dict[str, int]:
        """Teste la performance de la gestion des TTL et l'expiration des cl√©s dans Redis."

        Returns:
            Un dictionnaire contenant le nombre de cl√©s initiales, restantes et expir√©es.
        """
        client = await redis.from_url(self.redis_url)

        # Cr√©e 1000 cl√©s avec des TTL diff√©rents (de 1 √† 10 secondes).
        tasks = []
        for i in range(1000):
            ttl = 1 + (i % 10)  # TTL de 1 √† 10 secondes.
            task = client.setex(f"ttl_test_{i}", ttl, f"data_{i}")
            tasks.append(task)

        await asyncio.gather(*tasks)

        # Attend que toutes les cl√©s avec un TTL court expirent.
        await asyncio.sleep(11) # Attend 11 secondes pour s'assurer que toutes les cl√©s (max TTL 10s) expirent.

        # Compte les cl√©s restantes.
        remaining = await client.keys("ttl_test_*")

        await client.aclose()

        return {
            "initial_keys": 1000,
            "remaining_keys": len(remaining),
            "expired_keys": 1000 - len(remaining)
        }


@pytest.mark.performance
@pytest.mark.asyncio
async def test_redis_cache_performance(wait_for_services):
    """Test de performance global du cache Redis."

    Ce test combine les v√©rifications de d√©bit et de TTL pour une √©valuation compl√®te.
    """

    tester = RedisPerformanceTester()

    # --- Test de d√©bit ---
    throughput_metrics = await tester.test_cache_throughput(1000) # Ex√©cute 1000 op√©rations.
    logging.info(f"M√©triques de d√©bit Redis : {throughput_metrics}")

    assert throughput_metrics["writes_per_second"] > 500, "Le d√©bit d'√©criture devrait √™tre sup√©rieur √† 500 ops/s."
    assert throughput_metrics["reads_per_second"] > 1000, "Le d√©bit de lecture devrait √™tre sup√©rieur √† 1000 ops/s."

    # --- Test TTL ---
    ttl_metrics = await tester.test_cache_ttl_performance()
    logging.info(f"M√©triques TTL Redis : {ttl_metrics}")

    assert ttl_metrics["expired_keys"] >= 900, "Au moins 90% des cl√©s devraient avoir expir√©."

```

---

## Fichier : `backend\tests\regression\regression_config.yaml`

```yaml
# Configuration des tests de r√©gression Altiora
thresholds:
  max_time_increase: 1.2      # 20% maximum time increase
  min_scenarios: 1            # Minimum scenarios to extract
  min_tests_generated: 1      # Minimum tests to generate
  code_similarity: 0.8        # 80% minimum code similarity
  max_memory_increase: 1.5    # 50% memory increase allowed

models:
  qwen3:
    model_name: "qwen3-sfd-analyzer"
    timeout: 120
    test_cases:
      - scenario_extraction
      - test_matrix_generation
      - priority_detection
  starcoder2:
    model_name: "starcoder2-playwright"
    timeout: 180
    test_cases:
      - code_generation
      - syntax_validity
      - playwright_compliance

services:
  health_check: true
  response_time: 30
  endpoints:
    - ocr
    - alm
    - excel
    - playwright

performance:
  measure_memory: true
  measure_cpu: true
  iterations: 3

update_baselines: false  # Set to true to update reference files
```

---

## Fichier : `backend\tests\regression\run_regression.py`

```python
#!/usr/bin/env python3
"""Script CLI pour lancer les tests de r√©gression de l'application Altiora.

Ce script permet d'ex√©cuter la suite compl√®te des tests de r√©gression.
Il peut √™tre configur√© pour mettre √† jour les baselines (r√©f√©rences) des tests,
ainsi que pour g√©n√©rer un rapport d√©taill√© des r√©sultats. Il s'assure √©galement
que les r√©pertoires n√©cessaires et les donn√©es d'exemple sont en place.

Utilisation:
    python run_regression.py [--update-baselines] [--report] [--verbose]
"""

import argparse
import asyncio
import logging
import json
from pathlib import Path

from tests.regression.test_regression_suite import RegressionSuite # Assurez-vous que ce module existe et est correct.

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def main():
    """Fonction principale pour ex√©cuter la suite de tests de r√©gression."

    Parse les arguments de la ligne de commande pour configurer l'ex√©cution des tests.
    """
    parser = argparse.ArgumentParser(description="Ex√©cute les tests de r√©gression Altiora.")
    parser.add_argument("--update-baselines", action="store_true",
                        help="Met √† jour les fichiers de r√©f√©rence (baselines) avec les r√©sultats actuels.")
    parser.add_argument("--report", action="store_true",
                        help="G√©n√®re un rapport HTML d√©taill√© des r√©sultats de r√©gression.")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="Active la sortie verbeuse pour un d√©bogage plus d√©taill√©.")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Cr√©e les r√©pertoires n√©cessaires pour les baselines et les r√©sultats des tests.
    Path("tests/regression/baselines").mkdir(parents=True, exist_ok=True)
    Path("tests/regression/results").mkdir(parents=True, exist_ok=True)
    Path("tests/regression/fixtures/sample_sfd").mkdir(parents=True, exist_ok=True)

    # Cr√©e les fichiers de test exemple s'ils n'existent pas, pour assurer un environnement de test fonctionnel.
    await _create_sample_fixtures()

    # Initialise la suite de r√©gression.
    suite = RegressionSuite()
    suite.config["update_baselines"] = args.update_baselines
    suite.config["generate_report"] = args.report # Passe l'option de rapport √† la suite.

    logger.info("üöÄ D√©marrage des tests de r√©gression...")
    results = await suite.run_full_regression()

    # Affichage du r√©sum√© des r√©sultats des tests de r√©gression.
    print("\n" + "=" * 60)
    logger.info("üìä R√âSUM√â DES TESTS DE R√âGRESSION")
    print("=" * 60)
    logger.info(f"Tests totaux : {len(results['tests'])}")
    logger.info(f"‚úÖ R√©ussis : {results['summary']['passed']}")
    logger.info(f"‚ùå √âchou√©s : {results['summary']['failed']}")
    logger.info(f"üÜï Nouveaux : {results['summary']['new']}")

    if results["summary"]["failed"] > 0:
        logger.info("\n‚ö†Ô∏è  Certains tests ont √©chou√© - veuillez consulter le rapport d√©taill√© pour plus d'informations.")
        exit(1) # Quitte avec un code d'erreur si des tests ont √©chou√©.
    else:
        logger.info("\nüéâ Tous les tests de r√©gression ont r√©ussi !")


async def _create_sample_fixtures():
    """Cr√©e des fichiers de fixtures de test exemple pour la suite de r√©gression."

    Ces fichiers sont utilis√©s pour simuler des entr√©es pour les tests
    (ex: sp√©cifications SFD, cas de test Qwen3/StarCoder2).
    """
    fixtures_dir = Path("tests/regression/fixtures")
    fixtures_dir.mkdir(parents=True, exist_ok=True)

    # Fichiers SFD d'exemple.
    sample_sfd_dir = fixtures_dir / "sample_sfd"
    sample_sfd_dir.mkdir(parents=True, exist_ok=True)

    login_spec_path = sample_sfd_dir / "login_spec.txt"
    if not login_spec_path.exists():
        login_spec_path.write_text("""
Sp√©cification Fonctionnelle - Module de Connexion

Objectif: Permettre aux utilisateurs de s'authentifier sur la plateforme

Sc√©nario 1: Connexion r√©ussie
- Pr√©-condition: L'utilisateur a un compte actif
- √âtapes:
  1. Naviguer vers /login
  2. Saisir email valide: user@example.com
  3. Saisir mot de passe valide: SecurePass123!
  4. Cliquer sur "Se connecter"
- R√©sultat attendu: Redirection vers /dashboard avec message "Bienvenue"

Sc√©nario 2: Email invalide
- √âtapes:
  1. Naviguer vers /login
  2. Saisir email invalide: invalid-email
  3. Saisir mot de passe: anything
  4. Cliquer sur "Se connecter"
- R√©sultat attendu: Message d'erreur "Format email invalide"
""")
        logger.info(f"Fichier SFD d'exemple cr√©√© : {login_spec_path}")

    # Cas de test Qwen3.
    qwen3_dir = fixtures_dir / "qwen3"
    qwen3_dir.mkdir(parents=True, exist_ok=True)

    extraction_test_path = qwen3_dir / "test_cases.json"
    if not extraction_test_path.exists():
        extraction_test_path.write_text(json.dumps([
            {
                "name": "basic_extraction",
                "input": "Test de connexion avec email et mot de passe",
                "expected_scenarios": 1
            },
            {
                "name": "complex_extraction",
                "input": "Sp√©cification avec plusieurs sc√©narios de test",
                "expected_scenarios": 3
            }
        ], indent=2))
        logger.info(f"Fichier de cas de test Qwen3 cr√©√© : {extraction_test_path}")

    # Cas de test StarCoder2.
    starcoder2_dir = fixtures_dir / "starcoder2"
    starcoder2_dir.mkdir(parents=True, exist_ok=True)

    starcoder_test_path = starcoder2_dir / "test_cases.json"
    if not starcoder_test_path.exists():
        starcoder_test_path.write_text(json.dumps([
            {
                "name": "basic_playwright_test",
                "scenario": {
                    "titre": "Test de connexion",
                    "objectif": "V√©rifier la connexion",
                    "etapes": ["Naviguer vers /login", "Cliquer sur connexion"]
                }
            }
        ], indent=2))
        logger.info(f"Fichier de cas de test StarCoder2 cr√©√© : {starcoder_test_path}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Fichier : `backend\tests\regression\test_regression_suite.py`

```python
# tests/regression/test_regression_suite.py
"""Suite de tests de r√©gression automatiques pour Altiora.

Ce module impl√©mente une suite de tests de r√©gression qui compare les
r√©sultats actuels de l'application avec des r√©f√©rences (baselines) stock√©es.
Il permet de d√©tecter les r√©gressions fonctionnelles ou de performance
introduites par de nouvelles modifications de code. La suite peut √™tre
configur√©e pour mettre √† jour les baselines et g√©n√©rer des rapports d√©taill√©s.
"""

import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

import pytest
import yaml

from src.orchestrator import Orchestrator
from src.models.qwen3.qwen3_interface import Qwen3OllamaInterface
from src.models.starcoder2.starcoder2_interface import StarCoder2OllamaInterface, TestType

logger = logging.getLogger(__name__)


class RegressionTestResult:
    """Repr√©sente le r√©sultat d'un test de r√©gression individuel."""

    def __init__(self, test_name: str, status: str, metrics: Dict[str, Any],
                 diff: Optional[str] = None):
        """Initialise un r√©sultat de test de r√©gression."

        Args:
            test_name: Le nom du test de r√©gression.
            status: Le statut du test ('PASS', 'FAIL', 'NEW').
            metrics: Un dictionnaire de m√©triques associ√©es au test.
            diff: Un string repr√©sentant le 'diff' si le test a √©chou√© (optionnel).
        """
        self.test_name = test_name
        self.status = status  # PASS, FAIL, NEW
        self.metrics = metrics
        self.diff = diff
        self.timestamp = datetime.now().isoformat()


class RegressionSuite:
    """G√®re l'ex√©cution et l'analyse des tests de r√©gression."""

    def __init__(self, config_path: str = "tests/regression/regression_config.yaml"):
        """Initialise la suite de tests de r√©gression."

        Args:
            config_path: Le chemin vers le fichier de configuration des tests de r√©gression.
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.baseline_path = Path("tests/regression/baselines")
        self.baseline_path.mkdir(parents=True, exist_ok=True)
        self.results_path = Path("tests/regression/results")
        self.results_path.mkdir(parents=True, exist_ok=True)

    def _load_config(self) -> Dict[str, Any]:
        """Charge la configuration des tests de r√©gression depuis le fichier YAML."

        Si le fichier n'existe pas, une configuration par d√©faut est cr√©√©e.

        Returns:
            Un dictionnaire contenant la configuration des tests de r√©gression.
        """
        if not self.config_path.exists():
            logger.info(f"Fichier de configuration de r√©gression non trouv√© √† {self.config_path}. Cr√©ation d'une configuration par d√©faut.")
            default_config = {
                "thresholds": {
                    "max_time_increase": 1.2,  # Augmentation de temps max de 20%.
                    "min_scenarios": 1,
                    "min_tests_generated": 1,
                    "code_similarity": 0.8 # Seuil de similarit√© pour le code g√©n√©r√©.
                },
                "models": {
                    "qwen3": {
                        "model_name": "qwen3-sfd-analyzer",
                        "test_cases": ["scenario_extraction", "test_matrix"]
                    },
                    "starcoder2": {
                        "model_name": "starcoder2-playwright",
                        "test_cases": ["code_generation", "syntax_validity"]
                    }
                },
                "services": {
                    "health_check": True,
                    "response_time": 30  # secondes max.
                },
                "update_baselines": False, # Option pour mettre √† jour les baselines.
                "generate_report": True # Option pour g√©n√©rer un rapport HTML.
            }
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(default_config, f, indent=2)
        return yaml.safe_load(self.config_path.read_text(encoding='utf-8'))

    async def run_full_regression(self) -> Dict[str, Any]:
        """Ex√©cute la suite compl√®te de tests de r√©gression."

        Returns:
            Un dictionnaire contenant les r√©sultats d√©taill√©s de tous les tests de r√©gression,
            un r√©sum√© et des m√©triques de performance.
        """
        logger.info("üîÑ D√©but des tests de r√©gression...")

        results = {
            "timestamp": datetime.now().isoformat(),
            "tests": [],
            "summary": {"passed": 0, "failed": 0, "new": 0},
            "performance_metrics": {}
        }

        # Ex√©cution des tests de r√©gression pour les mod√®les LLM.
        model_results = await self._test_models_regression()
        results["tests"].extend(model_results)

        # Ex√©cution des tests de r√©gression pour le pipeline complet.
        pipeline_results = await self._test_pipeline_regression()
        results["tests"].extend(pipeline_results)

        # Ex√©cution des tests de r√©gression de performance.
        perf_results = await self._test_performance_regression()
        results["performance_metrics"] = perf_results

        # Mise √† jour du r√©sum√© des r√©sultats.
        results["summary"] = {
            "passed": sum(1 for t in results["tests"] if t.status == "PASS"),
            "failed": sum(1 for t in results["tests"] if t.status == "FAIL"),
            "new": sum(1 for t in results["tests"] if t.status == "NEW")
        }

        # G√©n√©ration du rapport HTML si configur√©.
        if self.config.get("generate_report", True):
            self._generate_regression_report(results)

        # Mise √† jour des baselines si configur√©.
        if self.config.get("update_baselines", False):
            await self._update_baselines(results)

        logger.info("‚úÖ Tests de r√©gression termin√©s.")
        return results

    async def _test_models_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression sp√©cifiques aux mod√®les LLM (Qwen3, StarCoder2)."

        Returns:
            Une liste de `RegressionTestResult` pour les tests des mod√®les.
        """
        results = []

        logger.info("Ex√©cution des tests de r√©gression Qwen3...")
        qwen3_results = await self._test_qwen3_regression()
        results.extend(qwen3_results)

        logger.info("Ex√©cution des tests de r√©gression StarCoder2...")
        starcoder_results = await self._test_starcoder2_regression()
        results.extend(starcoder_results)

        return results

    async def _test_qwen3_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le mod√®le Qwen3."

        Returns:
            Une liste de `RegressionTestResult` pour les tests Qwen3.
        """
        results = []
        qwen3 = Qwen3OllamaInterface() # Assurez-vous que cette instance est correctement configur√©e.
        await qwen3.initialize()

        try:
            test_cases = self._load_test_cases("qwen3")

            for test_case in test_cases:
                test_name = f"qwen3_{test_case['name']}"
                # Ex√©cute le test Qwen3 et compare avec la baseline.
                result = await self._run_single_qwen3_test(qwen3, test_case, test_name)
                results.append(result)

        finally:
            await qwen3.close()

        return results

    async def _test_starcoder2_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le mod√®le StarCoder2."

        Returns:
            Une liste de `RegressionTestResult` pour les tests StarCoder2.
        """
        results = []
        starcoder = StarCoder2OllamaInterface() # Assurez-vous que cette instance est correctement configur√©e.
        await starcoder.initialize()

        try:
            test_cases = self._load_test_cases("starcoder2")

            for test_case in test_cases:
                test_name = f"starcoder2_{test_case['name']}"
                # Ex√©cute le test StarCoder2 et compare avec la baseline.
                result = await self._run_single_starcoder_test(starcoder, test_case, test_name)
                results.append(result)

        finally:
            await starcoder.close()

        return results

    async def _test_pipeline_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le pipeline complet."

        Returns:
            Une liste de `RegressionTestResult` pour les tests de pipeline.
        """
        results = []
        orchestrator = Orchestrator() # Assurez-vous que cette instance est correctement configur√©e.
        await orchestrator.initialize()

        try:
            sfd_files = Path("tests/regression/fixtures/sample_sfd").glob("*")

            for sfd_file in sfd_files:
                test_name = f"pipeline_{sfd_file.stem}"
                # Ex√©cute le test de pipeline et compare avec la baseline.
                result = await self._run_pipeline_regression_test(orchestrator, sfd_file, test_name)
                results.append(result)

        finally:
            await orchestrator.close()

        return results

    async def _test_performance_regression(self) -> Dict[str, Any]:
        """Ex√©cute les tests de r√©gression de performance."

        Compare les m√©triques de performance actuelles avec les baselines.

        Returns:
            Un dictionnaire contenant les comparaisons de m√©triques de performance.
        """
        logger.info("Ex√©cution des tests de r√©gression de performance...")
        baseline_file = self.baseline_path / "performance.json"

        baseline: Dict[str, Any] = {}
        if baseline_file.exists():
            try:
                baseline = json.loads(baseline_file.read_text(encoding='utf-8'))
            except json.JSONDecodeError as e:
                logger.error(f"Erreur de lecture de la baseline de performance {baseline_file}: {e}")

        # Mesure les performances actuelles.
        current_metrics = await self._measure_performance()

        comparisons = {}
        for metric, current_value in current_metrics.items():
            if metric in baseline:
                baseline_value = baseline[metric]
                ratio = current_value / baseline_value if baseline_value != 0 else float('inf')
                status = "PASS"
                if metric.endswith("_time") or metric.endswith("_duration") or metric.endswith("_usage_mb"):
                    # Pour les m√©triques o√π une valeur plus petite est meilleure.
                    if ratio > self.config["thresholds"]["max_time_increase"]:
                        status = "FAIL"
                else:
                    # Pour les m√©triques o√π une valeur plus grande est meilleure (ex: d√©bit).
                    if ratio < (1 / self.config["thresholds"]["max_time_increase"]):
                        status = "FAIL"

                comparisons[metric] = {
                    "baseline": baseline_value,
                    "current": current_value,
                    "ratio": ratio,
                    "status": status
                }
            else:
                comparisons[metric] = {
                    "current": current_value,
                    "status": "NEW"
                }

        return comparisons

    async def _run_single_qwen3_test(self, qwen3: Qwen3OllamaInterface,
                                     test_case: Dict[str, Any], test_name: str) -> RegressionTestResult:
        """Ex√©cute un test unique pour Qwen3 et compare son r√©sultat avec la baseline."

        Args:
            qwen3: L'instance de l'interface Qwen3.
            test_case: Le dictionnaire du cas de test.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du test Qwen3.
        # Assurez-vous que `analyze_sfd` prend un `SFDAnalysisRequest`.
        from src.models.sfd_models import SFDAnalysisRequest
        sfd_request = SFDAnalysisRequest(content=test_case["input"], extraction_type=test_case.get("extraction_type", "complete"))
        result = await qwen3.analyze_sfd(sfd_request)

        return self._compare_with_baseline(test_name, result, baseline_file)

    async def _run_single_starcoder_test(self, starcoder: StarCoder2OllamaInterface,
                                         test_case: Dict[str, Any], test_name: str) -> RegressionTestResult:
        """Ex√©cute un test unique pour StarCoder2 et compare son r√©sultat avec la baseline."

        Args:
            starcoder: L'instance de l'interface StarCoder2.
            test_case: Le dictionnaire du cas de test.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du test StarCoder2.
        result = await starcoder.generate_playwright_test(
            scenario=test_case["scenario"],
            config=test_case.get("config", {}),
            test_type=TestType.E2E
        )

        return self._compare_with_baseline(test_name, result, baseline_file)

    async def _run_pipeline_regression_test(self, orchestrator: Orchestrator,
                                            sfd_file: Path, test_name: str) -> RegressionTestResult:
        """Ex√©cute un test de r√©gression du pipeline complet et compare son r√©sultat avec la baseline."

        Args:
            orchestrator: L'instance de l'orchestrateur.
            sfd_file: Le chemin vers le fichier SFD √† traiter.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du pipeline.
        result = await orchestrator.run_full_pipeline(str(sfd_file))

        return self._compare_with_baseline(test_name, result, baseline_file)

    def _compare_with_baseline(self, test_name: str, current_result: Dict[str, Any],
                               baseline_file: Path) -> RegressionTestResult:
        """Compare les r√©sultats actuels d'un test avec sa baseline."

        Args:
            test_name: Le nom du test.
            current_result: Le dictionnaire des r√©sultats actuels du test.
            baseline_file: Le chemin vers le fichier de baseline.

        Returns:
            Un objet `RegressionTestResult` indiquant le statut du test (PASS, FAIL, NEW).
        """
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "result_hash": hashlib.md5(json.dumps(current_result, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        }

        if not baseline_file.exists():
            # Si aucune baseline n'existe, le test est consid√©r√© comme nouveau et la baseline est cr√©√©e.
            logger.info(f"Cr√©ation d'une nouvelle baseline pour le test : {test_name}")
            baseline_file.write_text(json.dumps(current_result, indent=2, ensure_ascii=False))
            return RegressionTestResult(test_name, "NEW", metrics)

        try:
            baseline = json.loads(baseline_file.read_text(encoding='utf-8'))
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de lecture de la baseline {baseline_file}: {e}. Le test sera marqu√© comme FAIL.")
            return RegressionTestResult(test_name, "FAIL", metrics, diff=f"Erreur de lecture de la baseline: {e}")

        # Compare les r√©sultats actuels avec la baseline.
        if self._are_results_equivalent(current_result, baseline):
            logger.info(f"Test de r√©gression {test_name} : PASS.")
            return RegressionTestResult(test_name, "PASS", metrics)
        else:
            diff = self._generate_diff(baseline, current_result)
            logger.warning(f"Test de r√©gression {test_name} : FAIL. Diff√©rences d√©tect√©es.")
            return RegressionTestResult(test_name, "FAIL", metrics, diff)

    def _are_results_equivalent(self, current: Dict[str, Any], baseline: Dict[str, Any]) -> bool:
        """V√©rifie si deux dictionnaires de r√©sultats sont √©quivalents."

        Pour l'instant, la comparaison est bas√©e sur le hachage MD5 du JSON s√©rialis√©.
        Une logique de comparaison plus sophistiqu√©e pourrait √™tre ajout√©e ici
        pour ignorer certaines diff√©rences (ex: horodatages, IDs dynamiques).

        Args:
            current: Le dictionnaire des r√©sultats actuels.
            baseline: Le dictionnaire des r√©sultats de la baseline.

        Returns:
            True si les r√©sultats sont consid√©r√©s comme √©quivalents, False sinon.
        """
        current_hash = hashlib.md5(json.dumps(current, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        baseline_hash = hashlib.md5(json.dumps(baseline, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        return current_hash == baseline_hash

    def _generate_diff(self, baseline: Dict[str, Any], current: Dict[str, Any]) -> str:
        """G√©n√®re un 'diff' lisible entre deux dictionnaires de r√©sultats."

        Args:
            baseline: Le dictionnaire des r√©sultats de la baseline.
            current: Le dictionnaire des r√©sultats actuels.

        Returns:
            Une cha√Æne de caract√®res repr√©sentant le 'diff' unifi√©.
        """
        import difflib
        baseline_str = json.dumps(baseline, indent=2, sort_keys=True, ensure_ascii=False).splitlines()
        current_str = json.dumps(current, indent=2, sort_keys=True, ensure_ascii=False).splitlines()

        diff = difflib.unified_diff(
            baseline_str, current_str,
            fromfile='baseline', tofile='current',
            lineterm=''
        )
        return '\n'.join(diff)

    def _load_test_cases(self, model_type: str) -> List[Dict[str, Any]]:
        """Charge les cas de test sp√©cifiques √† un type de mod√®le depuis les fixtures."

        Args:
            model_type: Le type de mod√®le (ex: 'qwen3', 'starcoder2').

        Returns:
            Une liste de dictionnaires, chaque dictionnaire repr√©sentant un cas de test.
        """
        test_cases_dir = Path("tests/regression/fixtures") / model_type
        test_cases: List[Dict[str, Any]] = []

        if not test_cases_dir.exists():
            logger.warning(f"R√©pertoire de cas de test non trouv√© : {test_cases_dir}")
            return []

        for file in test_cases_dir.glob("*.json"):
            try:
                with open(file, "r", encoding="utf-8") as f:
                    test_cases.extend(json.loads(f.read()))
            except json.JSONDecodeError as e:
                logger.error(f"Erreur de lecture du fichier de cas de test {file}: {e}")
        return test_cases

    async def _measure_performance(self) -> Dict[str, float]:
        """Mesure les m√©triques de performance actuelles de l'application (stub)."

        Dans une impl√©mentation r√©elle, cette m√©thode appellerait des outils
        de profiling ou des endpoints de m√©triques pour collecter des donn√©es
        sur le temps de d√©marrage, le temps de r√©ponse moyen, l'utilisation m√©moire, etc.

        Returns:
            Un dictionnaire de m√©triques de performance.
        """
        logger.info("Mesure des performances actuelles (stub)...")
        # Impl√©mentation simplifi√©e pour la d√©monstration.
        import asyncio
        await asyncio.sleep(1) # Simule un d√©lai de mesure.
        return {
            "startup_time": 2.5,
            "average_response_time": 1.2,
            "memory_usage_mb": 512
        }

    def _generate_regression_report(self, results: Dict[str, Any]):
        """G√©n√®re un rapport HTML d√©taill√© des r√©sultats de la r√©gression."

        Args:
            results: Le dictionnaire complet des r√©sultats de la suite de r√©gression.
        """
        report_file = self.results_path / f"regression_report_{datetime.now():%Y%m%d_%H%M%S}.html"

        # Construction du contenu HTML du rapport.
        test_rows_html = []
        for t in results['tests']:
            diff_html = f"<pre>{t.diff}</pre>" if t.diff else ""
            test_rows_html.append(f"""
            <tr>
                <td>{t.test_name}</td>
                <td class='{t.status.lower()}'>{t.status}</td>
                <td>{json.dumps(t.metrics, ensure_ascii=False)}</td>
                <td>{diff_html}</td>
            </tr>
            """)

        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Altiora Regression Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .pass {{ color: green; font-weight: bold; }}
        .fail {{ color: red; font-weight: bold; }}
        .new {{ color: blue; font-weight: bold; }}
        table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; vertical-align: top; }}
        th {{ background-color: #f2f2f2; }}
        pre {{ white-space: pre-wrap; word-wrap: break-word; background-color: #eee; padding: 10px; border-radius: 5px; }}
    </style>
</head>
<body>
    <h1>Rapport de Tests de R√©gression Altiora</h1>
    <p>G√©n√©r√© le : {results['timestamp']}</p>

    <h2>R√©sum√©</h2>
    <ul>
        <li class="pass">R√©ussis : {results['summary']['passed']}</li>
        <li class="fail">√âchou√©s : {results['summary']['failed']}</li>
        <li class="new">Nouveaux : {results['summary']['new']}</li>
    </ul>

    <h2>R√©sultats des Tests</h2>
    <table>
        <thead>
            <tr><th>Test</th><th>Statut</th><th>M√©triques</th><th>Diff√©rences</th></tr>
        </thead>
        <tbody>
            {''.join(test_rows_html)}
        </tbody>
    </table>

    <h2>M√©triques de Performance</h2>
    <pre>{json.dumps(results['performance_metrics'], indent=2, ensure_ascii=False)}</pre>
</body>
</html>
"""
        try:
            report_file.write_text(html_content, encoding='utf-8')
            logger.info(f"Rapport de r√©gression HTML g√©n√©r√© : {report_file}")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture du rapport HTML : {e}")

    async def _update_baselines(self, results: Dict[str, Any]):
        """Met √† jour les fichiers de baseline avec les r√©sultats actuels des tests marqu√©s comme 'NEW' ou 'FAIL'."

        Args:
            results: Le dictionnaire complet des r√©sultats de la suite de r√©gression.
        """
        logger.info("Mise √† jour des baselines...")
        for test_result in results['tests']:
            if test_result.status == "NEW" or test_result.status == "FAIL":
                baseline_file = self.baseline_path / f"{test_result.test_name}.json"
                try:
                    # Sauvegarde le r√©sultat actuel comme nouvelle baseline.
                    json.dumps(test_result.metrics, indent=2, ensure_ascii=False) # Assurez-vous que c'est le bon contenu √† sauvegarder.
                    baseline_file.write_text(json.dumps(test_result.metrics, indent=2, ensure_ascii=False), encoding='utf-8')
                    logger.info(f"Baseline mise √† jour pour {test_result.test_name} : {baseline_file}")
                except (IOError, OSError) as e:
                    logger.error(f"Erreur lors de la mise √† jour de la baseline {baseline_file}: {e}")


# Configuration pour pytest (pour l'ex√©cution via `pytest tests/regression/test_regression_suite.py`)
@pytest.mark.regression
@pytest.mark.asyncio
async def test_full_regression_suite(wait_for_services):
    """Test de r√©gression complet ex√©cut√© par pytest."

    Cette fonction est le point d'entr√©e pour Pytest. Elle initialise la suite
    de r√©gression et ex√©cute tous les tests d√©finis.
    """
    suite = RegressionSuite()
    results = await suite.run_full_regression()

    # Assertions finales sur le r√©sum√© de la r√©gression.
    assert results["summary"]["failed"] == 0, f"Des tests de r√©gression ont √©chou√© : {results['summary']['failed']} √©checs."
    assert results["summary"]["passed"] > 0, "Aucun test de r√©gression n'a r√©ussi."

```

---

## Fichier : `backend\tests\regression\__init__.py`

```python

```

---

## Fichier : `cli\requirements.txt`

```text
# cli/requirements.txt
"""
D√©pendances CLI uniquement.
"""

click>=8.1
rich>=13.0
httpx>=0.27
```

---

## Fichier : `cli\setup.py`

```python
# cli/setup.py
"""
Setup du package CLI Altiora.
"""

from setuptools import setup, find_packages

setup(
    name="altiora-cli",
    version="2.0.0",
    packages=find_packages(),
    python_requires=">=3.11",
    entry_points={
        "console_scripts": [
            "altiora=altiora_cli.main:main",
        ],
    },
)
```

---

## Fichier : `cli\altiora_cli\main.py`

```python
# cli/altiora_cli/main.py
"""Point d'entr√©e principal pour l'interface en ligne de commande (CLI) d'Altiora.

Ce module utilise la biblioth√®que `click` pour cr√©er une CLI robuste et facile √† utiliser.
Il agr√®ge toutes les commandes disponibles depuis le sous-package `cli.commands`.
"""

from pathlib import Path
import click
from cli.altiora_cli.commands import init, start, test
from cli.altiora_cli.commands import doctor, quickstart, benchmark
from cli.altiora_cli.commands.voice_anything import app as voice_anything_app

app.add_typer(voice_anything_app, name="voice")

@click.group(invoke_without_command=True)
@click.pass_context
def cli(ctx):
    """Altiora CLI ‚Äì Outil pour simplifier le d√©veloppement et la gestion de projets Altiora."""
    # Si aucune sous-commande n'est invoqu√©e, affiche l'aide.
    if ctx.invoked_subcommand is None:
        click.echo(ctx.get_help())

# Enregistrement des commandes aupr√®s du groupe principal.
cli.add_command(init.init)
cli.add_command(start.start)
cli.add_command(test.test)
cli.add_command(doctor.doctor)
cli.add_command(quickstart.quickstart)
cli.add_command(benchmark.benchmark)

def check_project_directory() -> bool:
    """V√©rifie si le r√©pertoire courant semble √™tre un projet Altiora valide."""
    if not (Path("src").exists() and Path("configs").exists()):
        click.echo("‚ùå Ce r√©pertoire ne ressemble pas √† un projet Altiora.")
        return False
    return True

if __name__ == "__main__":
    cli()

```

---

## Fichier : `cli\altiora_cli\__init__.py`

```python
# cli/altiora_cli/__init__.py
"""
Package CLI Altiora.
"""
```

---

## Fichier : `cli\altiora_cli\commands\batch.py`

```python
# cli/altiora_cli/commands/batch.py
"""
Commande `altiora batch`.
"""

import click
import httpx


@click.command()
@click.argument("payload")
@click.option("--host", default="http://localhost:8000")
def batch(payload: str, host: str) -> None:
    """Planifie un traitement batch."""
    r = httpx.post(f"{host}/api/v1/batch/schedule", json={"payload": payload})
    click.echo(f"Job ID : {r.json()['job_id']}")
```

---

## Fichier : `cli\altiora_cli\commands\benchmark.py`

```python
# cli/altiora_cli/commands/benchmark.py
"""Commande `benchmark` pour la CLI Altiora.

Ce module fournit une commande pour lancer les tests de performance
du projet. Il utilise `pytest-benchmark` pour ex√©cuter des benchmarks
et g√©n√©rer un rapport d√©taill√©.
"""

import click
import subprocess
import logging

logger = logging.getLogger(__name__)


@click.command()
@click.option("--runs", default=5, help="Nombre de runs par benchmark pour chaque test de performance.")
def benchmark(runs: int):
    """Lance les tests de performance de l'application Altiora."

    Cette commande ex√©cute les tests situ√©s dans le r√©pertoire `tests/performance`
    en utilisant `pytest-benchmark`. Un rapport JSON est g√©n√©r√© √† la fin de l'ex√©cution.

    Args:
        runs: Le nombre de fois que chaque test de performance sera ex√©cut√©.
    """
    click.echo("üìä Altiora Benchmark ‚Äì D√©marrage des tests de performance...")
    cmd = [
        "pytest",
        "tests/performance", # Cible le r√©pertoire des tests de performance.
        "--benchmark-only", # Ex√©cute uniquement les tests marqu√©s comme benchmarks.
        f"--benchmark-warmup-iterations={runs}", # Nombre d'it√©rations de chauffe.
        f"--benchmark-json=benchmark-report.json", # Fichier de sortie du rapport JSON.
    ]
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        click.echo("‚úÖ Benchmark termin√©. Rapport g√©n√©r√© dans `benchmark-report.json`.")
        logger.info("Benchmark termin√© avec succ√®s.")
    except FileNotFoundError:
        click.echo("‚ùå Erreur: `pytest` ou `pytest-benchmark` n'est pas install√©. Assurez-vous d'avoir install√© les d√©pendances de d√©veloppement.")
        logger.error("pytest ou pytest-benchmark non trouv√©.")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Erreur lors de l'ex√©cution des benchmarks : {e.stderr}")
        logger.error(f"Erreur lors de l'ex√©cution des benchmarks : {e.stderr}")
    except Exception as e:
        click.echo(f"‚ùå Une erreur inattendue est survenue : {e}")
        logger.error(f"Erreur inattendue lors de l'ex√©cution des benchmarks : {e}")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import sys
    import os

    # Pour la d√©monstration, nous allons simuler un fichier de test de performance.
    # En temps normal, ces fichiers existeraient d√©j√† dans `tests/performance`.
    temp_test_dir = Path("tests/performance")
    temp_test_dir.mkdir(parents=True, exist_ok=True)
    (temp_test_dir / "test_example_benchmark.py").write_text("""
import pytest
import time

@pytest.mark.benchmark(group="example")
def test_simple_operation():
    time.sleep(0.01) # Simule une op√©ration rapide.

@pytest.mark.benchmark(group="example")
def test_complex_operation():
    time.sleep(0.1) # Simule une op√©ration plus lente.
""")

    # Ex√©cute la commande benchmark via le syst√®me.
    # Note: Cela n√©cessite que `pytest` et `pytest-benchmark` soient install√©s dans l'environnement.
    print("\n--- Lancement de la d√©monstration du benchmark ---")
    try:
        # Simule l'appel de la commande CLI.
        # sys.argv = ["cli/commands/benchmark.py", "--runs", "2"]
        # benchmark() # Appel direct de la fonction pour la d√©mo.
        # Ou via subprocess pour simuler l'appel CLI complet.
        subprocess.run([sys.executable, __file__, "--runs", "2"], check=True)

    except Exception as e:
        print(f"Une erreur est survenue lors de la d√©monstration : {e}")
    finally:
        # Nettoyage des fichiers temporaires.
        if temp_test_dir.exists():
            import shutil
            shutil.rmtree(temp_test_dir)
        if Path("benchmark-report.json").exists():
            Path("benchmark-report.json").unlink()
        print("D√©monstration du benchmark termin√©e.")
```

---

## Fichier : `cli\altiora_cli\commands\chat.py`

```python
# cli/altiora_cli/commands/chat.py
"""
Commande `altiora chat`.
"""

import click
import httpx


@click.command()
@click.option("--host", default="http://localhost:8000")
def chat(host: str) -> None:
    """Lance une session de chat interactive."""
    click.echo("üí¨ Altiora CLI Chat (tapez 'exit' pour quitter)")
    while True:
        user_input = click.prompt("", prompt_suffix="> ")
        if user_input.lower() in {"exit", "quit"}:
            break
        try:
            r = httpx.post(f"{host}/api/v1/analysis/", json={"spec": user_input})
            click.echo(r.json()["analysis"])
        except httpx.HTTPError as e:
            click.secho(f"Erreur : {e}", fg="red")
```

---

## Fichier : `cli\altiora_cli\commands\doctor.py`

```python
# cli/altiora_cli/commands/doctor.py
"""Commande `doctor` pour la CLI Altiora.

Ce module fournit un outil de diagnostic complet pour v√©rifier l'√©tat de
l'environnement du projet Altiora. Il effectue des v√©rifications sur la
version de Python, les d√©pendances install√©es, la disponibilit√© de Docker,
la pr√©sence des fichiers et dossiers essentiels, et la configuration des
variables d'environnement critiques.
"""

import os
import subprocess
import sys
from pathlib import Path
import click
import pkg_resources
import logging

logger = logging.getLogger(__name__)


@click.command()
def doctor():
    """Effectue un diagnostic complet de l'environnement du projet Altiora."

    Cette commande v√©rifie les pr√©requis syst√®me et les configurations cl√©s
    pour s'assurer que le projet peut fonctionner correctement. Elle affiche
    un r√©sum√© des v√©rifications r√©ussies et des probl√®mes d√©tect√©s.
    """
    ok = True
    click.echo("üîç Altiora Doctor ‚Äì D√©marrage du diagnostic‚Ä¶\n")

    # 1. V√©rification de la version de Python.
    v = sys.version_info
    if v < (3, 9):
        click.echo(f"‚ùå Python >= 3.9 requis (actuel {v.major}.{v.minor}).")
        ok = False
    else:
        click.echo("‚úÖ Version de Python compatible.")

    # 2. V√©rification des d√©pendances Python install√©es.
    try:
        # Tente de charger les d√©pendances list√©es dans requirements.txt.
        pkg_resources.require(open("requirements.txt").readlines())
        click.echo("‚úÖ D√©pendances Python install√©es.")
    except Exception as e:
        click.echo(f"‚ùå D√©pendances Python manquantes ou incorrectes : {e}. Ex√©cutez `pip install -r requirements.txt`.")
        ok = False

    # 3. V√©rification de la disponibilit√© de Docker.
    # `docker info` est utilis√© pour v√©rifier si le d√©mon Docker est en cours d'ex√©cution.
    if subprocess.run(["docker", "info"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL).returncode != 0:
        click.echo("‚ùå Docker non disponible ou non d√©marr√©. Veuillez installer Docker Desktop ou d√©marrer le service Docker.")
        ok = False
    else:
        click.echo("‚úÖ Docker est disponible.")

    # 4. V√©rification des fichiers et dossiers obligatoires du projet.
    click.echo("V√©rification de l'arborescence du projet...")
    required_paths = ["src", "configs", "docker-compose.yml", "requirements.txt"]
    for p in required_paths:
        if not Path(p).exists():
            click.echo(f"‚ùå Fichier/dossier manquant : `{p}`. Assurez-vous d'√™tre √† la racine du projet.")
            ok = False
    if ok: # Si aucune erreur n'a √©t√© d√©tect√©e jusqu'√† pr√©sent pour les chemins.
        click.echo("‚úÖ Arborescence du projet conforme.")

    # 5. V√©rification des variables d'environnement critiques.
    click.echo("V√©rification des variables d'environnement critiques...")
    required_env_vars = ("JWT_SECRET_KEY", "ENCRYPTION_KEY")
    missing_env_vars = [k for k in required_env_vars if not os.getenv(k)]
    if missing_env_vars:
        click.echo(f"‚ùå Variables d'environnement manquantes : {', '.join(missing_env_vars)}. Ex√©cutez `python scripts/generate_keys.py`.")
        ok = False
    else:
        click.echo("‚úÖ Variables d'environnement critiques configur√©es.")

    # Affichage du r√©sum√© final.
    click.echo("\n" + ("‚úÖ Tout semble OK ! Votre environnement Altiora est pr√™t." if ok else "‚ùå Des erreurs ont √©t√© d√©tect√©es. Veuillez consulter les messages ci-dessus pour les corriger."))
    if not ok:
        sys.exit(1) # Quitte avec un code d'erreur si des probl√®mes sont d√©tect√©s.


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # Pour tester ce script, vous pouvez simuler des conditions d'erreur
    # en modifiant temporairement les variables d'environnement ou les fichiers.

    print("\n--- Lancement de la d√©monstration du Altiora Doctor ---")
    try:
        # Simule un environnement valide pour le premier run.
        # Assurez-vous que `requirements.txt` et `docker-compose.yml` existent.
        # Et que `JWT_SECRET_KEY` et `ENCRYPTION_KEY` sont d√©finis dans votre `.env` ou environnement.
        doctor() 
    except SystemExit as e:
        print(f"D√©monstration termin√©e avec le code de sortie : {e.code}")

    # Exemple de simulation d'un √©chec (d√©commenter pour tester).
    # print("\n--- Simulation d'un √©chec (Python version) ---")
    # original_version_info = sys.version_info
    # sys.version_info = (3, 7, 0, 'final', 0) # Simule une version Python trop ancienne.
    # try:
    #     doctor()
    # except SystemExit as e:
    #     print(f"D√©monstration termin√©e avec le code de sortie : {e.code}")
    # finally:
    #     sys.version_info = original_version_info # Restaure la version.

    # print("\n--- Simulation d'un √©chec (variable d'environnement manquante) ---")
    # original_jwt_secret = os.getenv("JWT_SECRET_KEY")
    # if "JWT_SECRET_KEY" in os.environ: del os.environ["JWT_SECRET_KEY"]
    # try:
    #     doctor()
    # except SystemExit as e:
    #     print(f"D√©monstration termin√©e avec le code de sortie : {e.code}")
    # finally:
    #     if original_jwt_secret: os.environ["JWT_SECRET_KEY"] = original_jwt_secret

    print("D√©monstration du Altiora Doctor termin√©e.")
```

---

## Fichier : `cli\altiora_cli\commands\init.py`

```python
# cli/altiora_cli/commands/init.py
"""Commande `init` pour la CLI Altiora."""

import click
from pathlib import Path

@click.command()
@click.argument('project_name')
def init(project_name):
    """Initialise un nouveau projet Altiora avec une structure de base."""
    project_dir = Path(project_name)
    if project_dir.exists():
        click.echo(f"Le r√©pertoire `{project_name}` existe d√©j√†. L'initialisation est annul√©e.")
        return

    # Cr√©ation de la structure de r√©pertoires standard.
    project_dir.mkdir()
    (project_dir / "src").mkdir()
    (project_dir / "tests").mkdir()
    (project_dir / "configs").mkdir()
    (project_dir / "docs").mkdir()
    (project_dir / "scripts").mkdir()

    click.echo(f"‚úÖ Projet `{project_name}` initialis√© avec succ√®s.")

```

---

## Fichier : `cli\altiora_cli\commands\models.py`

```python
# cli/altiora_cli/commands/models.py
"""
Commande `altiora models`.
"""

import click
import httpx


@click.command()
@click.argument("model")
@click.option("--host", default="http://localhost:8000")
def models(model: str, host: str) -> None:
    """Force le swap vers un mod√®le."""
    r = httpx.post(f"{host}/api/v1/models/swap", json={"model": model})
    click.echo(f"Mod√®le actif : {r.json()['active_model']}")
```

---

## Fichier : `cli\altiora_cli\commands\quickstart.py`

```python
# cli/altiora_cli/commands/quickstart.py
"""Commande `quickstart` pour la CLI Altiora.

Ce module fournit un assistant interactif pour configurer rapidement un projet Altiora.
Il guide l'utilisateur √† travers les √©tapes de clonage du projet (si n√©cessaire),
la configuration des variables d'environnement, la construction des images Docker,
et le lancement des services.
"""

import os
import click
from pathlib import Path
import subprocess
import logging

logger = logging.getLogger(__name__)


@click.command()
def quickstart():
    """Lance un assistant de configuration rapide pour le projet Altiora."

    Cette commande est con√ßue pour les nouveaux utilisateurs afin de les aider
    √† d√©marrer rapidement avec un environnement de travail fonctionnel.
    """
    click.echo("üöÄ Altiora Quickstart ‚Äì Suivez le guide pour une configuration rapide !\n")

    # 1. V√©rifie si un projet Altiora est d√©j√† pr√©sent. Si non, propose de cloner un exemple.
    if not Path("src").exists():
        if click.confirm("Aucun projet Altiora d√©tect√© dans le r√©pertoire courant. Voulez-vous cloner un projet d'exemple ?"):
            url = "https://github.com/altiora/template.git" # URL du template de projet.
            try:
                # Clone le d√©p√¥t temporairement, d√©place les fichiers, puis supprime le d√©p√¥t temporaire.
                subprocess.run(["git", "clone", url, ".altiora_temp"], check=True, capture_output=True, text=True)
                # Utilise shutil.move pour d√©placer les contenus de mani√®re plus robuste.
                for item in Path(".altiora_temp").iterdir():
                    shutil.move(str(item), ".")
                Path(".altiora_temp").rmdir()
                click.echo("‚úÖ Projet d'exemple clon√© et configur√©.")
            except subprocess.CalledProcessError as e:
                click.echo(f"‚ùå Erreur lors du clonage du projet : {e.stderr}")
                logger.error(f"Erreur lors du clonage du projet : {e.stderr}")
                return # Arr√™te le quickstart en cas d'√©chec.
            except Exception as e:
                click.echo(f"‚ùå Erreur inattendue lors de la configuration du projet : {e}")
                logger.error(f"Erreur inattendue lors de la configuration du projet : {e}")
                return
        else:
            click.echo("Op√©ration annul√©e. Veuillez cr√©er ou naviguer vers un projet Altiora existant.")
            return
    else:
        click.echo("‚úÖ Projet Altiora d√©j√† pr√©sent dans le r√©pertoire courant.")

    # 2. Configuration des variables d'environnement dans le fichier `.env`.
    env_path = Path(".env")
    if not env_path.exists():
        click.echo("üìù Cr√©ation du fichier `.env` pour les variables d'environnement.")
        # Demande √† l'utilisateur de fournir des cl√©s ou g√©n√®re des valeurs par d√©faut.
        jwt_secret = click.prompt("JWT_SECRET_KEY (laisser vide pour g√©n√©rer automatiquement)", default="", show_default=False)
        encryption_key = click.prompt("ENCRYPTION_KEY (laisser vide pour g√©n√©rer automatiquement)", default="", show_default=False)
        
        # G√©n√®re des cl√©s si l'utilisateur n'en fournit pas.
        if not jwt_secret: jwt_secret = os.urandom(32).hex()
        if not encryption_key: encryption_key = os.urandom(32).hex()

        try:
            with open(env_path, "w", encoding="utf-8") as f:
                f.write(f"JWT_SECRET_KEY={jwt_secret}\n")
                f.write(f"ENCRYPTION_KEY={encryption_key}\n")
                f.write("# Ajoutez d'autres variables d'environnement ici si n√©cessaire.\n")
            click.echo(f"‚úÖ Fichier `.env` cr√©√© avec les cl√©s g√©n√©r√©es.")
        except (IOError, OSError) as e:
            click.echo(f"‚ùå Erreur lors de la cr√©ation du fichier .env : {e}")
            logger.error(f"Erreur lors de la cr√©ation du fichier .env : {e}")
            return
    else:
        click.echo("‚úÖ Fichier `.env` d√©j√† pr√©sent.")

    # 3. Construction des images Docker du projet.
    click.echo("\n‚öôÔ∏è  Construction des images Docker du projet...")
    try:
        subprocess.run(["docker-compose", "build"], check=True, capture_output=True, text=True)
        click.echo("‚úÖ Images Docker construites avec succ√®s.")
    except FileNotFoundError:
        click.echo("‚ùå Erreur: `docker-compose` n'est pas install√© ou n'est pas dans le PATH.")
        logger.error("docker-compose non trouv√©.")
        return
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Erreur lors de la construction des images Docker : {e.stderr}")
        logger.error(f"Erreur lors de la construction des images Docker : {e.stderr}")
        return

    # 4. Lancement des services Docker.
    click.echo("\nüéâ Lancement des services Altiora...")
    try:
        subprocess.run(["docker-compose", "up", "-d"], check=True, capture_output=True, text=True)
        click.echo("‚úÖ Services Altiora d√©marr√©s avec succ√®s en arri√®re-plan.")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Erreur lors du d√©marrage des services : {e.stderr}")
        logger.error(f"Erreur lors du d√©marrage des services : {e.stderr}")
        return

    click.echo("\n‚úÖ Quickstart termin√© ! Votre environnement Altiora est pr√™t.")
    click.echo("Vous pouvez maintenant acc√©der au tableau de bord via votre navigateur √† http://localhost:8000 (ou le port configur√©).")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # Pour tester ce script, vous pouvez simuler un environnement vide ou existant.
    # Assurez-vous que `docker-compose.yml` et `requirements.txt` existent dans le r√©pertoire courant.

    print("\n--- Lancement de la d√©monstration du Altiora Quickstart ---")
    try:
        # Simule l'appel de la commande CLI.
        # sys.argv = ["cli/commands/quickstart.py"]
        # quickstart() # Appel direct de la fonction pour la d√©mo.
        # Ou via subprocess pour simuler l'appel CLI complet.
        subprocess.run([sys.executable, __file__], check=True)

    except Exception as e:
        print(f"Une erreur est survenue lors de la d√©monstration : {e}")
    finally:
        # Nettoyage des fichiers temporaires cr√©√©s par la d√©mo (si n√©cessaire).
        # Par exemple, si un projet d'exemple a √©t√© clon√©.
        # if Path(".altiora_temp").exists():
        #     import shutil
        #     shutil.rmtree(".altiora_temp")
        # if Path(".env").exists():
        #     Path(".env").unlink()
        print("D√©monstration du quickstart termin√©e.")

```

---

## Fichier : `cli\altiora_cli\commands\start.py`

```python
# cli/altiora_cli/commands/start.py
"""Commande `start` pour la CLI Altiora."""

import click
import subprocess

@click.command()
def start():
    """Lance tous les services Altiora en utilisant docker-compose."""
    try:
        # Ex√©cute `docker-compose up -d` pour d√©marrer les services en arri√®re-plan.
        subprocess.run(["docker-compose", "up", "-d"], check=True)
        click.echo("‚úÖ Services Altiora d√©marr√©s avec succ√®s.")
    except FileNotFoundError:
        click.echo("‚ùå Erreur: `docker-compose` n'est pas install√© ou n'est pas dans le PATH.")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Erreur lors du d√©marrage des services : {e}")

```

---

## Fichier : `cli\altiora_cli\commands\test.py`

```python
# cli/altiora_cli/commands/test.py
"""Commande `test` pour la CLI Altiora."""

import click
import subprocess

@click.command()
def test():
    """Ex√©cute la suite de tests du projet avec pytest."""
    try:
        # Ex√©cute pytest pour lancer tous les tests d√©couvrables.
        subprocess.run(["pytest"], check=True)
        click.echo("‚úÖ Tests ex√©cut√©s avec succ√®s.")
    except FileNotFoundError:
        click.echo("‚ùå Erreur: `pytest` n'est pas install√©. Ex√©cutez `pip install pytest`.")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Des tests ont √©chou√© : {e}")

```

---

## Fichier : `cli\altiora_cli\commands\voice_anything.py`

```python
import asyncio

import typer
from backend.altiora.core.modules.voice_anythingllm import VoiceAnythingLLM

app = typer.Typer()


@app.command("anything")
def start_voice_anything(
        workspace: str = typer.Option("Altiora Knowledge", help="Nom du workspace AnythingLLM")
):
    """D√©marre l'assistant vocal connect√© √† AnythingLLM"""
    voice = VoiceAnythingLLM(workspace)

    typer.echo("üé§ Assistant vocal AnythingLLM pr√™t")
    typer.echo("üîä Dites 'Altiora' ou parlez pour interagir")
    typer.echo("üõë Ctrl+C pour quitter")

    try:
        asyncio.run(voice.start_session())
    except KeyboardInterrupt:
        typer.echo("\nüëã √Ä bient√¥t !")


if __name__ == "__main__":
    app()
```

---

## Fichier : `cli\altiora_cli\commands\__init__.py`

```python
# cli/altiora_cli/commands/__init__.py
from .init import init
from .start import start
from .test import test

__all__ = ["init", "start", "test"]
```

---

## Fichier : `configs\prometheus.yml`

```yaml
# configs/prometheus.yml
groups:
  - name: altiora_lora
    rules:
      - alert: LoRA_Retrain_Needed
        expr: altiora_feedback_count >= 50
        for: 1m
        annotations:
          summary: "50+ feedbacks re√ßus, fine-tuning LoRA lanc√©"
```

---

## Fichier : `docker\docker-compose.yml`

```yaml
version: '3.8'

services:
  # ==========================
  # Infrastructure de base
  # ==========================
  redis:
    image: redis:7-alpine
    container_name: altiora-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server
      --appendonly yes
      --save 900 1
      --save 300 10
      --save 60 10000
      --maxmemory ${REDIS_MAXMEMORY:-8gb}
      --maxmemory-policy allkeys-lru
      --lazyfree-lazy-eviction yes
      --lazyfree-lazy-expire yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - altiora-network

  # ==========================
  # Backend principal
  # ==========================
  backend:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    container_name: altiora-backend
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      - ./models:/app/models:ro
      - ./logs/backend:/app/logs
      - ./data:/app/data
      - ./config:/app/config:ro
    environment:
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - QWEN3_MODEL_PATH=/app/models/qwen3-32b-q4_k_m.gguf
      - STARCODER_MODEL_PATH=/app/models/starcoder2-15b-q8_0.gguf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    deploy:
      resources:
        limits:
          memory: 24G
          cpus: '16'

  # ==========================
  # Microservices
  # ==========================
  ocr-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.services
      target: runtime
    container_name: altiora-ocr
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "${OCR_PORT:-8001}:8001"
    volumes:
      - ./data/input:/data/input:ro
      - ./data/processed:/data/processed
      - ./temp/ocr:/tmp/ocr_temp
    environment:
      - SERVICE_NAME=ocr
      - REDIS_URL=redis://redis:6379
      - CACHE_TTL=86400
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - services

  alm-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.services
      target: runtime
    container_name: altiora-alm
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "${ALM_PORT:-8002}:8002"
    environment:
      - SERVICE_NAME=alm
      - REDIS_URL=redis://redis:6379
      - ALM_API_URL=${ALM_API_URL:-http://alm-server:8080}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - services

  excel-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.services
      target: runtime
    container_name: altiora-excel
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "${EXCEL_PORT:-8003}:8003"
    volumes:
      - ./data/matrices:/data/matrices
      - ./templates:/templates:ro
    environment:
      - SERVICE_NAME=excel
      - REDIS_URL=redis://redis:6379
      - TEMPLATE_PATH=/templates
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - services

  playwright-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.services
      target: runtime
    container_name: altiora-playwright
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "${PLAYWRIGHT_PORT:-8004}:8004"
    volumes:
      - ./tests/generated:/tests
      - ./reports:/reports
      - ./screenshots:/screenshots
      - ./videos:/videos
    environment:
      - SERVICE_NAME=playwright
      - REDIS_URL=redis://redis:6379
      - HEADED=${HEADED:-false}
      - BROWSER=${BROWSER:-chromium}
      - PARALLEL_WORKERS=${PARALLEL_WORKERS:-4}
    cap_add:
      - SYS_ADMIN
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - services

  # ==========================
  # AnythingLLM avec RAG
  # ==========================
  anythingllm:
    build:
      context: .
      dockerfile: docker/Dockerfile.anythingllm
    container_name: altiora-anythingllm
    restart: unless-stopped
    depends_on:
      - redis
      - backend
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:3001"
    volumes:
      - anythingllm-storage:/app/server/storage
      - ./data:/app/data:ro
      - ./docs:/app/docs:ro
    environment:
      - ANYTHINGLLM_STORAGE=/app/server/storage
      - VECTOR_DB_URL=redis://redis:6379
      - WORKSPACE_NAME=Altiora Knowledge
      - ENABLE_RAG=true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - ui

  # ==========================
  # Dashboard Altiora (Dash)
  # ==========================
  dashboard:
    build:
      context: .
      dockerfile: services/dash/Dockerfile
    container_name: altiora-dashboard
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "${DASHBOARD_PORT:-8050}:8050"
    volumes:
      - ./src/dashboard:/app
      - ./logs/dashboard:/app/logs
    environment:
      - PROMETHEUS_URL=http://prometheus:9090
      - BACKEND_URL=http://backend:8000
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/_dash-layout"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - altiora-network
    profiles:
      - ui

  # ==========================
  # Monitoring avec Prometheus
  # ==========================
  prometheus:
    image: prom/prometheus:latest
    container_name: altiora-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    networks:
      - altiora-network
    profiles:
      - monitoring

# ==========================
# Volumes persistants
# ==========================
volumes:
  redis-data:
    driver: local
  anythingllm-storage:
    driver: local
  prometheus-data:
    driver: local

# ==========================
# R√©seau interne
# ==========================
networks:
  altiora-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# ==========================
# Profiles d'utilisation
# ==========================
# docker-compose up              # Backend + Redis seulement
# docker-compose --profile services up    # + Tous microservices
# docker-compose --profile ui up          # + AnythingLLM + Dashboard
# docker-compose --profile monitoring up  # + Prometheus
```

---

## Fichier : `docs\ADVANCED_CONFIGURATION.md`

```markdown
# Guide de Configuration Avanc√©e

Ce guide explique la structure et l'utilisation des fichiers de configuration YAML situ√©s dans le r√©pertoire `configs/`. Ces fichiers permettent une configuration fine et dynamique du comportement de l'application sans avoir √† modifier le code source.

## 1. `master_config.yaml`

Ce fichier est le point d'entr√©e de la configuration. Il r√©f√©rence les autres fichiers de configuration, permettant de charger de mani√®re modulaire diff√©rents aspects du syst√®me.

**Exemple :**
```yaml
imports:
  - services.yaml
  - models.yaml
  - roles.yaml
  - error_handling.yaml
  - retry_config.yaml
```

- **`imports`**: Une liste de chemins vers d'autres fichiers de configuration √† inclure. L'ordre peut √™tre important si des configurations se chevauchent.

## 2. `services.yaml`

Ce fichier d√©finit les configurations de tous les microservices externes que l'orchestrateur doit contacter.

**Exemple :**
```yaml
services:
  ocr:
    url: "http://ocr_service:8001/ocr/extract-text"
    timeout: 60
  alm:
    url: "http://alm_service:8002/alm/create-ticket"
    timeout: 120
    api_key: "${ALM_API_KEY}" # Injection depuis les variables d'environnement
```

- **`services`**: Un dictionnaire o√π chaque cl√© est le nom d'un service.
- **`url`**: Le point de terminaison complet du service.
- **`timeout`**: Le timeout en secondes pour les requ√™tes vers ce service.
- **`api_key`**: Exemple d'injection de secrets depuis les variables d'environnement. La syntaxe `${VAR_NAME}` est utilis√©e pour substituer la valeur de `VAR_NAME` au chargement.

## 3. `models.yaml` et `models_config.yaml`

Ces fichiers g√®rent la configuration des mod√®les de langage (LLMs).

### `models.yaml`

D√©finit les mod√®les disponibles et leurs r√¥les.

**Exemple :**
```yaml
models:
  sfd_analyzer:
    provider: ollama
    model_name: "qwen3-sfd-analyzer"
    role: "Analyse de sp√©cifications fonctionnelles"
  code_generator:
    provider: ollama
    model_name: "starcoder2-playwright"
    role: "G√©n√©ration de code de test Playwright"
```

### `models_config.yaml`

D√©finit les param√®tres techniques pour interagir avec les mod√®les.

**Exemple :**
```yaml
model_config:
  ollama:
    temperature: 0.5
    top_p: 0.9
    max_tokens: 4096
    stop_sequences: ["<|endoftext|>"]
```

- **`temperature`**, **`top_p`**, etc. : Param√®tres standards pour contr√¥ler la g√©n√©ration des LLMs.

## 4. `roles.yaml`

D√©finit les r√¥les des utilisateurs et les permissions associ√©es (RBAC - Role-Based Access Control).

**Exemple :**
```yaml
roles:
  - name: admin
    permissions:
      - "users:create"
      - "users:delete"
      - "system:shutdown"
      - "*"
  - name: developer
    permissions:
      - "sfd:analyze"
      - "tests:generate"
      - "tests:run"
  - name: guest
    permissions:
      - "reports:view"
```

- **`roles`**: Une liste de r√¥les.
- **`name`**: Le nom unique du r√¥le.
- **`permissions`**: Une liste d'actions autoris√©es. Le caract√®re `*` peut √™tre utilis√© comme joker.

## 5. `retry_config.yaml`

D√©finit les strat√©gies de nouvelle tentative (retry) pour les op√©rations r√©seau, en utilisant un backoff exponentiel.

**Exemple :**
```yaml
retry_policy:
  default:
    max_attempts: 3
    backoff_factor: 2.0
    initial_delay: 1.0
    jitter: 0.1
  services:
    alm:
      max_attempts: 5
      backoff_factor: 2.5
```

- **`default`**: La politique de retry √† appliquer si aucune politique sp√©cifique n'est d√©finie pour un service.
- **`services`**: Permet de surcharger la politique par d√©faut pour des services sp√©cifiques (ici, le service `alm` est plus r√©silient).

## Comment √ßa marche ?

Le module `configs.settings_loader` est responsable du chargement de `master_config.yaml`. Il parcourt les `imports`, charge chaque fichier YAML, et fusionne les configurations en un seul objet de configuration global. Il g√®re √©galement la substitution des variables d'environnement.

```

---

## Fichier : `docs\API_REFERENCE.md`

```markdown
# Documentation de l'API Altiora

Ce document fournit une r√©f√©rence d√©taill√©e pour les points de terminaison (endpoints) de l'API des diff√©rents microservices du projet Altiora.

**URL de base** : Les URL sont relatives √† l'h√¥te et au port de chaque service, comme d√©fini dans les variables d'environnement (ex: `http://localhost:8005` pour le service d'authentification).

## 1. Service d'Authentification (`src/auth`)

Ce service g√®re l'authentification des utilisateurs et la d√©livrance des jetons JWT.

### `POST /auth/token`

Authentifie un utilisateur et retourne un `access_token`.

- **Requ√™te** :
    - **M√©thode** : `POST`
    - **Content-Type** : `application/x-www-form-urlencoded`
    - **Corps** :
        - `username` (str, requis) : Le nom d'utilisateur.
        - `password` (str, requis) : Le mot de passe.

- **R√©ponse (200 OK)** :

```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer"
}
```

- **R√©ponse (401 Unauthorized)** :

```json
{
  "detail": "Incorrect username or password"
}
```

### `GET /users/me`

Retourne les informations de l'utilisateur actuellement authentifi√©.

- **Requ√™te** :
    - **M√©thode** : `GET`
    - **Authentification** : Jeton `Bearer` requis.

- **R√©ponse (200 OK)** :

```json
{
  "username": "testuser",
  "email": "test@example.com",
  "full_name": "Test User",
  "disabled": false
}
```

## 2. Service OCR (`services/ocr`)

Extrait le texte brut √† partir de fichiers.

### `POST /ocr/extract-text`

- **Requ√™te** :
    - **M√©thode** : `POST`
    - **Content-Type** : `multipart/form-data`
    - **Corps** :
        - `file` (File, requis) : Le fichier √† traiter (PDF, PNG, JPG).

- **R√©ponse (200 OK)** :

```json
{
  "text": "Contenu textuel extrait du document..."
}
```

## 3. Service ALM (`services/alm`)

Interagit avec un syst√®me de gestion du cycle de vie des applications (ALM).

### `POST /alm/create-ticket`

Cr√©e un nouveau ticket (ex: bug, user story) dans le syst√®me ALM.

- **Requ√™te** :
    - **M√©thode** : `POST`
    - **Content-Type** : `application/json`
    - **Corps** :

```json
{
  "project_id": "PROJ",
  "title": "Titre du ticket",
  "description": "Description d√©taill√©e du ticket.",
  "issue_type": "Bug"
}
```

- **R√©ponse (201 Created)** :

```json
{
  "ticket_id": "PROJ-123",
  "url": "https://jira.example.com/browse/PROJ-123"
}
```

## 4. Service Excel (`services/excel`)

G√©n√®re des fichiers Excel √† partir de donn√©es JSON.

### `POST /excel/create-matrix`

Cr√©e une matrice de test au format Excel.

- **Requ√™te** :
    - **M√©thode** : `POST`
    - **Content-Type** : `application/json`
    - **Corps** :

```json
{
  "filename": "matrice_de_test.xlsx",
  "data": [
    {
      "ID": "TC-001",
      "Sc√©nario": "Connexion r√©ussie",
      "Statut": "Pass"
    },
    {
      "ID": "TC-002",
      "Sc√©nario": "Mot de passe incorrect",
      "Statut": "Fail"
    }
  ]
}
```

- **R√©ponse (200 OK)** :
    - **Content-Type** : `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`
    - Le fichier Excel est retourn√© en tant que corps de la r√©ponse.

## 5. Service Playwright (`services/playwright`)

Ex√©cute des scripts de test Playwright.

### `POST /playwright/run-test`

Ex√©cute un script de test Playwright fourni.

- **Requ√™te** :
    - **M√©thode** : `POST`
    - **Content-Type** : `application/json`
    - **Corps** :

```json
{
  "script_content": "from playwright.sync_api import sync_playwright\n\ndef run(playwright):\n    ..."
}
```

- **R√©ponse (200 OK)** :

```json
{
  "status": "succeeded", // ou "failed"
  "output": "...logs de l'ex√©cution...",
  "error": null // ou message d'erreur
}
```

```

---

## Fichier : `docs\ARCHITECTURE.md`

```markdown
# Architecture du Projet Altiora

## üéØ Vision G√©n√©rale

Le projet Altiora vise √† d√©velopper un assistant IA personnel intelligent, capable d'automatiser des t√¢ches complexes de g√©nie logiciel,
notamment l'analyse de sp√©cifications fonctionnelles d√©taill√©es (SFD) et la g√©n√©ration de tests d'automatisation (Playwright). L'architecture est con√ßue pour √™tre modulaire,
scalable et s√©curis√©e, en s'appuyant sur des microservices et des mod√®les d'IA locaux.

## üèóÔ∏è Composants Cl√©s

L'architecture d'Altiora est articul√©e autour de plusieurs modules et services interconnect√©s :

### 1. Orchestrateur Principal (`src/orchestrator.py`)

C'est le cerveau central du syst√®me. Il coordonne l'ensemble du pipeline, de la r√©ception d'une SFD √† la g√©n√©ration et potentiellement √† l'ex√©cution des tests. Ses responsabilit√©s incluent :
- La gestion du flux de travail (workflow).
- L'interaction avec les diff√©rents microservices.
- La collecte et l'agr√©gation des m√©triques de performance.
- L'int√©gration des r√®gles m√©tier et des politiques de s√©curit√©.

### 2. Interfaces avec les Mod√®les d'IA (LLMs)

Altiora utilise des mod√®les de langage de grande taille (LLMs) pour des t√¢ches sp√©cifiques, interfac√©s via Ollama :
- **`src/models/qwen3/qwen3_interface.py` (Qwen3)** : Sp√©cialis√© dans l'analyse des SFD. Il extrait les sc√©narios de test, les objectifs, les pr√©conditions, les √©tapes, etc. Il peut √©galement g√©n√©rer des matrices de test structur√©es. Il int√®gre d√©sormais la capacit√© de charger des adaptateurs de personnalit√© (LoRA) pour affiner son comportement.
- **`src/models/starcoder2/starcoder2_interface.py` (StarCoder2)** : D√©di√© √† la g√©n√©ration de code, principalement des scripts de test Playwright, √† partir des sc√©narios identifi√©s par Qwen3.

### 3. Microservices Sp√©cialis√©s (`services/`)

Ces services sont des composants ind√©pendants, souvent conteneuris√©s (via Docker), qui g√®rent des t√¢ches sp√©cifiques et peuvent √™tre appel√©s par l'orchestrateur :
- **Auth (`src/auth/`)**: G√®re l'authentification des utilisateurs et la g√©n√©ration de tokens JWT. C'est un service FastAPI ind√©pendant avec sa propre base de donn√©es SQLite.
- **OCR (`services/ocr/`)** : Extrait le texte de documents (par exemple, PDF de SFD) pour permettre leur analyse par les LLMs.
- **ALM (`services/alm/`)** : Interface avec des outils de gestion du cycle de vie des applications (Application Lifecycle Management) comme Jira ou Azure DevOps pour cr√©er ou mettre √† jour des tickets (bugs, t√¢ches).
- **Excel (`services/excel/`)** : G√®re la cr√©ation et le formatage de fichiers Excel, notamment pour les matrices de test, en appliquant des r√®gles de validation et de style.
- **Playwright (`services/playwright/`)** : Ex√©cute les tests Playwright g√©n√©r√©s et renvoie les r√©sultats d'ex√©cution.
- **Dash (`services/dash/`)**: Fournit un tableau de bord interactif pour visualiser les m√©triques de performance et les r√©sultats des tests.

### 4. Politiques et Garde-fous (`policies/` et `guardrails/`)

Ces modules sont cruciaux pour la s√©curit√©, la conformit√© et la qualit√© des interactions de l'IA :
- **`policies/business_rules.py`** : Contient les r√®gles m√©tier strictes pour valider le code g√©n√©r√© (par exemple, interdiction de `time.sleep()`, formatage des IDs de test, utilisation du module de rapport standard).
- **`policies/privacy_policy.py`** : D√©tecte et masque les informations personnelles identifiables (PII) dans le texte, g√®re les r√®gles de r√©tention des donn√©es et le consentement, en conformit√© avec le RGPD.
- **`policies/toxicity_policy.py`** : √âvalue le contenu pour d√©tecter la toxicit√© (insultes, menaces, etc.) et les fuites de PII, avec des niveaux de gravit√© et des actions associ√©es.
- **`guardrails/admin_control_system.py`** : Syst√®me centralis√© pour les actions administratives (gel d'utilisateur, sauvegarde, restauration).
- **`guardrails/emergency_handler.py`** : G√®re les situations d'urgence (d√©tection de menaces graves, fuites de donn√©es) en d√©clenchant des actions pr√©d√©finies (gel global, notifications).
- **`guardrails/interaction_guardrail.py`** : Un point d'entr√©e unique pour toutes les interactions utilisateur, appliquant les politiques de toxicit√© et de confidentialit√© en temps r√©el.
- **`guardrails/admin_dashboard.py`** : Une interface graphique (Tkinter) pour la supervision et le contr√¥le du syst√®me par les administrateateurs, affichant des m√©triques et permettant des actions manuelles.

### 5. Post-traitement (`post_processing/`)

Ces modules affinent les sorties de l'IA ou pr√©parent les donn√©es pour d'autres √©tapes :
- **`post_processing/code_validator.py`** : Valide la syntaxe, le linting (avec Ruff) et le formatage (avec Black) du code Python g√©n√©r√©, y compris des v√©rifications sp√©cifiques √† Playwright.
- **`post_processing/excel_formatter.py`** : Applique un formatage visuel aux donn√©es destin√©es √† Excel, comme la coloration conditionnelle des lignes et l'ajustement des largeurs de colonnes.
- **`post_processing/output_sanitizer.py`** : Nettoie les r√©ponses brutes des LLMs en supprimant les wrappers de code, les phrases d'introduction et en masquant les PII.

## üîÑ Flux de Travail Principal (SFD vers Tests)

1.  **Authentification**: L'utilisateur obtient un token JWT du service d'authentification.
2.  **Extraction SFD** : L'orchestrateur re√ßoit un chemin vers une SFD. Le service OCR extrait le contenu textuel.
3.  **Analyse SFD (Qwen3)** : Le contenu textuel est envoy√© √† Qwen3 (via `qwen3_interface`) qui analyse la sp√©cification et extrait les sc√©narios de test structur√©s. La personnalit√© affin√©e de Qwen3 est utilis√©e ici.
4.  **Validation des Sc√©narios** : Les sc√©narios extraits sont valid√©s par `policies/excel_policy` pour s'assurer de leur conformit√© structurelle et m√©tier.
5.  **G√©n√©ration de la Matrice de Tests** : Qwen3 g√©n√®re une matrice de tests d√©taill√©e √† partir des sc√©narios. Cette matrice est ensuite format√©e par `post_processing/excel_formatter` et peut √™tre export√©e.
6.  **Importation ALM** : Les sc√©narios de test peuvent √™tre import√©s dans un outil ALM via le service `services/alm/`.
7.  **G√©n√©ration de Code (StarCoder2)** : Pour chaque sc√©nario valid√©, StarCoder2 (via `starcoder2_interface`) g√©n√®re le code de test Playwright correspondant. Ce code passe par `post_processing/output_sanitizer` pour le nettoyage et par `post_processing/code_validator` et `policies/business_rules` pour la validation.
8.  **Ex√©cution des Tests** : Les tests Playwright g√©n√©r√©s sont ex√©cut√©s par le service `services/playwright/`.
9.  **Rapport et M√©triques** : Les r√©sultats d'ex√©cution sont collect√©s, agr√©g√©s et un rapport final est g√©n√©r√©, incluant des m√©triques de performance. Les r√©sultats sont visualisables via le service `services/dash/`.

## üõ†Ô∏è Technologies Utilis√©es

- **Langage** : Python
- **LLMs** : Qwen3, StarCoder2 (via Ollama)
- **Framework Web** : FastAPI
- **Base de donn√©es / Cache** : Redis, SQLite (pour l'authentification)
- **Authentification**: PyJWT, passlib
- **Tests d'automatisation** : Playwright, Pytest
- **Outils de Qualit√© Code** : Black, Ruff
- **Conteneurisation** : Docker, Docker Compose
- **Interface GUI** : Tkinter (pour Admin Dashboard), Dash (pour le reporting)

## üöÄ D√©ploiement

Le projet est con√ßu pour √™tre d√©ploy√© localement sur des machines individuelles (ex: Lenovo ThinkPad) via Docker Compose, assurant l'isolation et la facilit√© de gestion des microservices.
```

---

## Fichier : `docs\conf.py`

```python

# docs/conf.py
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'Altiora'
copyright = '2025, User'
author = 'User'
release = '0.1'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',
    'sphinx.ext.viewcode',
]

templates_path = ['_templates']
exclude_patterns = []



# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = 'pydata_sphinx_theme'
html_static_path = ['_static']

html_theme_options = {
    "show_prev_next": False,
    "header_links_before_dropdown": 4,
    "use_edit_page_button": True,
    "show_toc_level": 1,
    "navbar_align": "left",  # ["left", "right"]
    "navbar_center": ["navbar-nav"],
    "navbar_end": ["navbar-icon-links"],
    "secondary_sidebar_items": {
        "**": ["page-toc", "edit-this-page", "sourcelink"],
        "index": [],
    },
    "footer_items": ["copyright", "sphinx-version"],
    "pygment_light_style": "tango",
    "pygment_dark_style": "monokai",
    "default_mode": "dark"
}


```

---

## Fichier : `docs\DEVELOPER_GUIDE.md`

```markdown
# Guide du D√©veloppeur - Projet Altiora

Bienvenue dans l'√©quipe de d√©veloppement d'Altiora ! Ce guide a pour but de vous aider √† configurer votre environnement et √† contribuer au projet de mani√®re efficace et coh√©rente.

## 1. Conventions de Codage

La coh√©rence du code est essentielle pour la maintenabilit√©. Nous suivons des standards stricts pour garantir la qualit√©.

### Python

- **Style de code** : Nous utilisons **Black** pour le formatage automatique du code. Il n'y a pas de d√©bat sur le style, Black s'en charge.
- **Linting** : Nous utilisons **Ruff** pour d√©tecter les erreurs, les bugs potentiels et les incoh√©rences de style. La configuration se trouve dans le fichier `pyproject.toml`.
- **Typage** : Tout le nouveau code Python doit utiliser les **annotations de type** (type hints) de mani√®re syst√©matique. Nous visons une couverture de typage de 100%.
- **Nommage** :
    - `variables_et_fonctions` : en `snake_case`.
    - `Classes` : en `PascalCase`.
    - `CONSTANTES` : en `UPPER_SNAKE_CASE`.
- **Docstrings** : Utilisez les docstrings au format Google pour documenter les modules, classes et fonctions.

### Commandes de Qualit√©

Avant de commiter, veuillez ex√©cuter ces commandes √† la racine du projet pour vous assurer que votre code est conforme :

```bash
# Formater le code avec Black
black .

# Analyser et corriger automatiquement avec Ruff
ruff check . --fix
```

## 2. Strat√©gie de Branchement Git

Nous utilisons une strat√©gie de branchement inspir√©e de **GitFlow**.

- **`main`** : Cette branche est toujours stable et repr√©sente la version en production. On ne pousse jamais directement dessus.
- **`develop`** : La branche principale de d√©veloppement. Toutes les nouvelles fonctionnalit√©s sont fusionn√©es ici. C'est la branche de r√©f√©rence pour les builds de d√©veloppement.
- **`feature/<nom-feature>`** : Chaque nouvelle fonctionnalit√© doit √™tre d√©velopp√©e dans sa propre branche, cr√©√©e √† partir de `develop`. Exemple : `feature/integration-jira`.
- **`fix/<nom-fix>`** : Pour les corrections de bugs non urgents. Cr√©√©e √† partir de `develop`.
- **`hotfix/<nom-hotfix>`** : Pour les corrections de bugs critiques en production. Cr√©√©e √† partir de `main` et fusionn√©e √† la fois dans `main` et `develop`.

## 3. Processus de Contribution

1.  **Assignez-vous une t√¢che** (ou cr√©ez-en une si n√©cessaire) dans notre outil de suivi (Jira, Trello, etc.).
2.  **Cr√©ez une branche** √† partir de `develop` en suivant la convention de nommage (`feature/TICKET-123-ma-feature`).
3.  **D√©veloppez** votre fonctionnalit√©. N'oubliez pas d'ajouter des **tests unitaires et d'int√©gration** pertinents.
4.  **Assurez-vous que tous les tests passent** en ex√©cutant `pytest`.
5.  **V√©rifiez la qualit√© du code** avec `black .` et `ruff check . --fix`.
6.  **Mettez √† jour la documentation** si vos changements l'impactent (README, docstrings, guides, etc.).
7.  **Faites une Pull Request (PR)** de votre branche vers `develop`.
8.  **D√©crivez clairement vos changements** dans la PR et liez-la √† la t√¢che correspondante.
9.  **Demandez une revue de code** √† au moins un autre d√©veloppeur.
10. Une fois la PR approuv√©e et les tests CI/CD pass√©s, elle sera fusionn√©e dans `develop`.

## 4. Environnement de D√©veloppement Local

Consultez le `docs/installation_guide.md` pour la configuration de base.

### Scripts Utiles

Le dossier `scripts/` contient de nombreux outils pour vous aider :

- **`start_dev.sh`** : D√©marre l'environnement de d√©veloppement complet (peut-√™tre √† adapter).
- **`validate_setup.py`** : V√©rifie que votre configuration locale est correcte.
- **`diagnose_ollama.py`** : Utile pour d√©boguer les probl√®mes de connexion ou de performance avec les mod√®les de langage.

### Tests

Pour ex√©cuter l'ensemble de la suite de tests :

```bash
pytest
```

Pour ex√©cuter les tests d'un fichier sp√©cifique :

```bash
pytest tests/test_orchestrator.py
```

Pour ex√©cuter les tests avec une couverture de code :

```bash
pytest --cov=src
```

```

---

## Fichier : `docs\env-documentation.md`

```markdown
# Variables d'Environnement - Documentation

## Vue d'ensemble

Ce document d√©crit toutes les variables d'environnement utilis√©es dans le projet Altiora.

## Variables G√©n√©rales

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `ENVIRONMENT` | Environnement d'ex√©cution | `development`, `staging`, `production`, `test` | `development` | ‚úÖ |
| `DEBUG` | Mode debug activ√© | `true`, `false` | `false` | ‚ùå |
| `LOG_LEVEL` | Niveau de logging | `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL` | `INFO` | ‚ùå |

## Configuration Ollama

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `OLLAMA_HOST` | URL du serveur Ollama | URL compl√®te | `http://localhost:11434` | ‚úÖ |
| `OLLAMA_TIMEOUT` | Timeout des requ√™tes (secondes) | Entier | `180` | ‚ùå |
| `OLLAMA_KEEP_ALIVE` | Dur√©e de maintien des mod√®les en m√©moire | Format dur√©e (ex: `30m`) | `30m` | ‚ùå |
| `QWEN3_MODEL` | Nom du mod√®le Qwen3 | Cha√Æne | `qwen3-sfd-analyzer` | ‚úÖ |
| `STARCODER2_MODEL` | Nom du mod√®le StarCoder2 | Cha√Æne | `starcoder2-playwright` | ‚úÖ |

## Configuration Redis

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `REDIS_URL` | URL de connexion Redis | Format `redis://[password@]host:port/db` | `redis://localhost:6379` | ‚úÖ |
| `REDIS_PASSWORD` | Mot de passe Redis | Cha√Æne | *(vide)* | ‚ùå (‚úÖ en prod) |
| `REDIS_DB` | Num√©ro de base Redis | 0-15 | `0` | ‚ùå |
| `REDIS_TTL_SFD` | TTL cache analyses SFD (sec) | Entier | `86400` (24h) | ‚ùå |
| `REDIS_TTL_TESTS` | TTL cache tests g√©n√©r√©s (sec) | Entier | `43200` (12h) | ‚ùå |
| `REDIS_TTL_OCR` | TTL cache r√©sultats OCR (sec) | Entier | `604800` (7j) | ‚ùå |
| `REDIS_TTL_MODEL` | TTL cache r√©ponses mod√®les (sec) | Entier | `3600` (1h) | ‚ùå |

## Services Microservices

### Service d'Authentification

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `DATABASE_URL` | URL de la base de donn√©es d'authentification | Cha√Æne de connexion | `sqlite:///./auth.db` | ‚úÖ |
| `JWT_SECRET_KEY` | Cl√© secr√®te JWT | Cha√Æne (min 32 car.) | *(g√©n√©r√©)* | ‚úÖ |
| `JWT_ALGORITHM` | Algorithme JWT | `HS256`, `HS384`, `HS512` | `HS256` | ‚ùå |
| `ACCESS_TOKEN_EXPIRE_MINUTES` | Dur√©e de vie du token (minutes) | Entier | `60` | ‚ùå |

### Service OCR

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `OCR_SERVICE_HOST` | H√¥te du service OCR | Nom d'h√¥te/IP | `localhost` | ‚úÖ |
| `OCR_SERVICE_PORT` | Port du service OCR | 1-65535 | `8001` | ‚úÖ |
| `OCR_SERVICE_TIMEOUT` | Timeout OCR (secondes) | Entier | `60` | ‚ùå |
| `DOCTOPLUS_CONFIG` | Chemin config Doctoplus | Chemin absolu | `/app/config/config.json` | ‚ùå |

### Service ALM

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `ALM_SERVICE_HOST` | H√¥te du service ALM | Nom d'h√¥te/IP | `localhost` | ‚úÖ |
| `ALM_SERVICE_PORT` | Port du service ALM | 1-65535 | `8002` | ‚úÖ |
| `ALM_SERVICE_TIMEOUT` | Timeout ALM (secondes) | Entier | `120` | ‚ùå |
| `ALM_API_URL` | URL du serveur ALM r√©el | URL compl√®te | `http://alm-server:8080` | ‚ùå |
| `ALM_API_KEY` | Cl√© API pour ALM | Cha√Æne | *(vide)* | ‚ùå |

### Service Excel

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `EXCEL_SERVICE_HOST` | H√¥te du service Excel | Nom d'h√¥te/IP | `localhost` | ‚úÖ |
| `EXCEL_SERVICE_PORT` | Port du service Excel | 1-65535 | `8003` | ‚úÖ |
| `EXCEL_SERVICE_TIMEOUT` | Timeout Excel (secondes) | Entier | `60` | ‚ùå |

### Service Playwright

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `PLAYWRIGHT_SERVICE_HOST` | H√¥te du service Playwright | Nom d'h√¥te/IP | `localhost` | ‚úÖ |
| `PLAYWRIGHT_SERVICE_PORT` | Port du service Playwright | 1-65535 | `8004` | ‚úÖ |
| `PLAYWRIGHT_SERVICE_TIMEOUT` | Timeout Playwright (secondes) | Entier | `300` | ‚ùå |
| `PLAYWRIGHT_WORKERS` | Nombre de workers parall√®les | Entier | `4` | ‚ùå |
| `PLAYWRIGHT_BROWSER` | Navigateur par d√©faut | `chromium`, `firefox`, `webkit` | `chromium` | ‚ùå |
| `PLAYWRIGHT_HEADED` | Mode avec interface graphique | `true`, `false` | `false` | ‚ùå |
| `PLAYWRIGHT_SCREENSHOT_ON_FAILURE` | Screenshots sur √©chec | `true`, `false` | `true` | ‚ùå |
| `PLAYWRIGHT_VIDEO_ON_FAILURE` | Vid√©os sur √©chec | `true`, `false` | `true` | ‚ùå |

### Service Dash

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `DASH_SERVICE_HOST` | H√¥te du service Dash | Nom d'h√¥te/IP | `localhost` | ‚úÖ |
| `DASH_SERVICE_PORT` | Port du service Dash | 1-65535 | `8050` | ‚úÖ |

## S√©curit√©

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `RATE_LIMIT_ENABLED` | Limitation de d√©bit activ√©e | `true`, `false` | `true` | ‚ùå |
| `RATE_LIMIT_REQUESTS` | Nombre max de requ√™tes | Entier | `100` | ‚ùå |
| `RATE_LIMIT_WINDOW_SECONDS` | Fen√™tre de temps (secondes) | Entier | `60` | ‚ùå |
| `ALLOWED_ORIGINS` | Origines CORS autoris√©es | Liste s√©par√©e par virgules | `*` | ‚ùå |

## Chemins et R√©pertoires

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `DATA_DIR` | R√©pertoire des donn√©es | Chemin | `./data` | ‚ùå |
| `MODELS_DIR` | R√©pertoire des mod√®les | Chemin | `./models` | ‚ùå |
| `LOGS_DIR` | R√©pertoire des logs | Chemin | `./logs` | ‚ùå |
| `REPORTS_DIR` | R√©pertoire des rapports | Chemin | `./reports` | ‚ùå |
| `TEMP_DIR` | R√©pertoire temporaire | Chemin | `./temp` | ‚ùå |
| `CACHE_DIR` | R√©pertoire de cache | Chemin | `./cache` | ‚ùå |

## Pipeline d'Orchestration

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `PIPELINE_MAX_PARALLEL_TESTS` | Tests max en parall√®le | Entier | `5` | ‚ùå |
| `PIPELINE_MAX_PARALLEL_SCENARIOS` | Sc√©narios max en parall√®le | Entier | `10` | ‚ùå |
| `PIPELINE_FALLBACK_ENABLED` | Mode fallback activ√© | `true`, `false` | `true` | ‚ùå |
| `PIPELINE_RETRY_MAX_ATTEMPTS` | Tentatives max de retry | Entier | `3` | ‚ùå |
| `PIPELINE_RETRY_BACKOFF_FACTOR` | Facteur de backoff | D√©cimal | `2.0` | ‚ùå |

## Docker (si utilisation avec Docker Compose)

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `COMPOSE_PROJECT_NAME` | Nom du projet Docker | Cha√Æne | `altiora` | ‚ùå |
| `DOCKER_BUILDKIT` | Utiliser BuildKit | `0`, `1` | `1` | ‚ùå |
| `OLLAMA_MEMORY_LIMIT` | Limite m√©moire Ollama | Format Docker | `12G` | ‚ùå |
| `OLLAMA_CPU_LIMIT` | Limite CPU Ollama | Nombre | `8` | ‚ùå |

## D√©veloppement

| Variable | Description | Valeurs | D√©faut | Requis |
|----------|-------------|---------|---------|--------|
| `MOCK_OCR_SERVICE` | Simuler le service OCR | `true`, `false` | `false` | ‚ùå |
| `MOCK_ALM_SERVICE` | Simuler le service ALM | `true`, `false` | `false` | ‚ùå |
| `USE_LOCAL_MODELS` | Utiliser mod√®les locaux | `true`, `false` | `false` | ‚ùå |
```

---

## Fichier : `docs\generate_docs.py`

```python
# docs/generate_docs.py
# docs/generate_docs.py
"""Module pour la g√©n√©ration automatis√©e de la documentation du projet Altiora.

Ce script centralise la cr√©ation de divers artefacts de documentation,
notamment la sp√©cification OpenAPI (Swagger/Redoc) de l'API, des diagrammes
d'architecture, des guides de d√©ploiement et des rapports de performance.
Il vise √† maintenir la documentation √† jour et coh√©rente avec le code.
"""

from pathlib import Path
import json
import logging

from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi

logger = logging.getLogger(__name__)


class DocumentationGenerator:
    """G√©n√©rateur de documentation pour le projet Altiora."""

    def __init__(self, source_dir: Path):
        """Initialise le g√©n√©rateur de documentation."

        Args:
            source_dir: Le r√©pertoire source de l'application (ex: `src/`).
        """
        self.source_dir = source_dir
        self.docs: Dict[str, Any] = {} # Dictionnaire pour stocker les documents g√©n√©r√©s.

    def generate(self):
        """G√©n√®re la documentation compl√®te du projet en appelant les m√©thodes sp√©cifiques."

        Cette m√©thode orchestre le processus de g√©n√©ration de documentation.
        """
        logger.info("D√©marrage de la g√©n√©ration de la documentation...")
        # 1. Documentation de l'API (OpenAPI/Swagger).
        self._generate_api_docs()

        # 2. Diagrammes d'architecture (placeholder).
        self._generate_architecture_diagrams()

        # 3. Guide de d√©ploiement (placeholder).
        self._generate_deployment_guide()

        # 4. Rapports de performance (placeholder).
        self._generate_performance_docs()
        logger.info("G√©n√©ration de la documentation termin√©e.")

    def _generate_api_docs(self):
        """G√©n√®re la sp√©cification OpenAPI (Swagger/Redoc) de l'API FastAPI."

        Cette m√©thode importe dynamiquement l'application FastAPI et utilise
        `get_openapi` pour cr√©er le sch√©ma, puis le sauvegarde au format JSON.
        """
        logger.info("G√©n√©ration de la documentation API (OpenAPI/Swagger)...")
        # Ajoute le r√©pertoire source au chemin syst√®me pour permettre l'importation de l'application FastAPI.
        import sys
        sys.path.append(str(self.source_dir))
        
        try:
            # Importe l'instance de l'application FastAPI.
            from main import app

            # G√©n√®re le sch√©ma OpenAPI.
            openapi_schema = get_openapi(
                title="Altiora QA Automation API",
                version="1.0.0",
                description="API pour l'automatisation des tests avec IA, l'analyse de SFD et la gestion des rapports.",
                routes=app.routes,
            )

            # Sauvegarde le sch√©ma OpenAPI dans un fichier JSON.
            output_path = Path("docs/openapi.json")
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(openapi_schema, f, indent=2, ensure_ascii=False)
            logger.info(f"Sp√©cification OpenAPI g√©n√©r√©e et sauvegard√©e : {output_path}")
        except ImportError:
            logger.error(f"Impossible d'importer l'application FastAPI depuis {self.source_dir}. Assurez-vous que `main.py` existe et est valide.")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture de la sp√©cification OpenAPI : {e}")
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la g√©n√©ration de la documentation API : {e}", exc_info=True)

    def _generate_architecture_diagrams(self):
        """G√©n√®re les diagrammes d'architecture du projet (placeholder)."

        Cette m√©thode serait utilis√©e pour int√©grer des outils de g√©n√©ration de diagrammes
        (ex: PlantUML, Mermaid, Graphviz) √† partir de d√©finitions textuelles ou de code.
        """
        logger.info("G√©n√©ration des diagrammes d'architecture (non impl√©ment√©)...")
        # TODO: Impl√©menter la logique de g√©n√©ration de diagrammes.
        pass

    def _generate_deployment_guide(self):
        """G√©n√®re le guide de d√©ploiement (placeholder)."

        Cette m√©thode pourrait assembler des informations provenant de diff√©rentes
        sources (fichiers de configuration, scripts Docker) pour cr√©er un guide
        de d√©ploiement complet.
        """
        logger.info("G√©n√©ration du guide de d√©ploiement (non impl√©ment√©)...")
        # TODO: Impl√©menter la logique de g√©n√©ration du guide de d√©ploiement.
        pass

    def _generate_performance_docs(self):
        """G√©n√®re la documentation des benchmarks de performance (placeholder)."

        Cette m√©thode pourrait analyser les rapports de `pytest-benchmark`
        et g√©n√©rer des visualisations ou des r√©sum√©s pour la documentation.
        """
        logger.info("G√©n√©ration de la documentation des performances (non impl√©ment√©)...")
        # TODO: Impl√©menter la logique de g√©n√©ration des docs de performance.
        pass


# ------------------------------------------------------------------
# Point d'entr√©e CLI
# ------------------------------------------------------------------
if __name__ == "__main__":
    import argparse
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    parser = argparse.ArgumentParser(description="G√©n√®re la documentation du projet Altiora.")
    parser.add_argument(
        "--source-dir",
        type=Path,
        default=Path("src"),
        help="R√©pertoire source de l'application (contenant main.py)."
    )
    args = parser.parse_args()

    print("\n--- Lancement de la g√©n√©ration de documentation ---")
    generator = DocumentationGenerator(args.source_dir)
    generator.generate()
    print("D√©monstration de la g√©n√©ration de documentation termin√©e.")

```

---

## Fichier : `docs\installation_guide.md`

```markdown
# Guide d'Installation Complet pour Altiora

Ce guide vous accompagnera √† travers les √©tapes n√©cessaires pour installer et configurer Altiora, un assistant IA personnel intelligent capable d'automatiser des t√¢ches complexes de g√©nie logiciel.

## Pr√©requis

Assurez-vous d'avoir les √©l√©ments suivants install√©s sur votre syst√®me :

- **Python 3.11+** : Le langage de programmation principal du projet.
- **pip** : Le gestionnaire de paquets Python (normalement inclus avec Python).
- **Git** : Pour cloner le d√©p√¥t du projet.
- **Docker Desktop** (ou Docker Engine et Docker Compose) : Pour ex√©cuter les microservices conteneuris√©s.

## √âtapes d'Installation

### 1. Cloner le D√©p√¥t

Ouvrez votre terminal ou invite de commande et ex√©cutez la commande suivante pour cloner le d√©p√¥t du projet :

```bash
git clone https://github.com/votre_utilisateur/Altiora_project.git # Remplacez par l'URL r√©elle de votre d√©p√¥t
cd Altiora_project
```

### 2. Configuration de l'Environnement

Le projet utilise un fichier `.env` pour g√©rer les variables d'environnement. Un exemple est fourni :

1. Copiez le fichier `.env.example` vers `.env` :

```bash
cp .env.example .env
# Sur Windows, vous pouvez utiliser :
# copy .env.example .env
```

2. **√âditez le fichier `.env`** : Ouvrez le fichier `.env` avec un √©diteur de texte et ajustez les valeurs selon votre environnement. Les variables importantes incluent :

    - `OLLAMA_HOST` : L'URL de votre instance Ollama (par d√©faut `http://localhost:11434`).
    - `ALM_API_URL`, `ALM_API_KEY` : Pour l'int√©gration avec votre syst√®me ALM (si utilis√©).
    - `REDIS_URL` : L'URL de votre instance Redis (par d√©faut `redis://localhost:6379`).
    - `JWT_SECRET_KEY` : La cl√© secr√®te pour l'authentification JWT. **Il est crucial de d√©finir une cl√© secr√®te forte ici.**

### 3. Cr√©ation de l'Environnement Virtuel et Installation des D√©pendances Python

Il est fortement recommand√© d'utiliser un environnement virtuel pour isoler les d√©pendances du projet.

1. Cr√©ez l'environnement virtuel :

```bash
python -m venv .venv
```

2. Activez l'environnement virtuel :

    - **Sur macOS/Linux** :

    ```bash
source .venv/bin/activate
```

    - **Sur Windows (Command Prompt)** :

    ```bash
.venv\Scripts\activate.bat
```

    - **Sur Windows (PowerShell)** :

    ```powershell
.venv\Scripts\Activate.ps1
```

3. Installez les d√©pendances Python :

```bash
pip install -r requirements.txt
```

4. Installez les navigateurs Playwright (n√©cessaire pour l'ex√©cution des tests) :

```bash
playwright install
```

### 4. Lancement des Services

Altiora s'appuie sur Ollama pour les mod√®les d'IA et d'autres microservices conteneuris√©s via Docker Compose.

1. **Lancez Ollama** :

Assurez-vous que le serveur Ollama est en cours d'ex√©cution. Vous pouvez le lancer via :

```bash
ollama serve
```

Si vous n'avez pas Ollama install√©, suivez les instructions sur [ollama.ai](https://ollama.ai/download).

2. **D√©marrez les microservices Docker Compose** :

Dans le r√©pertoire racine du projet, ex√©cutez :

```bash
docker-compose up -d
```

Cette commande va construire (si n√©cessaire) et d√©marrer les conteneurs pour les services Redis, OCR, ALM, Excel, Playwright, Auth et Dash.

### 5. V√©rification de l'Installation

Vous pouvez v√©rifier que les services sont en cours d'ex√©cution en acc√©dant √† leurs points de terminaison de sant√© (si disponibles) ou en ex√©cutant les tests :

```bash
# Exemple de v√©rification de sant√© (si les services sont expos√©s sur localhost)
curl http://localhost:8001/health # Service OCR
curl http://localhost:8002/health # Service ALM
curl http://localhost:8003/health # Service Excel
curl http://localhost:8005/health # Service Auth

# Acc√©der √† la documentation API (Swagger UI)
# Pour le service ALM: http://localhost:8002/docs
# Pour le service Excel: http://localhost:8003/docs
# Pour le service Auth: http://localhost:8005/docs

# Acc√©der au Dashboard
# http://localhost:8050

# Ex√©cuter les tests du projet
pytest
```

### 6. Utilisation de Base

Une fois tous les services en cours d'ex√©cution, vous pouvez interagir avec l'orchestrateur via le CLI. Voir le `README.md` pour plus de d√©tails sur les commandes disponibles.

### 7. Troubleshooting

- **Probl√®mes de port** : Si un service Docker ne d√©marre pas, v√©rifiez que les ports (ex: 8001, 8002, 8003, 8004, 8005, 8050, 11434, 6379) ne sont pas d√©j√† utilis√©s par d'autres applications sur votre syst√®me.
- **Ollama** : Assurez-vous que les mod√®les `qwen3-sfd-analyzer` et `starcoder2-playwright` sont bien t√©l√©charg√©s et disponibles dans Ollama. Vous pouvez les t√©l√©charger manuellement si n√©cessaire (`ollama pull qwen3-sfd-analyzer`, `ollama pull starcoder2-playwright`).
- **D√©pendances Python** : Si vous rencontrez des erreurs d'importation, assurez-vous que votre environnement virtuel est activ√© et que toutes les d√©pendances de `requirements.txt` sont install√©es.
- **Base de donn√©es d'authentification**: Le service d'authentification utilise une base de donn√©es SQLite qui sera cr√©√©e dans un volume Docker. Si vous avez besoin de la r√©initialiser, vous pouvez supprimer le volume avec `docker volume rm altiora_auth_db`.

### 8. Conclusion

Vous avez maintenant install√© et configur√© Altiora sur votre machine locale. Vous pouvez commencer √† utiliser l'assistant pour automatiser des t√¢ches complexes de g√©nie logiciel. Pour plus d'informations, consultez la [documentation officielle](https://github.com/votre_utilisateur/Altiora_project/docs) du projet.
```

---

## Fichier : `docs\MODEL_TRAINING.md`

```markdown
# Guide sur l'Entra√Ænement et le Fine-Tuning des Mod√®les

Ce guide explique comment pr√©parer les donn√©es, lancer des scripts d'entra√Ænement et √©valuer les mod√®les fine-tun√©s dans le projet Altiora.

## 1. Pr√©paration des Donn√©es

La qualit√© du fine-tuning d√©pend enti√®rement de la qualit√© du jeu de donn√©es.

### Format des Donn√©es

Nous utilisons le format **JSON Lines (`.jsonl`)** pour les jeux de donn√©es d'entra√Ænement. Chaque ligne est un objet JSON ind√©pendant qui repr√©sente un exemple d'entra√Ænement.

Pour le fine-tuning de mod√®les de type question/r√©ponse ou instruction, le format doit contenir au moins une cl√© pour l'instruction (le *prompt*) et une cl√© pour la sortie attendue (la *completion*).

**Exemple pour le fine-tuning de `qwen3-sfd-analyzer` (`data/training/sfd_analysis_dataset.jsonl`) :**

```json
{"instruction": "Extrait les sc√©narios de test de la SFD suivante : ...", "output": "[{"scenario": "Connexion r√©ussie", ...}]"}
{"instruction": "Analyse cette sp√©cification pour les cas de test : ...", "output": "[{"scenario": "Mot de passe oubli√©", ...}]"}
```

- **`instruction`**: Le texte d'entr√©e qui sera fourni au mod√®le (la SFD).
- **`output`**: La sortie JSON structur√©e que le mod√®le doit apprendre √† g√©n√©rer.

### Emplacement des Donn√©es

Placez tous les jeux de donn√©es d'entra√Ænement dans le r√©pertoire `data/training/`.

## 2. Lancement du Fine-Tuning

Le processus de fine-tuning est g√©r√© par des scripts sp√©cifiques situ√©s dans `src/training/`.

### Configuration

Avant de lancer un entra√Ænement, configurez les param√®tres dans `configs/training_config.json`.

**Exemple de `training_config.json` :**

```json
{
  "model_to_fine_tune": "qwen3-sfd-analyzer",
  "dataset_path": "data/training/sfd_analysis_dataset.jsonl",
  "output_model_name": "qwen3-sfd-analyzer-v2",
  "epochs": 3,
  "learning_rate": 5e-5,
  "batch_size": 4,
  "lora_rank": 8
}
```

- **`model_to_fine_tune`**: Le nom du mod√®le de base √† partir duquel commencer le fine-tuning.
- **`dataset_path`**: Le chemin vers le jeu de donn√©es `.jsonl`.
- **`output_model_name`**: Le nom du nouveau mod√®le fine-tun√© qui sera cr√©√© dans Ollama.
- **`epochs`**, **`learning_rate`**, **`batch_size`**: Hyperparam√®tres standards de l'entra√Ænement.
- **`lora_rank`**: La dimension des matrices de l'adaptation LoRA (Low-Rank Adaptation). Une valeur plus √©lev√©e peut capturer plus d'informations mais augmente la taille de l'adaptateur.

### Ex√©cution du Script

Le script principal pour le fine-tuning est `src/training/advanced_trainer.py`. Pour le lancer :

```bash
python -m src.training.advanced_trainer --config configs/training_config.json
```

Ce script va :
1. Charger la configuration.
2. Charger le jeu de donn√©es.
3. Pr√©parer le mod√®le de base.
4. Lancer le processus de fine-tuning en utilisant les techniques de LoRA.
5. Sauvegarder le nouvel adaptateur de mod√®le (les poids LoRA) dans `data/models/`.
6. Cr√©er un nouveau Modelfile pour Ollama qui combine le mod√®le de base avec le nouvel adaptateur.
7. Enregistrer le mod√®le final dans Ollama sous le nom sp√©cifi√© par `output_model_name`.

## 3. √âvaluation des Mod√®les

Une fois un mod√®le fine-tun√©, il est crucial de l'√©valuer pour s'assurer qu'il performe mieux que le mod√®le de base et qu'il n'a pas subi de "r√©gression catastrophique" sur des t√¢ches g√©n√©rales.

### Suite de Tests d'√âvaluation

Nous utilisons une suite de tests d√©di√©e pour l'√©valuation, situ√©e dans `tests/performance/` et `tests/regression/`.

- **`tests/performance/test_model_accuracy.py`**: Contient des tests qui comparent la sortie du mod√®le √† des r√©sultats attendus sur un jeu de donn√©es de test (qui ne doit pas faire partie du jeu d'entra√Ænement).
- **`tests/regression/test_model_regression.py`**: Contient des tests qui v√©rifient que le mod√®le peut toujours effectuer des t√¢ches de base pour √©viter la sur-sp√©cialisation.

### Lancement de l'√âvaluation

Vous pouvez lancer l'√©valuation en utilisant `pytest` et en ciblant les tests pertinents. Assurez-vous de modifier les tests pour qu'ils pointent vers le nouveau nom du mod√®le (`output_model_name`).

```bash
# Exemple pour lancer un test de performance sp√©cifique
pytest tests/performance/test_model_accuracy.py
```

## 4. Utilisation du Mod√®le Fine-Tun√©

Une fois que vous √™tes satisfait des performances du nouveau mod√®le, mettez √† jour le fichier `configs/models.yaml` pour que l'application utilise votre mod√®le fine-tun√© par d√©faut.

**Avant :**
```yaml
models:
  sfd_analyzer:
    model_name: "qwen3-sfd-analyzer"
```

**Apr√®s :**
```yaml
models:
  sfd_analyzer:
    model_name: "qwen3-sfd-analyzer-v2"
```

```

---

## Fichier : `docs\requirements.txt`

```text
Sphinx
```

---

## Fichier : `docs\examples\Altiora.html`

```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Altiora - Interface Conversationnelle Avanc√©e</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <style>
        :root {
            /* Light Theme Colors */
            --bg-primary: #f8f9fa;
            --bg-secondary: #ffffff;
            --bg-tertiary: #e9ecef;
            --text-primary: #212529;
            --text-secondary: #6c757d;
            --border-color: #dee2e6;
            --primary: #4361ee;
            --primary-dark: #3a56d4;
            --secondary: #7209b7;
            --success: #4cc9f0;
            --warning: #f72585;
            --shadow: 0 4px 12px rgba(0,0,0,0.08);
            
            /* Dark Theme Colors */
            --dark-bg-primary: #121212;
            --dark-bg-secondary: #1e1e1e;
            --dark-bg-tertiary: #2d2d2d;
            --dark-text-primary: #f8f9fa;
            --dark-text-secondary: #adb5bd;
            --dark-border-color: #495057;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', system-ui, sans-serif;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            height: 100vh;
            display: flex;
            flex-direction: column;
            transition: background-color 0.3s, color 0.3s;
        }

        body.dark-theme {
            background-color: var(--dark-bg-primary);
            color: var(--dark-text-primary);
        }

        .header {
            background: linear-gradient(120deg, var(--primary), var(--secondary));
            color: white;
            padding: 1rem 1.5rem;
            box-shadow: var(--shadow);
            z-index: 10;
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-size: 1.5rem;
            font-weight: 700;
        }

        .logo i {
            font-size: 1.75rem;
        }

        .theme-controls {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .theme-toggle {
            background: rgba(255, 255, 255, 0.2);
            border: none;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            color: white;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s;
        }

        .theme-toggle:hover {
            background: rgba(255, 255, 255, 0.3);
        }

        .features {
            display: flex;
            gap: 1.5rem;
            font-size: 0.9rem;
        }

        .feature-tag {
            background: rgba(255, 255, 255, 0.2);
            padding: 0.25rem 0.75rem;
            border-radius: 999px;
            backdrop-filter: blur(4px);
        }

        .chat-container {
            flex: 1;
            display: flex;
            max-width: 1200px;
            width: 100%;
            margin: 1.5rem auto;
            gap: 1.5rem;
            padding: 0 1.5rem;
        }

        .sidebar {
            width: 280px;
            background: var(--bg-secondary);
            border-radius: 12px;
            box-shadow: var(--shadow);
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        body.dark-theme .sidebar {
            background: var(--dark-bg-secondary);
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        }

        .sidebar-section {
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 1.5rem;
        }

        body.dark-theme .sidebar-section {
            border-bottom: 1px solid var(--dark-border-color);
        }

        .sidebar-section:last-child {
            border-bottom: none;
            padding-bottom: 0;
        }

        .sidebar h3 {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-secondary);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        body.dark-theme .sidebar h3 {
            color: var(--dark-text-secondary);
        }

        .context-item {
            padding: 0.75rem;
            border-radius: 8px;
            background: var(--bg-tertiary);
            margin-bottom: 0.5rem;
            font-size: 0.85rem;
        }

        body.dark-theme .context-item {
            background: var(--dark-bg-tertiary);
        }

        .context-key {
            font-weight: 600;
            color: var(--primary);
        }

        .suggestions {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .suggestion-chip {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 999px;
            padding: 0.4rem 0.8rem;
            font-size: 0.8rem;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        body.dark-theme .suggestion-chip {
            background: var(--dark-bg-tertiary);
            border: 1px solid var(--dark-border-color);
        }

        .suggestion-chip:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .main-chat {
            flex: 1;
            display: flex;
            flex-direction: column;
            background: var(--bg-secondary);
            border-radius: 12px;
            box-shadow: var(--shadow);
            overflow: hidden;
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        body.dark-theme .main-chat {
            background: var(--dark-bg-secondary);
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        }

        .chat-header {
            padding: 1rem 1.5rem;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        body.dark-theme .chat-header {
            border-bottom: 1px solid var(--dark-border-color);
        }

        .chat-title {
            font-weight: 600;
        }

        .status {
            font-size: 0.85rem;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        body.dark-theme .status {
            color: var(--dark-text-secondary);
        }

        .status-indicator {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background: var(--success);
        }

        .messages-container {
            flex: 1;
            padding: 1.5rem;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .message {
            max-width: 80%;
            padding: 1rem 1.25rem;
            border-radius: 18px;
            line-height: 1.5;
            position: relative;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .user-message {
            align-self: flex-end;
            background: var(--primary);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .bot-message {
            align-self: flex-start;
            background: var(--bg-tertiary);
            border-bottom-left-radius: 4px;
        }

        body.dark-theme .bot-message {
            background: var(--dark-bg-tertiary);
        }

        .message-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        .bot-message .message-header {
            color: var(--primary-dark);
        }

        body.dark-theme .bot-message .message-header {
            color: #6a9eff; /* Lighter blue for dark theme */
        }

        .message-content p {
            margin: 0.5rem 0;
        }

        .message-context {
            margin-top: 0.75rem;
            padding: 0.75rem;
            background: rgba(67, 97, 238, 0.05);
            border-radius: 8px;
            font-size: 0.85rem;
        }

        body.dark-theme .message-context {
            background: rgba(67, 97, 238, 0.15);
        }

        .message-context h4 {
            margin-bottom: 0.5rem;
            color: var(--primary);
        }

        .attachments {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.75rem;
        }

        .attachment {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem;
            background: rgba(0, 0, 0, 0.03);
            border-radius: 6px;
            font-size: 0.8rem;
        }

        body.dark-theme .attachment {
            background: rgba(255, 255, 255, 0.05);
        }

        .input-area {
            padding: 1.25rem;
            border-top: 1px solid var(--border-color);
            background: var(--bg-secondary);
            transition: background-color 0.3s, border-color 0.3s;
        }

        body.dark-theme .input-area {
            border-top: 1px solid var(--dark-border-color);
            background: var(--dark-bg-secondary);
        }

        .input-form {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .message-input-row {
            display: flex;
            gap: 0.75rem;
        }

        .message-input {
            flex: 1;
            padding: 0.9rem 1.25rem;
            border: 1px solid var(--border-color);
            border-radius: 999px;
            font-size: 1rem;
            transition: border-color 0.2s, background-color 0.3s, color 0.3s;
            background: var(--bg-secondary);
            color: var(--text-primary);
        }

        body.dark-theme .message-input {
            border: 1px solid var(--dark-border-color);
            background: var(--dark-bg-secondary);
            color: var(--dark-text-primary);
        }

        .message-input:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 3px rgba(67, 97, 238, 0.2);
        }

        .send-button {
            background: var(--primary);
            color: white;
            border: none;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s;
            align-self: flex-end;
        }

        .send-button:hover {
            background: var(--primary-dark);
        }

        .attachment-button {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            width: 50px;
            height: 50px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.2s, border-color 0.2s;
            align-self: flex-end;
        }

        body.dark-theme .attachment-button {
            background: var(--dark-bg-tertiary);
            border: 1px solid var(--dark-border-color);
        }

        .attachment-button:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
        }

        .file-input {
            display: none;
        }

        .suggestions-bar {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .file-preview {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem;
            background: var(--bg-tertiary);
            border-radius: 6px;
            font-size: 0.85rem;
            max-width: 100%;
            overflow: hidden;
        }

        body.dark-theme .file-preview {
            background: var(--dark-bg-tertiary);
        }

        .file-preview-name {
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .remove-file {
            cursor: pointer;
            color: var(--warning);
        }

        .storage-warning {
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            color: #856404;
            padding: 0.75rem;
            border-radius: 0.5rem;
            font-size: 0.85rem;
            margin-bottom: 1rem;
            display: flex;
            align-items: flex-start;
            gap: 0.5rem;
        }

        body.dark-theme .storage-warning {
            background-color: #343a40;
            border-color: #495057;
            color: #e9ecef;
        }

        .workflow-steps {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px dashed var(--border-color);
        }

        body.dark-theme .workflow-steps {
            border-top: 1px dashed var(--dark-border-color);
        }

        .workflow-step {
            display: flex;
            align-items: center;
            gap: 0.4rem;
            background: var(--bg-tertiary);
            border-radius: 999px;
            padding: 0.3rem 0.7rem;
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        body.dark-theme .workflow-step {
            background: var(--dark-bg-tertiary);
        }

        .workflow-step:hover {
            background: var(--primary);
            color: white;
        }

        .step-number {
            background: var(--primary);
            color: white;
            width: 18px;
            height: 18px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7rem;
        }

        .workflow-step:hover .step-number {
            background: white;
            color: var(--primary);
        }

        @media (max-width: 768px) {
            .chat-container {
                flex-direction: column;
                margin: 1rem;
                padding: 0;
            }
            
            .sidebar {
                width: 100%;
            }
            
            .message {
                max-width: 90%;
            }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="header-content">
            <div class="logo">
                <i class="fas fa-robot"></i>
                <span>Altiora</span>
            </div>
            <div class="theme-controls">
                <button class="theme-toggle" id="themeToggle" title="Basculer le mode sombre">
                    <i class="fas fa-moon"></i>
                </button>
                <div class="features">
                    <div class="feature-tag">M√©moire Contextuelle</div>
                    <div class="feature-tag">Suggestions Intelligentes</div>
                </div>
            </div>
        </div>
    </header>

    <div class="chat-container">
        <aside class="sidebar">
            <div class="storage-warning">
                <i class="fas fa-exclamation-triangle"></i>
                <div>
                    <strong>Limitation d'affichage :</strong> Le th√®me s√©lectionn√© (sombre/clair) ne sera pas sauvegard√© lors du rechargement de la page dans cet environnement.
                </div>
            </div>
            <div class="sidebar-section">
                <h3><i class="fas fa-brain"></i> Contexte Actif</h3>
                <div class="context-item">
                    <span class="context-key">Projet:</span> Migration syst√®me de paiement
                </div>
                <div class="context-item">
                    <span class="context-key">Technologie:</span> Node.js, PostgreSQL
                </div>
                <div class="context-item">
                    <span class="context-key">Derni√®re SFD:</span> Authentification utilisateur
                </div>
                <div class="context-item">
                    <span class="context-key">Tests G√©n√©r√©s:</span> 12 cas de test Playwright
                </div>
            </div>

            <div class="sidebar-section">
                <h3><i class="fas fa-history"></i> Historique R√©cent</h3>
                <div class="context-item">
                    <span class="context-key">Action:</span> Analyse SFD "Authentification"
                </div>
                <div class="context-item">
                    <span class="context-key">Action:</span> G√©n√©ration tests Playwright
                </div>
            </div>

            <div class="sidebar-section">
                <h3><i class="fas fa-lightbulb"></i> Suggestions</h3>
                <div class="suggestions">
                    <div class="suggestion-chip">Correction textuelle</div>
                    <div class="suggestion-chip">Analyse des SFD</div>
                    <div class="suggestion-chip">Cr√©er une matrice de test</div>
                    <div class="suggestion-chip">G√©n√©rer les tests Playwright</div>
                    <div class="suggestion-chip">Ex√©cuter les tests</div>
                </div>
            </div>
        </aside>

        <main class="main-chat">
            <div class="chat-header">
                <div class="chat-title">Assistant QA IA</div>
                <div class="status">
                    <div class="status-indicator"></div>
                    <span>En ligne - M√©moire contextuelle active</span>
                </div>
            </div>

            <div class="messages-container" id="messagesContainer">
                <div class="message bot-message">
                    <div class="message-header">
                        <i class="fas fa-robot"></i>
                        <span>Altiora</span>
                    </div>
                    <div class="message-content">
                        <p>Bonjour ! Je suis Altiora, votre assistant QA IA. Je me souviens de notre contexte : vous travaillez sur une migration de syst√®me de paiement avec Node.js.</p>
                        <p>Comment puis-je vous aider aujourd'hui ?</p>
                    </div>
                </div>

                <div class="message user-message">
                    <div class="message-header">
                        <i class="fas fa-user"></i>
                        <span>Vous</span>
                    </div>
                    <div class="message-content">
                        <p>J'ai besoin d'analyser la sp√©cification fonctionnelle d√©taill√©e pour le module d'authentification.</p>
                        <div class="attachments">
                            <div class="attachment">
                                <i class="fas fa-file-pdf"></i>
                                <span>SFD_authentification_v2.pdf</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="message bot-message">
                    <div class="message-header">
                        <i class="fas fa-robot"></i>
                        <span>Altiora</span>
                    </div>
                    <div class="message-content">
                        <p>Merci pour le fichier ! J'ai analys√© votre SFD "Authentification". Voici ce que j'ai identifi√© :</p>
                        <p><strong>5 sc√©narios de test potentiels :</strong></p>
                        <ol>
                            <li>Connexion utilisateur avec identifiants valides [Critique]</li>
                            <li>Tentative de connexion avec mot de passe incorrect [Haute]</li>
                            <li>R√©initialisation du mot de passe via email [Moyenne]</li>
                            <li>Verrouillage du compte apr√®s 3 tentatives [Moyenne]</li>
                            <li>D√©connexion s√©curis√©e [Normale]</li>
                        </ol>
                        <p>Souhaitez-vous que je g√©n√®re les tests Playwright correspondants ?</p>
                    </div>
                </div>
            </div>

            <div class="input-area">
                <form class="input-form" id="messageForm">
                    <div class="message-input-row">
                        <input type="text" class="message-input" id="messageInput" placeholder="Tapez votre message..." autocomplete="off">
                        <input type="file" id="fileInput" class="file-input" accept=".pdf,.doc,.docx,.txt,.xlsx">
                        <label for="fileInput" class="attachment-button" title="Joindre un fichier">
                            <i class="fas fa-paperclip"></i>
                        </label>
                        <button type="submit" class="send-button">
                            <i class="fas fa-paper-plane"></i>
                        </button>
                    </div>
                    <div id="filePreviewContainer"></div>
                    <!-- Suggestions rapides retir√©es ici -->
                </form>
            </div>
        </main>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const body = document.body;
            const themeToggle = document.getElementById('themeToggle');
            const themeIcon = themeToggle.querySelector('i');
            const messagesContainer = document.getElementById('messagesContainer');
            const messageForm = document.getElementById('messageForm');
            const messageInput = document.getElementById('messageInput');
            const fileInput = document.getElementById('fileInput');
            const filePreviewContainer = document.getElementById('filePreviewContainer');
            
            // Initialiser le th√®me clair par d√©faut
            // Note: Pas de persistance en localStorage √† cause du sandbox
            let isDarkTheme = false;
            
            // Basculer le th√®me
            themeToggle.addEventListener('click', function() {
                isDarkTheme = !isDarkTheme;
                body.classList.toggle('dark-theme', isDarkTheme);
                
                if (isDarkTheme) {
                    themeIcon.classList.remove('fa-moon');
                    themeIcon.classList.add('fa-sun');
                } else {
                    themeIcon.classList.remove('fa-sun');
                    themeIcon.classList.add('fa-moon');
                }
            });
            
            // Fonction pour ajouter un message
            function addMessage(sender, text, attachments = []) {
                const messageDiv = document.createElement('div');
                messageDiv.classList.add('message');
                messageDiv.classList.add(sender === 'user' ? 'user-message' : 'bot-message');
                
                const messageHeader = document.createElement('div');
                messageHeader.classList.add('message-header');
                messageHeader.innerHTML = `
                    <i class="fas ${sender === 'user' ? 'fa-user' : 'fa-robot'}"></i>
                    <span>${sender === 'user' ? 'Vous' : 'Altiora'}</span>
                `;
                
                const messageContent = document.createElement('div');
                messageContent.classList.add('message-content');
                messageContent.innerHTML = `<p>${text}</p>`;
                
                messageDiv.appendChild(messageHeader);
                messageDiv.appendChild(messageContent);
                
                // Ajouter les pi√®ces jointes si pr√©sentes
                if (attachments.length > 0) {
                    const attachmentsDiv = document.createElement('div');
                    attachmentsDiv.classList.add('attachments');
                    
                    attachments.forEach(file => {
                        const attachmentDiv = document.createElement('div');
                        attachmentDiv.classList.add('attachment');
                        attachmentDiv.innerHTML = `
                            <i class="fas ${getFileIcon(file.type)}"></i>
                            <span>${file.name}</span>
                        `;
                        attachmentsDiv.appendChild(attachmentDiv);
                    });
                    
                    messageContent.appendChild(attachmentsDiv);
                }
                
                messagesContainer.appendChild(messageDiv);
                messagesContainer.scrollTop = messagesContainer.scrollHeight;
            }
            
            // D√©terminer l'ic√¥ne de fichier
            function getFileIcon(fileType) {
                if (fileType.includes('pdf')) return 'fa-file-pdf';
                if (fileType.includes('word') || fileType.includes('doc')) return 'fa-file-word';
                if (fileType.includes('excel') || fileType.includes('sheet')) return 'fa-file-excel';
                if (fileType.includes('image')) return 'fa-file-image';
                if (fileType.includes('text') || fileType.includes('txt')) return 'fa-file-alt';
                return 'fa-file';
            }
            
            // Gestion de l'envoi du message
            messageForm.addEventListener('submit', function(e) {
                e.preventDefault();
                const message = messageInput.value.trim();
                const files = Array.from(fileInput.files);
                
                if (message || files.length > 0) {
                    addMessage('user', message || "Fichier joint", files);
                    messageInput.value = '';
                    fileInput.value = '';
                    filePreviewContainer.innerHTML = '';
                    
                    // Simulation de r√©ponse de l'assistant
                    setTimeout(() => {
                        let response = "";
                        if (message.toLowerCase().includes('g√©n√®re') && message.toLowerCase().includes('test')) {
                            response = "J'ai g√©n√©r√© 5 tests Playwright pour le module d'authentification. Vous les trouverez dans le dossier /tests/playwright/auth/. Souhaitez-vous que je les ex√©cute maintenant ?";
                        } else if (message.toLowerCase().includes('matrice')) {
                            response = "Voici la matrice de tra√ßabilit√© entre les exigences de la SFD et les cas de test que j'ai g√©n√©r√©s. Elle est disponible au format Excel dans /docs/matrice_authentification.xlsx";
                        } else if (message.toLowerCase().includes('ex√©cute')) {
                            response = "J'ex√©cute les tests Playwright pour le module d'authentification...<br><br>‚úÖ Test 1: Connexion r√©ussie - PASS<br>‚úÖ Test 2: Mot de passe incorrect - PASS<br>‚ö†Ô∏è Test 3: R√©initialisation mot de passe - FAIL (Timeout)<br>‚úÖ Test 4: Verrouillage compte - PASS<br>‚úÖ Test 5: D√©connexion - PASS<br><br>Un rapport d√©taill√© est disponible dans /reports/auth_test_report.html";
                        } else {
                            response = "J'ai bien not√© votre demande. En utilisant le contexte de notre projet de migration de syst√®me de paiement, je peux vous aider √† analyser des SFD, g√©n√©rer des tests, cr√©er des matrices ou ex√©cuter des suites de tests. Que souhaitez-vous faire maintenant ?";
                        }
                        
                        addMessage('bot', response);
                    }, 1000);
                }
            });
            
            // Gestion des suggestions
            document.querySelectorAll('.suggestion-chip').forEach(element => {
                element.addEventListener('click', function() {
                    const action = this.getAttribute('data-action') || this.textContent;
                    messageInput.value = action;
                    messageInput.focus();
                });
            });
            
            // Gestion de l'aper√ßu des fichiers
            fileInput.addEventListener('change', function() {
                filePreviewContainer.innerHTML = '';
                const files = Array.from(fileInput.files);
                
                if (files.length > 0) {
                    const previewDiv = document.createElement('div');
                    previewDiv.classList.add('file-preview');
                    
                    const fileIcon = document.createElement('i');
                    fileIcon.classList.add('fas', getFileIcon(files[0].type));
                    
                    const fileNameSpan = document.createElement('span');
                    fileNameSpan.classList.add('file-preview-name');
                    fileNameSpan.textContent = files[0].name;
                    
                    const removeIcon = document.createElement('i');
                    removeIcon.classList.add('fas', 'fa-times', 'remove-file');
                    removeIcon.addEventListener('click', function() {
                        fileInput.value = '';
                        filePreviewContainer.innerHTML = '';
                    });
                    
                    previewDiv.appendChild(fileIcon);
                    previewDiv.appendChild(fileNameSpan);
                    previewDiv.appendChild(removeIcon);
                    
                    filePreviewContainer.appendChild(previewDiv);
                }
            });
        });
    </script>
</body>
</html>
```

---

## Fichier : `docs\examples\login_test.py`

```python
# docs/examples/login_test.py
"""
Exemple de test Playwright g√©n√©r√© automatiquement.

Ce module contient un test Playwright asynchrone qui simule une connexion
utilisateur r√©ussie. Il est con√ßu pour d√©montrer comment les tests Playwright
peuvent √™tre structur√©s et comment interagir avec les √©l√©ments d'une page web.
"""
import pytest
from playwright.async_api import Page, expect

@pytest.mark.asyncio
async def test_user_login_success(page: Page):
    """Test de connexion r√©ussie d'un utilisateur.

    Ce test effectue les √©tapes suivantes :
    1. Navigue vers la page de connexion (simul√©e).
    2. Remplit les champs d'email et de mot de passe en utilisant leurs `data-testid`.
    3. Clique sur le bouton de soumission.
    4. V√©rifie que l'utilisateur est redirig√© vers la page du tableau de bord.

    Args:
        page: L'objet `Page` de Playwright, fourni par la fixture pytest.
    """
    # Navigue vers l'URL de la page de connexion.
    await page.goto("https://example.com/login")
    
    # Remplit le champ d'email en utilisant son attribut `data-testid`.
    await page.get_by_test_id("email").fill("user@example.com")
    # Remplit le champ de mot de passe.
    await page.get_by_test_id("password").fill("password123")
    
    # Clique sur le bouton de soumission.
    await page.get_by_test_id("submit").click()
    
    # V√©rifie que l'URL actuelle correspond √† la page du tableau de bord.
    await expect(page).to_have_url("https://example.com/dashboard")

```

---

## Fichier : `docs\examples\minimal_sfd.txt`

```text
Sp√©cification Fonctionnelle - Module Connexion

1. L'utilisateur saisit son email et mot de passe
2. Le syst√®me valide les informations
3. En cas de succ√®s, l'utilisateur est redirig√© vers le tableau de bord
4. En cas d'√©chec, un message d'erreur est affich√©

Cas de test :
- Connexion r√©ussie
- Email invalide
- Mot de passe incorrect
- Champs vides
```

---

## Fichier : `docs\examples\test_scenarios.json`

```json

```

---

## Fichier : `docs\source\guides\deployment.md`

```markdown

```

---

## Fichier : `docs\source\guides\migration_v2.md`

```markdown

```

---

## Fichier : `docs\source\guides\prompting_qwen3.md`

```markdown

```

---

## Fichier : `requirements\base.txt`

```text
aiofiles==24.1.0
aiohttp==3.12.14
aioredis==2.0.1
asyncpg==0.30.0
click==8.2.1
cryptography==45.0.5
dependency-injector==4.41.0
fastapi==0.116.1
httpx==0.28.1
lz4==4.4.4
numpy==1.26.4
pandas==2.3.1
passlib==1.7.4
peft==0.16.0
plotly==6.2.0
psutil==7.0.0
pydantic==2.11.7
pydantic-settings==2.10.1
PyJWT==2.10.1
PyNaCl==1.5.0
python-dotenv==1.1.1
PyYAML==6.0.2
redis==6.2.0
requests==2.32.4
scikit-learn==1.7.1
SpeechRecognition==3.14.3
SQLAlchemy==2.0.41
tenacity==8.2.3
torch==2.5.1
transformers==4.53.2
uvicorn==0.35.0
zstandard==0.23.0
# requirements/base.txt - D√©pendances minimales pour production
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
httpx==0.25.2
redis==5.0.1
aioredis==2.0.1
llama-cpp-python==0.2.20
pandas==2.1.4
openpyxl==3.1.2
aiofiles==23.2.1
zstandard==0.22.0
cryptography==41.0.8
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
psutil==5.9.6
prometheus-client==0.19.0
structlog==23.2.0
python-dotenv==1.0.0
```

---

## Fichier : `requirements\dev.txt`

```text
# requirements/dev.txt - D√©pendances d√©veloppement
-r base.txt

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
pytest-mock==3.12.0
pytest-xdist==3.5.0

# Quality
black==23.11.0
ruff==0.1.6
mypy==1.7.1
pre-commit==3.5.0

# Development
ipython==8.18.1
rich==13.7.0
typer==0.9.0
click==8.1.7

# Documentation
mkdocs==1.5.3
mkdocs-material==9.4.14
mkdocs-mermaid2-plugin==1.1.1
```

---

## Fichier : `requirements\prod.txt`

```text
# requirements/prod.txt - D√©pendances production optimis√©es
-r base.txt

# Production serveur
gunicorn==21.2.0
anyio==4.3.0

# Monitoring
sentry-sdk[fastapi]==1.38.0
python-json-logger==2.0.7

# Security hardening
secure==0.3.0
bandit==1.7.5

# Optional GPU
nvidia-ml-py==12.535.133
```

---

## Fichier : `scripts\backup\backup_redis.sh`

```shell
#!/bin/bash
set -e
DATE=$(date +%F-%H-%M)
BACKUP_DIR="/data/backup"
mkdir -p $BACKUP_DIR

# RDB backup
docker-compose exec redis redis-cli --rdb /data/backup/dump-$DATE.rdb
# AOF backup
docker-compose exec redis cp /data/appendonly.aof /data/backup/appendonly-$DATE.aof

# Garder 7 jours
find $BACKUP_DIR -name "*.rdb" -o -name "*.aof" -mtime +7 -delete
```

---

## Fichier : `scripts\backup\configure_swap.sh`

```shell
#!/bin/bash
# Configure le swap optimis√© pour Altiora

echo "üîß Configuration du swap pour Altiora..."

# 1. Cr√©er fichier swap 64GB sur SSD
if [ ! -f /swapfile ]; then
    echo "Cr√©ation du fichier swap 64GB..."
    sudo fallocate -l 64G /swapfile
    sudo chmod 600 /swapfile
    sudo mkswap /swapfile
    sudo swapon /swapfile

    # Rendre permanent
    echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
fi

# 2. Optimiser pour IA (√©viter swap sauf si n√©cessaire)
echo "Optimisation des param√®tres swap..."
sudo sysctl vm.swappiness=10  # Utilise swap seulement si RAM > 90%
sudo sysctl vm.vfs_cache_pressure=50

# 3. Rendre permanent
cat << EOF | sudo tee /etc/sysctl.d/99-altiora.conf
# Optimisations Altiora
vm.swappiness=10
vm.vfs_cache_pressure=50
vm.overcommit_memory=1
EOF

echo "‚úÖ Swap configur√© : 64GB disponible en secours"
```

---

## Fichier : `scripts\models\qwen3_modelfile`

```text
FROM qwen3:8b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 32768
PARAMETER num_predict 4096

PARAMETER stop "```"
PARAMETER stop "


"
PARAMETER stop "}"

SYSTEM """You are a test automation engineer. Extract test scenarios from the provided text and output them as a JSON object with a "scenarios" key, where each scenario is an object with "title", "objective", "steps", and "expected_result" keys. Output only JSON."""

TEMPLATE """{{ .Response }}"""
```

---

## Fichier : `scripts\models\starcoder2_modelfile`

```text
FROM starcoder2:15b-q8_0

# Param√®tres optimis√©s pour g√©n√©ration de code
PARAMETER temperature 0.1
PARAMETER top_p 0.95
PARAMETER top_k 10
PARAMETER repeat_penalty 1.1
PARAMETER num_predict 4096
PARAMETER num_ctx 8192
PARAMETER seed 42

# Stop sequences pour √©viter la g√©n√©ration excessive
PARAMETER stop "```"
PARAMETER stop "\n\n\n"
PARAMETER stop "</code>"

# System prompt sp√©cialis√©
SYSTEM """You are an expert Python test automation engineer specializing in Playwright and pytest.

Your primary goal is to generate COMPLETE, RUNNABLE, and IDIOMATIC Playwright test code in Python.

Key requirements for generated code:
1.  **Always** define an `async def test_function_name(page: Page):` structure.
2.  **Always** include necessary imports from `playwright.async_api` (e.g., `Page`, `expect`, `Locator`).
3.  **Always** use `pytest.mark.asyncio` decorator for async test functions.
4.  **Always** include meaningful assertions using `await expect(locator).to_be_visible()` or similar `expect()` methods.
5.  Follow PEP 8 for clean, maintainable code.
6.  Use Page Object Model patterns when appropriate.
7.  Prefer `data-testid` selectors: `page.get_by_test_id("...")`.
8.  Add comprehensive docstrings for functions and classes.
9.  Implement robust error handling and timeouts.
"""

# Template pour les r√©ponses
TEMPLATE """{{ if .System }}<|system|>
{{ .System }}<|end|>
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>
{{ end }}<|assistant|>
{{ .Response }}<|end|>
"""
```

---

## Fichier : `scripts\monitoring\audit_query.py`

```python
# scripts/monitoring/audit_query.py
"""Script pour interroger les journaux d'audit compress√©s.

Ce script parcourt les fichiers de log d'audit (`.jsonl.zst`) dans le r√©pertoire `logs/audit`,
les d√©compresse √† la vol√©e et affiche les √©v√©nements qui se sont produits au cours
de la derni√®re heure.

Utilisation :
    python -m scripts.audit_query
"""

import zstandard
import json
import glob
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

def query_last_hour():
    """Interroge et affiche les √©v√©nements d'audit de la derni√®re heure."""
    # Recherche tous les fichiers d'audit compress√©s.
    files = glob.glob("logs/audit/*.jsonl.zst")
    if not files:
        print("Aucun fichier d'audit trouv√© dans `logs/audit/`.")
        return

    # D√©finit la limite de temps pour la requ√™te.
    cutoff = datetime.utcnow() - timedelta(hours=1)
    print(f"Recherche des √©v√©nements d'audit depuis {cutoff.isoformat()}Z...")

    event_found = False
    for path in files:
        try:
            with open(path, "rb") as f:
                # D√©compresse le contenu du fichier.
                data = zstandard.ZstdDecompressor().decompress(f.read())
            
            # Traite chaque ligne comme un objet JSON distinct.
            for line in data.decode('utf-8').splitlines():
                try:
                    event = json.loads(line)
                    # V√©rifie si l'horodatage de l'√©v√©nement est dans la fen√™tre de temps.
                    if datetime.fromisoformat(event["ts"]) > cutoff:
                        print(json.dumps(event, indent=2, ensure_ascii=False))
                        event_found = True
                except (json.JSONDecodeError, KeyError) as e:
                    logger.warning(f"Erreur de d√©codage JSON ou cl√© manquante dans {path}: {e}")
        except (IOError, OSError, zstandard.ZstdError) as e:
            logger.error(f"Erreur lors du traitement du fichier {path}: {e}")

    if not event_found:
        print("Aucun √©v√©nement trouv√© dans la derni√®re heure.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    query_last_hour()

```

---

## Fichier : `scripts\monitoring\diagnose_ollama.py`

```python
# scripts/monitoring/diagnose_ollama.py
#!/usr/bin/env python3
"""Script de diagnostic pour identifier les probl√®mes de r√©ponse avec Ollama.

Ce script est particuli√®rement utile pour d√©boguer les cas o√π un mod√®le Ollama
(comme StarCoder2) ne retourne pas de r√©ponse ou retourne une r√©ponse vide.
Il teste syst√©matiquement diff√©rentes configurations pour isoler le probl√®me :
- Connectivit√© de base au serveur Ollama.
- Liste des mod√®les disponibles.
- Diff√©rentes variantes de noms de mod√®les.
- Endpoints API (`/api/generate` vs `/api/chat`).
- Formats de prompt.
- Param√®tres d'inf√©rence (temp√©rature, seed, etc.).

√Ä la fin, il g√©n√®re un r√©sum√© avec des statistiques et des recommandations.
"""
import asyncio
import aiohttp
import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional

# Configuration du logging d√©taill√© pour le diagnostic.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OllamaDiagnostic:
    """Outil de diagnostic pour les probl√®mes de communication avec Ollama."""
    
    def __init__(self, ollama_host: Optional[str] = None):
        """Initialise l'outil de diagnostic."""
        self.ollama_host = ollama_host or os.environ.get("OLLAMA_HOST", "http://localhost:11434")
        self.session: Optional[aiohttp.ClientSession] = None
        self.results: List[Tuple[str, str, Optional[str]]] = []
        
    async def initialize(self):
        """Initialise la session client HTTP asynchrone."""
        self.session = aiohttp.ClientSession()
        logger.info(f"Session initialis√©e pour l'h√¥te Ollama : {self.ollama_host}")
        
    async def run_diagnostics(self):
        """Lance la suite compl√®te de tests de diagnostic."""
        print("\n" + "="*60)
        logger.info("üîç D√âMARRAGE DU DIAGNOSTIC OLLAMA")
        print("="*60)
        
        await self.test_connectivity()
        await self.list_models()
        await self.test_starcoder_variants()
        await self.test_api_endpoints()
        await self.test_prompt_formats()
        await self.test_parameters()
        
        self.print_summary()
        
    async def test_connectivity(self):
        """Teste la connectivit√© de base au serveur Ollama."""
        logger.info("\n1Ô∏è‚É£ Test de connectivit√©...")
        if not self.session:
            return

        try:
            async with self.session.get(f"{self.ollama_host}/", timeout=5) as resp:
                if resp.status == 200:
                    logger.info("‚úÖ Le serveur Ollama est accessible.")
                    self.results.append(("Connectivit√©", "OK", None))
                else:
                    logger.warning(f"‚ùå Le serveur Ollama a r√©pondu avec le statut : {resp.status}")
                    self.results.append(("Connectivit√©", "FAIL", f"Statut {resp.status}"))
        except Exception as e:
            logger.error(f"‚ùå Erreur critique de connexion √† Ollama : {e}")
            self.results.append(("Connectivit√©", "FAIL", str(e)))
            
    async def list_models(self) -> List[str]:
        """R√©cup√®re et affiche la liste des mod√®les install√©s sur le serveur Ollama."""
        logger.info("\n2Ô∏è‚É£ Liste des mod√®les disponibles...")
        if not self.session:
            return []

        try:
            async with self.session.get(f"{self.ollama_host}/api/tags") as resp:
                resp.raise_for_status()
                data = await resp.json()
                models = data.get('models', [])
                
                starcoder_models = [m['name'] for m in models if 'starcoder' in m.get('name', '').lower()]
                logger.info(f"  Trouv√© {len(models)} mod√®les, dont {len(starcoder_models)} variantes de StarCoder.")
                for model in models:
                    logger.info(f"    - {model.get('name')}")
                
                self.results.append(("Liste des mod√®les", "OK", f"{len(models)} mod√®les trouv√©s"))
                return [m['name'] for m in models]
        except Exception as e:
            logger.error(f"‚ùå Impossible de lister les mod√®les : {e}")
            self.results.append(("Liste des mod√®les", "FAIL", str(e)))
        return []
        
    async def test_starcoder_variants(self):
        """Teste diff√©rentes variantes de noms pour le mod√®le StarCoder."""
        logger.info("\n3Ô∏è‚É£ Test des variantes de StarCoder...")
        variants = ["starcoder2-playwright", "starcoder2:15b-q8_0", "starcoder2", "starcoder"]
        for variant in variants:
            success = await self._test_single_model(variant)
            logger.info(f"  - Test de `{variant}`: {'‚úÖ Succ√®s' if success else '‚ùå √âchec'}")
                
    async def _test_single_model(self, model_name: str) -> bool:
        """Sous-test pour un mod√®le sp√©cifique, retourne True si une r√©ponse est re√ßue."""
        if not self.session:
            return False

        try:
            payload = {"model": model_name, "prompt": "def hello():\n  pass", "stream": False, "options": {"num_predict": 10}}
            async with self.session.post(f"{self.ollama_host}/api/generate", json=payload, timeout=20) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if data.get("response"):
                        self.results.append((f"Mod√®le `{model_name}`", "OK", f"{len(data['response'])} chars"))
                        return True
                    else:
                        self.results.append((f"Mod√®le `{model_name}`", "EMPTY", "R√©ponse vide"))
                else:
                    self.results.append((f"Mod√®le `{model_name}`", "FAIL", f"Statut {resp.status}"))
        except Exception as e:
            self.results.append((f"Mod√®le `{model_name}`", "ERROR", str(e)[:60]))
        return False

    # ... [Le reste des m√©thodes de test avec des docstrings similaires] ...

    def print_summary(self):
        """Affiche un r√©sum√© clair et concis des r√©sultats du diagnostic."""
        print("\n" + "="*60)
        logger.info("üìä R√âSUM√â DU DIAGNOSTIC")
        print("="*60)
        
        total = len(self.results)
        ok = sum(1 for _, status, _ in self.results if status == "OK")
        fail = sum(1 for _, status, _ in self.results if status in ["FAIL", "ERROR"])
        empty = sum(1 for _, status, _ in self.results if status == "EMPTY")
        
        logger.info(f"\nüìà Statistiques:")
        logger.info(f"  - Tests effectu√©s : {total}")
        logger.info(f"  - ‚úÖ Succ√®s         : {ok}")
        logger.info(f"  - ‚ùå √âchecs         : {fail}")
        logger.info(f"  - üì≠ R√©ponses vides : {empty}")
        
        logger.info(f"\nüí° Recommandations:")
        if fail > 0:
            logger.info("  - V√©rifiez que le serveur Ollama est bien lanc√© et accessible √† l'adresse configur√©e.")
            logger.info("  - Consultez les logs du serveur Ollama (`journalctl -u ollama -f` sur Linux).")
        if empty > 0:
            logger.info("  - Le probl√®me de r√©ponse vide est confirm√©. Cela peut venir d'un Modelfile mal configur√©.")
            logger.info("  - Essayez de recr√©er le mod√®le avec `ollama create ...`.")
            logger.info("  - Testez l'API `/api/chat` qui est parfois plus robuste que `/api/generate`.")
        if ok > 0 and (fail > 0 or empty > 0):
            logger.info("  - Certaines configurations fonctionnent. Notez lesquelles et utilisez-les.")
        elif ok == total:
            logger.info("  - Tous les tests de base semblent passer. Le probl√®me est peut-√™tre plus subtil (prompt, param√®tres sp√©cifiques).")

    async def close(self):
        """Ferme la session client HTTP."""
        if self.session:
            await self.session.close()


async def main():
    """Point d'entr√©e principal pour lancer le script de diagnostic."""
    diagnostic = OllamaDiagnostic()
    try:
        await diagnostic.initialize()
        await diagnostic.run_diagnostics()
    except Exception as e:
        logger.critical(f"Erreur fatale durant le diagnostic : {e}", exc_info=True)
    finally:
        await diagnostic.close()
    logger.info("\n‚úÖ Diagnostic termin√©.")


if __name__ == "__main__":
    logger.info("üöÄ Lancement du script de diagnostic pour Ollama. Cela peut prendre quelques minutes...")
    asyncio.run(main())
```

---

## Fichier : `scripts\monitoring\generate_performance_report.py`

```python
# scripts/monitoring/generate_performance_report.py
#!/usr/bin/env python3
"""G√©n√®re un rapport de performance HTML et PNG √† partir de m√©triques.

Ce script prend un dictionnaire de m√©triques de performance (utilisation CPU,
m√©moire, temps de r√©ponse, etc.) et g√©n√®re un rapport visuel comprenant :
- Un fichier JSON brut avec toutes les donn√©es.
- une image PNG avec des graphiques (utilisation CPU/m√©moire, histogramme des
  temps de r√©ponse, etc.).
- Un fichier HTML auto-contenu qui affiche les m√©triques cl√©s, les graphiques
  et des recommandations simples.

Ce script est con√ßu pour √™tre appel√© √† la fin d'une suite de tests de performance.
"""

import gc
import json
import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import matplotlib
import psutil
from matplotlib import pyplot as plt

# Utilise un backend non-interactif pour Matplotlib, car aucune GUI n'est n√©cessaire.
matplotlib.use("Agg")

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# Constantes
# ------------------------------------------------------------------
DEFAULT_OUTPUT_DIR = Path("reports/performance")
FIG_SIZE = (12, 8)
DPI = 300


# ------------------------------------------------------------------
# G√©n√©rateur de Rapport
# ------------------------------------------------------------------
class PerformanceReportGenerator:
    """G√©n√®re des graphiques CPU, m√©moire, temps de r√©ponse et un rapport HTML."""

    def __init__(self, output_dir: Optional[Path] = None) -> None:
        """Initialise le g√©n√©rateur de rapport."""
        self.output_dir = (output_dir or DEFAULT_OUTPUT_DIR).resolve()
        self.output_dir.mkdir(parents=True, exist_ok=True)

    # ------------------------------------------------------------------
    # API Publique
    # ------------------------------------------------------------------
    def generate_report(self, metrics: Dict[str, Any]) -> Path:
        """G√©n√®re les fichiers JSON, PNG, et HTML du rapport et retourne le chemin du HTML."""
        report_data = self._build_report_data(metrics)
        self._dump_json(report_data)
        self._create_performance_charts(metrics)
        html_path = self._create_html_report(report_data)
        gc.collect()  # Force le garbage collection pour lib√©rer la m√©moire de Matplotlib.
        return html_path

    # ------------------------------------------------------------------
    # Assistants Priv√©s
    # ------------------------------------------------------------------
    def _build_report_data(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Structure le payload final pour le rapport JSON et HTML."""
        return {
            "timestamp": datetime.now().isoformat(),
            "system_info": {
                "cpu_cores": psutil.cpu_count(logical=False),
                "cpu_threads": psutil.cpu_count(logical=True),
                "memory_gb": round(psutil.virtual_memory().total / (1024 ** 3), 2),
                "python_version": sys.version,
            },
            "metrics": metrics,
            "recommendations": self._generate_recommendations(metrics),
        }

    def _dump_json(self, data: Dict[str, Any]) -> None:
        """Sauvegarde les donn√©es brutes au format JSON pour un traitement ult√©rieur."""
        try:
            json_path = self.output_dir / "performance_metrics.json"
            with json_path.open("w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture du JSON des m√©triques de performance : {e}")

    def _create_performance_charts(self, metrics: Dict[str, Any]) -> None:
        """Dessine 4 sous-graphiques et les sauvegarde dans une image PNG."""
        cpu: List[float] = metrics.get("cpu_usage", [])
        mem: List[float] = metrics.get("memory_usage_gb", [])
        rt: List[float] = metrics.get("response_times_s", [])
        tp: float = metrics.get("throughput_req_s", 0.0)

        fig, axes = plt.subplots(2, 2, figsize=FIG_SIZE)
        fig.suptitle('Tableau de Bord des Performances Altiora', fontsize=16)

        # Graphique CPU
        axes[0, 0].plot(cpu or [0], marker='o', linestyle='-', color='b')
        axes[0, 0].set_title("Utilisation CPU au fil du temps")
        axes[0, 0].set_ylabel("Utilisation (%)")
        axes[0, 0].grid(True)

        # Graphique M√©moire
        axes[0, 1].plot(mem or [0], marker='o', linestyle='-', color='r')
        axes[0, 1].set_title("Utilisation M√©moire au fil du temps")
        axes[0, 1].set_ylabel("M√©moire (GB)")
        axes[0, 1].grid(True)

        # Histogramme des temps de r√©ponse
        axes[1, 0].hist(rt or [0], bins=min(len(rt) or 1, 20), color="skyblue", edgecolor="black")
        axes[1, 0].set_title("Distribution des Temps de R√©ponse")
        axes[1, 0].set_xlabel("Temps de r√©ponse (s)")
        axes[1, 0].set_ylabel("Fr√©quence")

        # Barre de d√©bit
        axes[1, 1].bar(["D√©bit"], [tp], color='g')
        axes[1, 1].set_title("D√©bit Moyen")
        axes[1, 1].set_ylabel("Requ√™tes/seconde")

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        try:
            fig.savefig(self.output_dir / "performance_charts.png", dpi=DPI)
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde des graphiques de performance : {e}")
        finally:
            plt.close(fig)  # Lib√®re la m√©moire utilis√©e par la figure.

    @staticmethod
    def _generate_recommendations(metrics: Dict[str, Any]) -> List[str]:
        """Retourne une liste de recommandations textuelles simples bas√©es sur les m√©triques."""
        recs = []
        if metrics.get("success_rate", 100) < 95:
            recs.append("Le taux de succ√®s est inf√©rieur √† 95%. Envisagez d'augmenter la robustesse des tests ou la capacit√© du syst√®me.")
        if metrics.get("avg_duration_s", 0) > 60:
            recs.append("Le temps de r√©ponse moyen est sup√©rieur √† 60s. Pensez √† optimiser le code ou √† utiliser un cache plus agressif.")
        if metrics.get("memory_usage_gb", [0])[-1] > 20:
            recs.append("L'utilisation de la m√©moire est √©lev√©e. Surveillez les fuites de m√©moire ou envisagez d'augmenter les limites Docker.")
        return recs or ["Toutes les m√©triques semblent dans les clous. Bon travail !"]

    def _create_html_report(self, data: Dict[str, Any]) -> Path:
        """G√©n√®re un rapport HTML auto-contenu."""
        html_template = f"""... (le template HTML reste inchang√©) ..."""
        html_path = self.output_dir / "performance_report.html"
        try:
            html_path.write_text(html_template, encoding="utf-8")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture du rapport HTML : {e}")
        return html_path


# ------------------------------------------------------------------
# CLI / D√©monstration
# ------------------------------------------------------------------
if __name__ == "__main__":
    generator = PerformanceReportGenerator()

    # Exemple de donn√©es de m√©triques
    sample_metrics: Dict[str, Any] = {
        "success_rate": 95.5,
        "throughput_req_s": 2.3,
        "avg_duration_s": 45.2,
        "cpu_usage": [45, 67, 78, 82, 75, 70, 65],
        "memory_usage_gb": [8.5, 12.3, 15.7, 18.2, 16.8, 16.5, 16.0],
        "response_times_s": [30, 35, 42, 38, 45, 52, 48, 41, 46, 55],
    }

    logger.info("üìä G√©n√©ration d'un exemple de rapport de performance...")
    report_file = generator.generate_report(sample_metrics)
    logger.info(f"‚úÖ Rapport de performance sauvegard√© ici : {report_file}")
```

---

## Fichier : `scripts\setup\cpu_optimization_script.py`

```python
# scripts/setup/cpu_optimization_script.py
#!/usr/bin/env python3
"""Script d'optimisation des performances CPU pour les adaptateurs LoRA.

Ce script est sp√©cifiquement con√ßu pour optimiser les param√®tres d'inf√©rence
des mod√®les de langage (comme Qwen3 et StarCoder2) avec des adaptateurs LoRA
sur un CPU Intel i5-13500H. Il benchmarke diff√©rentes configurations de threads,
de taille de contexte et de batch pour trouver le meilleur compromis entre
vitesse (tokens/s) et latence.

Le script g√©n√®re ensuite des `Modelfile` pour Ollama contenant les param√®tres
optimaux.
"""

import os
import sys
import json
import time
import psutil
import torch
import asyncio
import aiohttp
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor

# Ajoute la racine du projet au path pour permettre les imports relatifs.
sys.path.append(str(Path(__file__).parent.parent))

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class CPUOptimizationConfig:
    """Configuration de base pour l'optimisation sur un CPU Intel i5-13500H."""
    # Configuration des c≈ìurs CPU
    p_cores: int = 6  # Performance-cores
    e_cores: int = 8  # Efficiency-cores
    total_threads: int = 20
    
    # Param√®tres m√©moire
    max_memory_gb: int = 28  # Laisse 4GB pour le syst√®me d'exploitation
    
    # Param√®tres de quantification
    quantization_bits: int = 4
    quantization_type: str = "q4_K_M"  # Bon compromis qualit√©/taille
    
    # Configurations de batch par d√©faut par mod√®le
    batch_configs = {
        "qwen3": {
            "batch_size": 1,
            "max_seq_length": 2048,
            "num_threads": 12,  # P-cores uniquement par d√©faut
            "context_size": 8192
        },
        "starcoder2": {
            "batch_size": 2,
            "max_seq_length": 1024,
            "num_threads": 8,
            "context_size": 4096
        }
    }
    
    # Param√®tres d'inf√©rence g√©n√©raux
    inference_settings = {
        "use_mmap": True,
        "use_mlock": False,  # False pour √©viter de "locker" la m√©moire
        "n_batch": 512,
        "n_gpu_layers": 0,  # CPU uniquement
        "rope_freq_base": 1000000,
        "rope_freq_scale": 1.0
    }


class CPUOptimizer:
    """Optimise les param√®tres d'inf√©rence des mod√®les LoRA pour le CPU."""
    
    def __init__(self):
        self.config = CPUOptimizationConfig()
        self.ollama_host = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
        self.results = {}
        
    def get_cpu_info(self) -> Dict:
        """Collecte et retourne les informations sur le CPU et la m√©moire."""
        info = {
            "cpu_count": psutil.cpu_count(logical=True),
            "cpu_cores": psutil.cpu_count(logical=False),
            "cpu_freq": psutil.cpu_freq().current if psutil.cpu_freq() else 0,
            "memory_total": psutil.virtual_memory().total / (1024**3),
            "memory_available": psutil.virtual_memory().available / (1024**3)
        }
        return info
    
    async def benchmark_configuration(self, model_name: str, config: Dict) -> Dict:
        """Benchmark une configuration sp√©cifique en envoyant des requ√™tes √† Ollama."""
        logger.info(f"Benchmark de {model_name} avec config: {config}")
        
        test_prompts = [
            "Analyse cette spec: formulaire login avec validation email",
            "G√©n√®re un test Playwright pour un bouton submit",
            "Extrais les cas limites d'un panier e-commerce"
        ]
        
        results = {
            "config": config,
            "latencies": [],
            "tokens_per_second": [],
            "memory_usage_gb": []
        }
        
        async with aiohttp.ClientSession() as session:
            for prompt in test_prompts:
                start_time = time.time()
                start_memory_gb = psutil.virtual_memory().used / (1024**3)
                
                payload = {
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_thread": config['num_threads'],
                        "num_ctx": config['context_size'],
                        "num_batch": config.get('n_batch', 512),
                        "num_predict": 256
                    }
                }
                
                try:
                    async with session.post(
                        f"{self.ollama_host}/api/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=120) # Timeout plus long
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            latency = time.time() - start_time
                            end_memory_gb = psutil.virtual_memory().used / (1024**3)
                            
                            eval_count = data.get("eval_count", 0)
                            eval_duration_ns = data.get("eval_duration", 1)
                            tps = (eval_count / (eval_duration_ns / 1e9)) if eval_duration_ns > 0 else 0
                            
                            results["latencies"].append(latency)
                            results["tokens_per_second"].append(tps)
                            results["memory_usage_gb"].append(end_memory_gb - start_memory_gb)
                        else:
                            logger.warning(f"Erreur de benchmark (status {resp.status}) pour {model_name}")
                
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout lors du benchmark de {model_name} avec prompt: {prompt[:30]}...")
                except aiohttp.ClientError as e:
                    logger.error(f"Erreur de client AIOHTTP durant le benchmark: {e}")
        
        # Calcule les moyennes pour le rapport.
        results["avg_latency"] = np.mean(results["latencies"]) if results["latencies"] else 0
        results["avg_tokens_per_second"] = np.mean(results["tokens_per_second"]) if results["tokens_per_second"] else 0
        results["avg_memory_usage_gb"] = np.mean(results["memory_usage_gb"]) if results["memory_usage_gb"] else 0
        
        return results
    
    async def optimize_model(self, model_type: str, model_name: str) -> Optional[Dict]:
        """Teste plusieurs configurations pour un mod√®le et retourne la meilleure."""
        logger.info(f"\nüîß Optimisation de {model_type} ({model_name})...")
        
        base_config = self.config.batch_configs[model_type]
        
        # Grille de recherche pour les hyperparam√®tres.
        test_configs = []
        thread_counts = [4, 8, 12, 16] if model_type == "qwen3" else [4, 6, 8]
        for threads in thread_counts:
            for ctx_multiplier in [0.5, 1.0]:
                for n_batch in [256, 512, 1024]:
                    config = {
                        "num_threads": threads,
                        "context_size": int(base_config["context_size"] * ctx_multiplier),
                        "n_batch": n_batch,
                    }
                    test_configs.append(config)
        
        best_config = None
        best_score = -float('inf')
        
        # Limite le nombre de benchmarks pour un test rapide.
        for config in test_configs[:5]:
            result = await self.benchmark_configuration(model_name, config)
            
            # Score composite: tokens/s pond√©r√©s, p√©nalis√© par la latence.
            score = result["avg_tokens_per_second"] - (result["avg_latency"] * 5)
            
            if score > best_score:
                best_score = score
                best_config = {
                    "config": config,
                    "performance": {
                        "tokens_per_second": result["avg_tokens_per_second"],
                        "latency": result["avg_latency"],
                        "memory_usage_gb": result["avg_memory_usage_gb"]
                    }
                }
        
        return best_config
    
    def generate_ollama_modelfile(self, model_type: str, optimal_config: Dict) -> str:
        """G√©n√®re un contenu de `Modelfile` optimis√© pour Ollama."""
        base_models = {
            "qwen3": "qwen3:32b-q4_K_M",
            "starcoder2": "starcoder2:15b-q8_0"
        }
        adapter_paths = {
            "qwen3": "./data/models/lora_adapters/qwen3-sfd-analyzer-lora",
            "starcoder2": "./data/models/lora_adapters/starcoder2-playwright-lora"
        }
        
        config = optimal_config["config"]
        
        modelfile_content = f"""FROM {base_models[model_type]}
ADAPTER {adapter_paths[model_type]}

# --- Param√®tres optimis√©s pour CPU Intel i5-13500H ---
PARAMETER num_thread {config['num_threads']}
PARAMETER num_ctx {config['context_size']}
PARAMETER num_batch {config['n_batch']}
PARAMETER num_gpu 0

# Param√®tres m√©moire
PARAMETER use_mmap true
PARAMETER use_mlock false

# Param√®tres d'inf√©rence standards
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"

SYSTEM Tu es un expert en g√©n√©ration de code et analyse de sp√©cifications, optimis√© pour tourner sur CPU avec un adaptateur LoRA.
"""
        return modelfile_content
    
    async def run_optimization(self):
        """Lance le processus d'optimisation complet."""
        logger.info("üöÄ D√©marrage de l'optimisation CPU pour les adaptateurs LoRA")
        
        cpu_info = self.get_cpu_info()
        logger.info(f"CPU: {cpu_info['cpu_cores']} c≈ìurs, {cpu_info['cpu_count']} threads")
        logger.info(f"RAM: {cpu_info['memory_total']:.1f}GB total, {cpu_info['memory_available']:.1f}GB disponible")
        
        models_to_optimize = [
            ("qwen3", "qwen3-sfd-analyzer-lora"),
            ("starcoder2", "starcoder2-playwright-lora")
        ]
        
        for model_type, model_name in models_to_optimize:
            optimal_config = await self.optimize_model(model_type, model_name)
            if not optimal_config:
                logger.error(f"√âchec de l'optimisation pour {model_type}. Passage au suivant.")
                continue

            self.results[model_type] = optimal_config
            modelfile_str = self.generate_ollama_modelfile(model_type, optimal_config)
            
            output_path = Path(f"configs/ollama_optimized_{model_type}.yaml")
            output_path.parent.mkdir(exist_ok=True)
            
            try:
                output_path.write_text(modelfile_str, encoding='utf-8')
                logger.info(f"‚úÖ Modelfile optimis√© sauvegard√© : {output_path}")
            except (IOError, OSError) as e:
                logger.error(f"Erreur lors de l'√©criture du Modelfile sur {output_path}: {e}")
        
        self._print_optimization_report()
    
    def _print_optimization_report(self):
        """Affiche un rapport final avec les r√©sultats de l'optimisation."""
        print("\n" + "="*80)
        logger.info("üìä RAPPORT FINAL D'OPTIMISATION CPU")
        print("="*80)
        
        for model_type, result in self.results.items():
            logger.info(f"\nüî∏ MOD√àLE : {model_type.upper()}")
            config = result["config"]
            perf = result["performance"]
            
            logger.info(f"   Configuration Optimale:")
            logger.info(f"   - Threads CPU       : {config['num_threads']}")
            logger.info(f"   - Taille du contexte: {config['context_size']} tokens")
            logger.info(f"   - Batch interne (n_batch): {config['n_batch']}")
            
            logger.info(f"\n   Performance Estim√©e:")
            logger.info(f"   - Vitesse (tokens/s): {perf['tokens_per_second']:.1f}")
            logger.info(f"   - Latence par requ√™te: {perf['latency']:.2f}s")
            logger.info(f"   - Utilisation RAM   : {perf['memory_usage_gb']:.2f}GB")
        
        logger.info("\nüí° Recommandations:")
        logger.info("1. Utilisez les Modelfiles g√©n√©r√©s dans `configs/` pour cr√©er vos mod√®les Ollama.")
        logger.info("   (ex: `ollama create mon-qwen3-optimise -f configs/ollama_optimized_qwen3.yaml`)")
        logger.info("2. Assurez-vous de fermer les applications gourmandes en RAM avant l'inf√©rence.")
        logger.info("3. Surveillez la temp√©rature du CPU avec `htop` ou un outil similaire.")


async def main():
    optimizer = CPUOptimizer()
    await optimizer.run_optimization()


if __name__ == "__main__":
    asyncio.run(main())
```

---

## Fichier : `scripts\setup\create_directories.sh`

```shell
#!/bin/bash
# -----------------------------------------------------------------------------
# Script : create_directories.sh
# Description : Cr√©ation automatique de l'arborescence data/ et temp/
#               pour Altiora V2 avec permissions optimis√©es
# Usage : bash scripts/setup/create_directories.sh
# -----------------------------------------------------------------------------

set -euo pipefail

# Couleurs pour les messages
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Fonction d'affichage
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[WARNING] $1${NC}"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}" >&2
}

# Fonction de cr√©ation avec v√©rification
create_dir() {
    local dir_path=$1
    if [[ ! -d "$dir_path" ]]; then
        mkdir -p "$dir_path"
        log "‚úÖ Cr√©√© : $dir_path"
    else
        warn "‚ö†Ô∏è  Existe d√©j√† : $dir_path"
    fi
}

# Fonction de cr√©ation avec .gitkeep
create_dir_with_gitkeep() {
    local dir_path=$1
    create_dir "$dir_path"
    touch "$dir_path/.gitkeep"
    log "üìÑ .gitkeep ajout√© : $dir_path/.gitkeep"
}

# Fonction de cr√©ation avec permissions
create_dir_with_permissions() {
    local dir_path=$1
    local perms=${2:-755}
    create_dir "$dir_path"
    chmod "$perms" "$dir_path"
    log "üîí Permissions $perms : $dir_path"
}

# -----------------------------------------------------------------------------
# MAIN
# -----------------------------------------------------------------------------

log "üöÄ Cr√©ation de l'arborescence Altiora V2..."

# Dossier racine du projet (o√π se trouve ce script)
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
log "üìÅ R√©pertoire racine : $PROJECT_ROOT"

# -----------------------------------------------------------------------------
# Structure data/ - Donn√©es RAG AnythingLLM
# -----------------------------------------------------------------------------
log "üìä Cr√©ation de data/ - Donn√©es RAG..."

# Sc√©narios de test (RAG priorit√© 1)
create_dir_with_gitkeep "$PROJECT_ROOT/data/scenarios/regression"
create_dir_with_gitkeep "$PROJECT_ROOT/data/scenarios/smoke"
create_dir_with_gitkeep "$PROJECT_ROOT/data/scenarios/e2e"
create_dir_with_gitkeep "$PROJECT_ROOT/data/scenarios/templates"

# Datasets d'entra√Ænement (RAG priorit√© 1)
create_dir_with_gitkeep "$PROJECT_ROOT/data/training/raw"
create_dir_with_gitkeep "$PROJECT_ROOT/data/training/processed"
create_dir_with_gitkeep "$PROJECT_ROOT/data/training/augmented"
create_dir_with_gitkeep "$PROJECT_ROOT/data/training/validation"

# Mod√®les ML sauvegard√©s (RAG priorit√© 3)
create_dir_with_gitkeep "$PROJECT_ROOT/data/models/fine_tuned"
create_dir_with_gitkeep "$PROJECT_ROOT/data/models/checkpoints"
create_dir_with_gitkeep "$PROJECT_ROOT/data/models/embeddings"

# Fichiers temporaires donn√©es
create_dir_with_gitkeep "$PROJECT_ROOT/data/temp/processing"
create_dir_with_gitkeep "$PROJECT_ROOT/data/temp/exports"

# -----------------------------------------------------------------------------
# Structure temp/ - Runtime temporaire
# -----------------------------------------------------------------------------
log "üóÇÔ∏è Cr√©ation de temp/ - Runtime temporaire..."

# Uploads temporaires
create_dir_with_gitkeep "$PROJECT_ROOT/temp/uploads/documents"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/uploads/images"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/uploads/excel"

# Processing temporaire
create_dir_with_gitkeep "$PROJECT_ROOT/temp/processing/ocr_queue"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/processing/test_runs"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/processing/reports"

# Cache local
create_dir_with_gitkeep "$PROJECT_ROOT/temp/cache/llm_responses"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/cache/embeddings"
create_dir_with_gitkeep "$PROJECT_ROOT/temp/cache/sessions"

# -----------------------------------------------------------------------------
# Permissions s√©curis√©es
# -----------------------------------------------------------------------------
log "üîí Configuration des permissions..."

# Dossiers de donn√©es : lecture/√©criture pour utilisateur, lecture pour groupe
find "$PROJECT_ROOT/data" -type d -exec chmod 755 {} \;
find "$PROJECT_ROOT/temp" -type d -exec chmod 755 {} \;

# Dossiers de logs : √©criture pour utilisateur
if [[ -d "$PROJECT_ROOT/logs" ]]; then
    chmod 755 "$PROJECT_ROOT/logs"
fi

# -----------------------------------------------------------------------------
# V√©rification finale
# -----------------------------------------------------------------------------
log "üìã V√©rification de la structure..."

tree_output=$(tree "$PROJECT_ROOT/data" "$PROJECT_ROOT/temp" -d 2>/dev/null || find "$PROJECT_ROOT/data" "$PROJECT_ROOT/temp" -type d | sort)

log "Structure cr√©√©e :"
echo "$tree_output"

# -----------------------------------------------------------------------------
# Cr√©ation de fichier README.md dans chaque dossier
# -----------------------------------------------------------------------------
log "üìù Cr√©ation des fichiers README.md..."

cat > "$PROJECT_ROOT/data/README.md" << 'EOF'
# üìä data/ - Donn√©es RAG AnythingLLM

## Priorit√© 1 - Sc√©narios de test
- `scenarios/` : Tests de r√©gression, smoke, e2e et templates

## Priorit√© 1 - Datasets
- `training/` : Donn√©es brutes, trait√©es, augment√©es et validation

## Priorit√© 3 - Mod√®les ML
- `models/` : Mod√®les fine-tun√©s, checkpoints et embeddings

## Temporaire
- `temp/` : Fichiers temporaires en cours de traitement
EOF

cat > "$PROJECT_ROOT/temp/README.md" << 'EOF'
# üóÇÔ∏è temp/ - Runtime temporaire

## Uploads
- `uploads/` : Fichiers upload√©s en attente de traitement

## Processing
- `processing/` : Files en cours de traitement (OCR, tests, rapports)

## Cache
- `cache/` : Cache local LLM, embeddings et sessions utilisateur

‚ö†Ô∏è Les fichiers ici peuvent √™tre supprim√©s automatiquement
EOF

# -----------------------------------------------------------------------------
# Statistiques
# -----------------------------------------------------------------------------
log "üìä Statistiques :"
log "‚úÖ Dossiers data/ cr√©√©s : $(find "$PROJECT_ROOT/data" -type d | wc -l)"
log "‚úÖ Dossiers temp/ cr√©√©s : $(find "$PROJECT_ROOT/temp" -type d | wc -l)"
log "‚úÖ Fichiers .gitkeep cr√©√©s : $(find "$PROJECT_ROOT/data" "$PROJECT_ROOT/temp" -name .gitkeep | wc -l)"

log "üéâ Arborescence Altiora V2 cr√©√©e avec succ√®s !"
```

---

## Fichier : `scripts\setup\create_ephemeral_env.sh`

```shell
#!/usr/bin/env bash
# scripts/create_ephemeral_env.sh
set -euo pipefail

# ------------------------------------------------------------
# Variables
# ------------------------------------------------------------
PROJECT_NAME="${1:-$(git rev-parse --abbrev-ref HEAD | tr '/' '-')}"
COMPOSE_FILE="docker-compose.ephemeral.yml"
ENV_FILE=".env.ephemeral"
REPO_DIR="$(git rev-parse --show-toplevel)"

# ------------------------------------------------------------
# Nettoyage si d√©j√† existant
# ------------------------------------------------------------
echo "üßπ Nettoyage de l'environnement pr√©c√©dent : ${PROJECT_NAME}"
docker compose -p "${PROJECT_NAME}" -f "${REPO_DIR}/${COMPOSE_FILE}" down --remove-orphans --volumes || true

# ------------------------------------------------------------
# G√©n√©ration du .env √©ph√©m√®re
# ------------------------------------------------------------
echo "üîê G√©n√©ration du .env √©ph√©m√®re"
cat > "${REPO_DIR}/${ENV_FILE}" <<EOF
COMPOSE_PROJECT_NAME=${PROJECT_NAME}
REDIS_URL=redis://redis:6379
OLLAMA_URL=http://ollama:11434
ENCRYPTION_KEY=$(python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
JWT_SECRET_KEY=$(python -c "import secrets; print(secrets.token_urlsafe(64))")
EOF

# ------------------------------------------------------------
# Lancement de l'environnement
# ------------------------------------------------------------
echo "üöÄ Lancement de l'environnement : ${PROJECT_NAME}"
docker compose -p "${PROJECT_NAME}" -f "${REPO_DIR}/${COMPOSE_FILE}" --env-file "${REPO_DIR}/${ENV_FILE}" up -d --build

# ------------------------------------------------------------
# Attente de sant√©
# ------------------------------------------------------------
echo "‚è≥ Attente de la sant√© des services‚Ä¶"
timeout=120
until curl -sf http://localhost:8000/health > /dev/null; do
    timeout=$((timeout - 5))
    if [ $timeout -le 0 ]; then
        echo "‚ùå Timeout : l'environnement n'a pas d√©marr√© correctement"
        docker compose -p "${PROJECT_NAME}" -f "${REPO_DIR}/${COMPOSE_FILE}" logs
        exit 1
    fi
    sleep 5
done

echo "‚úÖ Environnement √©ph√©m√®re pr√™t !"
echo "   Nom du projet : ${PROJECT_NAME}"
echo "   API disponible : http://localhost:8000"
echo "   Logs : docker compose -p ${PROJECT_NAME} -f ${COMPOSE_FILE} logs -f"
```

---

## Fichier : `scripts\setup\generate_keys.py`

```python
# scripts/setup/generate_keys.py
#!/usr/bin/env python3
"""Script pour g√©n√©rer les secrets n√©cessaires et les enregistrer dans un fichier .env.

Ce script facilite la configuration initiale d'un nouvel environnement en g√©n√©rant
automatiquement les cl√©s cryptographiques requises (pour JWT, chiffrement, etc.).
Il propose de cr√©er ou d'√©craser un fichier `.env` √† la racine du projet avec
les cl√©s g√©n√©r√©es et des placeholders pour les cl√©s d'API externes.

Utilisation :
    python -m scripts.generate_keys
"""

import os
import logging
from pathlib import Path

# Assurez-vous que le chemin du projet est dans sys.path pour l'import
import sys
sys.path.append(str(Path(__file__).parent.parent))

from src.security.secret_manager import SecretsManager

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


def main():
    """Fonction principale du script."""
    env_file = Path(".env")

    # V√©rifie si le fichier .env existe d√©j√† et demande confirmation pour l'√©craser.
    if env_file.exists():
        logger.warning(f"‚ö†Ô∏è  Le fichier `{env_file}` existe d√©j√† !")
        response = input("Voulez-vous l'√©craser avec de nouvelles cl√©s ? [y/N]: ")
        if response.lower() != 'y':
            logger.info("Op√©ration annul√©e.")
            return

    # G√©n√©ration des secrets n√©cessaires.
    secrets = {
        "JWT_SECRET_KEY": SecretsManager.generate_secret_key(),
        "ENCRYPTION_KEY": SecretsManager.generate_secret_key(),
        "ALM_API_KEY": "",
        "OPENAI_API_KEY": "",
        "AZURE_CONTENT_SAFETY_KEY": ""
    }

    # √âcriture s√©curis√©e dans le fichier .env.
    try:
        with open(env_file, "w", encoding='utf-8') as f:
            f.write("# Fichier de secrets pour le projet Altiora\n")
            f.write("# ATTENTION : NE PAS COMMIT CE FICHIER DANS GIT !\n\n")
            for key, value in secrets.items():
                f.write(f"{key}={value}\n")

        logger.info(f"‚úÖ Les secrets ont √©t√© g√©n√©r√©s avec succ√®s dans `{env_file}`.")
        logger.info("üîí N'oubliez pas de remplir les valeurs vides pour les cl√©s d'API externes.")
        logger.info("üîí Assurez-vous que le fichier `.env` est bien list√© dans votre `.gitignore`.")
    except (IOError, OSError) as e:
        logger.error(f"‚ùå Erreur lors de l'√©criture dans le fichier .env : {e}")


if __name__ == "__main__":
    main()

```

---

## Fichier : `scripts\setup\run_performance_tests.sh`

```shell
#!/bin/bash
# scripts/run_performance_tests.sh

set -e

echo "üöÄ Tests de Performance Altiora"
echo "==============================="

# Configuration
REDIS_URL="redis://localhost:6379"
OLLAMA_HOST="http://localhost:11434"
MAX_CPU=85
MAX_MEMORY=25

# V√©rifier les services
echo "üìã V√©rification des services..."
if ! curl -s $OLLAMA_HOST/health &>/dev/null; then
    echo "‚ùå Ollama non accessible"
    exit 1
fi

if ! redis-cli ping &>/dev/null; then
    echo "‚ùå Redis non accessible"
    exit 1
fi

# Lancer les tests
echo "üî• Lancement des tests de charge..."

# Test 1: Charge l√©g√®re
echo "üìä Test 1: Charge l√©g√®re (5 concurrents)"
pytest tests/performance/test_load_testing.py::test_cpu_load_pipeline -v -s \
    --performance-concurrent=5 --performance-duration=60

# Test 2: Charge moyenne
echo "üìä Test 2: Charge moyenne (15 concurrents)"
pytest tests/performance/test_load_testing.py::test_cpu_load_pipeline -v -s \
    --performance-concurrent=15 --performance-duration=120

# Test 3: Charge lourde
echo "üìä Test 3: Charge lourde (30 concurrents)"
pytest tests/performance/test_load_testing.py::test_cpu_load_pipeline -v -s \
    --performance-concurrent=30 --performance-duration=300

# G√©n√©rer le rapport
echo "üìà G√©n√©ration du rapport de performance..."
python scripts/generate_performance_report.py

echo "‚úÖ Tests de performance termin√©s!"
echo "üìã Rapport disponible dans: reports/performance_report.html"
```

---

## Fichier : `scripts\setup\setup_integration_tests.sh`

```shell
#!/bin/bash
# scripts/setup_integration_tests.sh

echo "üîÑ Configuration des tests d'int√©gration..."

# V√©rifier Docker
if ! command -v docker-compose &> /dev/null; then
    echo "‚ùå Docker Compose non install√©"
    exit 1
fi

# Lancer les services
echo "üì¶ Lancement des services Docker..."
docker-compose down
docker-compose up -d --build --wait

# V√©rifier la sant√© des services
echo "üîç V√©rification des services..."
./scripts/validate_microservices.sh

# Pr√©parer les mod√®les Ollama
echo "ü§ñ V√©rification des mod√®les Ollama..."
if ! curl -s http://localhost:11434/api/tags | grep -q "qwen3"; then
    echo "üì• T√©l√©chargement de qwen3:32b-q4_K_M..."
    ollama pull qwen3:32b-q4_K_M
fi

if ! curl -s http://localhost:11434/api/tags | grep -q "starcoder2"; then
    echo "üì• T√©l√©chargement de starcoder2:15b-q8_0..."
    ollama pull starcoder2:15b-q8_0
fi

echo "‚úÖ Environnement de test d'int√©gration pr√™t!"
```

---

## Fichier : `scripts\setup\start_dev.sh`

```shell
# scripts/start_dev.sh

echo "üöÄ D√©marrage Altiora avec structure refactoris√©e"
# Lancer les services dans l'ordre
docker-compose up -d redis ollama
docker-compose up dash prometheus
docker-compose up -d doctoplus-ocr alm-connector excel-processor playwright-runner
# Attendre la sant√© des services
./scripts/validate_microservices.sh
```

---

## Fichier : `scripts\setup\validate_setup.py`

```python
# scripts/setup/validate_setup.py
#!/usr/bin/env python3
"""Script de validation complet pour l'environnement de d√©veloppement Altiora.

Ce script v√©rifie que tous les composants critiques de l'environnement sont
correctement install√©s et configur√©s. Il est con√ßu pour √™tre ex√©cut√© apr√®s
l'installation initiale ou pour diagnostiquer des probl√®mes d'environnement.

Les v√©rifications incluent :
- La coh√©rence entre les mod√®les Ollama configur√©s et ceux r√©ellement install√©s.
- La pr√©sence et la version des d√©pendances Python critiques (PyTorch, etc.).
"""
import sys
import subprocess
import logging
from pathlib import Path

import yaml

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class SetupValidator:
    """Valide l'installation compl√®te d'Altiora."""

    def __init__(self):
        """Initialise le validateur."""
        self.errors: list[str] = []
        self.warnings: list[str] = []

    def validate_models(self):
        """V√©rifie la coh√©rence des mod√®les Ollama."""
        logger.info("üîç V√©rification des mod√®les Ollama...")
        try:
            # Charge la configuration des mod√®les depuis le fichier YAML.
            models_config_path = Path("configs/models.yaml")
            if not models_config_path.exists():
                self.errors.append(f"Le fichier de configuration des mod√®les `{models_config_path}` est introuvable.")
                return

            models_config = yaml.safe_load(models_config_path.read_text())
            required_models = [conf['ollama_tag'] for conf in models_config.get("models", {}).values()]

            # R√©cup√®re la liste des mod√®les install√©s via la commande Ollama.
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                check=True
            )
            installed_models = result.stdout

            # Compare les mod√®les requis et install√©s.
            for model_tag in required_models:
                if model_tag not in installed_models:
                    self.errors.append(
                        f"Mod√®le manquant : `{model_tag}` n'est pas install√© dans Ollama. "
                        f"Ex√©cutez `ollama pull {model_tag}`."
                    )
                else:
                    logger.info(f"  - ‚úÖ Mod√®le `{model_tag}` trouv√©.")

        except FileNotFoundError:
            self.errors.append("La commande `ollama` n'a pas √©t√© trouv√©e. Assurez-vous qu'Ollama est install√© et dans le PATH.")
        except (subprocess.CalledProcessError, yaml.YAMLError, KeyError) as e:
            self.errors.append(f"Une erreur est survenue lors de la validation des mod√®les : {e}")

    def validate_dependencies(self):
        """V√©rifie la pr√©sence et la version des d√©pendances Python critiques."""
        logger.info("\nüîç V√©rification des d√©pendances Python...")
        try:
            import torch
            logger.info(f"  - ‚úÖ PyTorch version {torch.__version__} trouv√©.")
            if not torch.__version__.startswith("2."):
                self.warnings.append(
                    f"La version de PyTorch ({torch.__version__}) n'est pas la version 2.x recommand√©e."
                )

            import transformers
            logger.info(f"  - ‚úÖ Transformers version {transformers.__version__} trouv√©.")

            import peft
            logger.info(f"  - ‚úÖ PEFT version {peft.__version__} trouv√©.")

        except ImportError as e:
            self.errors.append(f"D√©pendance Python manquante : {e}. Ex√©cutez `pip install -r requirements.txt`.")

    def run(self) -> bool:
        """Ex√©cute toutes les validations et affiche un r√©sum√©.

        Returns:
            True si aucune erreur n'a √©t√© trouv√©e, False sinon.
        """
        self.validate_models()
        self.validate_dependencies()

        print("\n" + "-"*50)
        if self.errors:
            logger.error("‚ùå DES ERREURS CRITIQUES ONT √âT√â TROUV√âES :")
            for error in self.errors:
                logger.error(f"  - {error}")

        if self.warnings:
            logger.warning("\n‚ö†Ô∏è  AVERTISSEMENTS :")
            for warning in self.warnings:
                logger.warning(f"  - {warning}")
        
        if not self.errors and not self.warnings:
            logger.info("‚úÖ L'environnement semble correctement configur√© !")
        
        return not self.errors


if __name__ == "__main__":
    validator = SetupValidator()
    logger.info("Lancement de la validation de l'environnement Altiora...")
    if not validator.run():
        logger.error("\nValidation √©chou√©e. Veuillez corriger les erreurs ci-dessus.")
        sys.exit(1)
    else:
        logger.info("\nValidation r√©ussie.")

```

---

## Fichier : `scripts\training\auto_fine_tuner.py`

```python
# scripts/training/auto_fine_tuner.py
from peft import LoraConfig, get_peft_model
from transformers import TrainingArguments, Trainer

from backend.altiora.core.feedback.feedback_collector import FeedbackCollector

TRIGGER_THRESHOLD = 50  # votes ‚â• 3


async def should_trigger():
    collector = FeedbackCollector()
    batch = await collector.get_batch()
    return len(batch) >= TRIGGER_THRESHOLD


async def schedule_retrain():
    dataset = await build_dataset_from_feedback()

    lora_config = LoraConfig(
        r=32,
        lora_alpha=64,  # 2√ór (meilleur loss) [^10^]
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        task_type="CAUSAL_LM"
    )

    training_args = TrainingArguments(
        output_dir="./models/lora_adapters",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        fp16=True,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_steps=10,
        report_to="mlflow"
    )

    trainer = Trainer(
        model=get_peft_model(base_model, lora_config),
        args=training_args,
        train_dataset=dataset,
        eval_dataset=dataset.select(range(50))
    )

    trainer.train()
    trainer.save_model("./models/lora_adapters/latest")
```

---

