# cli/commands/benchmark.py
"""Commande `benchmark` pour la CLI Altiora.

Ce module fournit une commande pour lancer les tests de performance
du projet. Il utilise `pytest-benchmark` pour ex√©cuter des benchmarks
et g√©n√©rer un rapport d√©taill√©.
"""

import click
import subprocess
import logging

logger = logging.getLogger(__name__)


@click.command()
@click.option("--runs", default=5, help="Nombre de runs par benchmark pour chaque test de performance.")
def benchmark(runs: int):
    """Lance les tests de performance de l'application Altiora."

    Cette commande ex√©cute les tests situ√©s dans le r√©pertoire `tests/performance`
    en utilisant `pytest-benchmark`. Un rapport JSON est g√©n√©r√© √† la fin de l'ex√©cution.

    Args:
        runs: Le nombre de fois que chaque test de performance sera ex√©cut√©.
    """
    click.echo("üìä Altiora Benchmark ‚Äì D√©marrage des tests de performance...")
    cmd = [
        "pytest",
        "tests/performance", # Cible le r√©pertoire des tests de performance.
        "--benchmark-only", # Ex√©cute uniquement les tests marqu√©s comme benchmarks.
        f"--benchmark-warmup-iterations={runs}", # Nombre d'it√©rations de chauffe.
        f"--benchmark-json=benchmark-report.json", # Fichier de sortie du rapport JSON.
    ]
    try:
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        click.echo("‚úÖ Benchmark termin√©. Rapport g√©n√©r√© dans `benchmark-report.json`.")
        logger.info("Benchmark termin√© avec succ√®s.")
    except FileNotFoundError:
        click.echo("‚ùå Erreur: `pytest` ou `pytest-benchmark` n'est pas install√©. Assurez-vous d'avoir install√© les d√©pendances de d√©veloppement.")
        logger.error("pytest ou pytest-benchmark non trouv√©.")
    except subprocess.CalledProcessError as e:
        click.echo(f"‚ùå Erreur lors de l'ex√©cution des benchmarks : {e.stderr}")
        logger.error(f"Erreur lors de l'ex√©cution des benchmarks : {e.stderr}")
    except Exception as e:
        click.echo(f"‚ùå Une erreur inattendue est survenue : {e}")
        logger.error(f"Erreur inattendue lors de l'ex√©cution des benchmarks : {e}")


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    import sys
    import os

    # Pour la d√©monstration, nous allons simuler un fichier de test de performance.
    # En temps normal, ces fichiers existeraient d√©j√† dans `tests/performance`.
    temp_test_dir = Path("tests/performance")
    temp_test_dir.mkdir(parents=True, exist_ok=True)
    (temp_test_dir / "test_example_benchmark.py").write_text("""
import pytest
import time

@pytest.mark.benchmark(group="example")
def test_simple_operation():
    time.sleep(0.01) # Simule une op√©ration rapide.

@pytest.mark.benchmark(group="example")
def test_complex_operation():
    time.sleep(0.1) # Simule une op√©ration plus lente.
""")

    # Ex√©cute la commande benchmark via le syst√®me.
    # Note: Cela n√©cessite que `pytest` et `pytest-benchmark` soient install√©s dans l'environnement.
    print("\n--- Lancement de la d√©monstration du benchmark ---")
    try:
        # Simule l'appel de la commande CLI.
        # sys.argv = ["cli/commands/benchmark.py", "--runs", "2"]
        # benchmark() # Appel direct de la fonction pour la d√©mo.
        # Ou via subprocess pour simuler l'appel CLI complet.
        subprocess.run([sys.executable, __file__, "--runs", "2"], check=True)

    except Exception as e:
        print(f"Une erreur est survenue lors de la d√©monstration : {e}")
    finally:
        # Nettoyage des fichiers temporaires.
        if temp_test_dir.exists():
            import shutil
            shutil.rmtree(temp_test_dir)
        if Path("benchmark-report.json").exists():
            Path("benchmark-report.json").unlink()
        print("D√©monstration du benchmark termin√©e.")