# tests/regression/test_regression_suite.py
"""Suite de tests de r√©gression automatiques pour Altiora.

Ce module impl√©mente une suite de tests de r√©gression qui compare les
r√©sultats actuels de l'application avec des r√©f√©rences (baselines) stock√©es.
Il permet de d√©tecter les r√©gressions fonctionnelles ou de performance
introduites par de nouvelles modifications de code. La suite peut √™tre
configur√©e pour mettre √† jour les baselines et g√©n√©rer des rapports d√©taill√©s.
"""

import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

import pytest
import yaml

from src.orchestrator import Orchestrator
from src.models.qwen3.qwen3_interface import Qwen3OllamaInterface
from src.models.starcoder2.starcoder2_interface import StarCoder2OllamaInterface, TestType

logger = logging.getLogger(__name__)


class RegressionTestResult:
    """Repr√©sente le r√©sultat d'un test de r√©gression individuel."""

    def __init__(self, test_name: str, status: str, metrics: Dict[str, Any],
                 diff: Optional[str] = None):
        """Initialise un r√©sultat de test de r√©gression."

        Args:
            test_name: Le nom du test de r√©gression.
            status: Le statut du test ('PASS', 'FAIL', 'NEW').
            metrics: Un dictionnaire de m√©triques associ√©es au test.
            diff: Un string repr√©sentant le 'diff' si le test a √©chou√© (optionnel).
        """
        self.test_name = test_name
        self.status = status  # PASS, FAIL, NEW
        self.metrics = metrics
        self.diff = diff
        self.timestamp = datetime.now().isoformat()


class RegressionSuite:
    """G√®re l'ex√©cution et l'analyse des tests de r√©gression."""

    def __init__(self, config_path: str = "tests/regression/regression_config.yaml"):
        """Initialise la suite de tests de r√©gression."

        Args:
            config_path: Le chemin vers le fichier de configuration des tests de r√©gression.
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.baseline_path = Path("tests/regression/baselines")
        self.baseline_path.mkdir(parents=True, exist_ok=True)
        self.results_path = Path("tests/regression/results")
        self.results_path.mkdir(parents=True, exist_ok=True)

    def _load_config(self) -> Dict[str, Any]:
        """Charge la configuration des tests de r√©gression depuis le fichier YAML."

        Si le fichier n'existe pas, une configuration par d√©faut est cr√©√©e.

        Returns:
            Un dictionnaire contenant la configuration des tests de r√©gression.
        """
        if not self.config_path.exists():
            logger.info(f"Fichier de configuration de r√©gression non trouv√© √† {self.config_path}. Cr√©ation d'une configuration par d√©faut.")
            default_config = {
                "thresholds": {
                    "max_time_increase": 1.2,  # Augmentation de temps max de 20%.
                    "min_scenarios": 1,
                    "min_tests_generated": 1,
                    "code_similarity": 0.8 # Seuil de similarit√© pour le code g√©n√©r√©.
                },
                "models": {
                    "qwen3": {
                        "model_name": "qwen3-sfd-analyzer",
                        "test_cases": ["scenario_extraction", "test_matrix"]
                    },
                    "starcoder2": {
                        "model_name": "starcoder2-playwright",
                        "test_cases": ["code_generation", "syntax_validity"]
                    }
                },
                "services": {
                    "health_check": True,
                    "response_time": 30  # secondes max.
                },
                "update_baselines": False, # Option pour mettre √† jour les baselines.
                "generate_report": True # Option pour g√©n√©rer un rapport HTML.
            }
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(default_config, f, indent=2)
        return yaml.safe_load(self.config_path.read_text(encoding='utf-8'))

    async def run_full_regression(self) -> Dict[str, Any]:
        """Ex√©cute la suite compl√®te de tests de r√©gression."

        Returns:
            Un dictionnaire contenant les r√©sultats d√©taill√©s de tous les tests de r√©gression,
            un r√©sum√© et des m√©triques de performance.
        """
        logger.info("üîÑ D√©but des tests de r√©gression...")

        results = {
            "timestamp": datetime.now().isoformat(),
            "tests": [],
            "summary": {"passed": 0, "failed": 0, "new": 0},
            "performance_metrics": {}
        }

        # Ex√©cution des tests de r√©gression pour les mod√®les LLM.
        model_results = await self._test_models_regression()
        results["tests"].extend(model_results)

        # Ex√©cution des tests de r√©gression pour le pipeline complet.
        pipeline_results = await self._test_pipeline_regression()
        results["tests"].extend(pipeline_results)

        # Ex√©cution des tests de r√©gression de performance.
        perf_results = await self._test_performance_regression()
        results["performance_metrics"] = perf_results

        # Mise √† jour du r√©sum√© des r√©sultats.
        results["summary"] = {
            "passed": sum(1 for t in results["tests"] if t.status == "PASS"),
            "failed": sum(1 for t in results["tests"] if t.status == "FAIL"),
            "new": sum(1 for t in results["tests"] if t.status == "NEW")
        }

        # G√©n√©ration du rapport HTML si configur√©.
        if self.config.get("generate_report", True):
            self._generate_regression_report(results)

        # Mise √† jour des baselines si configur√©.
        if self.config.get("update_baselines", False):
            await self._update_baselines(results)

        logger.info("‚úÖ Tests de r√©gression termin√©s.")
        return results

    async def _test_models_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression sp√©cifiques aux mod√®les LLM (Qwen3, StarCoder2)."

        Returns:
            Une liste de `RegressionTestResult` pour les tests des mod√®les.
        """
        results = []

        logger.info("Ex√©cution des tests de r√©gression Qwen3...")
        qwen3_results = await self._test_qwen3_regression()
        results.extend(qwen3_results)

        logger.info("Ex√©cution des tests de r√©gression StarCoder2...")
        starcoder_results = await self._test_starcoder2_regression()
        results.extend(starcoder_results)

        return results

    async def _test_qwen3_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le mod√®le Qwen3."

        Returns:
            Une liste de `RegressionTestResult` pour les tests Qwen3.
        """
        results = []
        qwen3 = Qwen3OllamaInterface() # Assurez-vous que cette instance est correctement configur√©e.
        await qwen3.initialize()

        try:
            test_cases = self._load_test_cases("qwen3")

            for test_case in test_cases:
                test_name = f"qwen3_{test_case['name']}"
                # Ex√©cute le test Qwen3 et compare avec la baseline.
                result = await self._run_single_qwen3_test(qwen3, test_case, test_name)
                results.append(result)

        finally:
            await qwen3.close()

        return results

    async def _test_starcoder2_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le mod√®le StarCoder2."

        Returns:
            Une liste de `RegressionTestResult` pour les tests StarCoder2.
        """
        results = []
        starcoder = StarCoder2OllamaInterface() # Assurez-vous que cette instance est correctement configur√©e.
        await starcoder.initialize()

        try:
            test_cases = self._load_test_cases("starcoder2")

            for test_case in test_cases:
                test_name = f"starcoder2_{test_case['name']}"
                # Ex√©cute le test StarCoder2 et compare avec la baseline.
                result = await self._run_single_starcoder_test(starcoder, test_case, test_name)
                results.append(result)

        finally:
            await starcoder.close()

        return results

    async def _test_pipeline_regression(self) -> List[RegressionTestResult]:
        """Ex√©cute les tests de r√©gression pour le pipeline complet."

        Returns:
            Une liste de `RegressionTestResult` pour les tests de pipeline.
        """
        results = []
        orchestrator = Orchestrator() # Assurez-vous que cette instance est correctement configur√©e.
        await orchestrator.initialize()

        try:
            sfd_files = Path("tests/regression/fixtures/sample_sfd").glob("*")

            for sfd_file in sfd_files:
                test_name = f"pipeline_{sfd_file.stem}"
                # Ex√©cute le test de pipeline et compare avec la baseline.
                result = await self._run_pipeline_regression_test(orchestrator, sfd_file, test_name)
                results.append(result)

        finally:
            await orchestrator.close()

        return results

    async def _test_performance_regression(self) -> Dict[str, Any]:
        """Ex√©cute les tests de r√©gression de performance."

        Compare les m√©triques de performance actuelles avec les baselines.

        Returns:
            Un dictionnaire contenant les comparaisons de m√©triques de performance.
        """
        logger.info("Ex√©cution des tests de r√©gression de performance...")
        baseline_file = self.baseline_path / "performance.json"

        baseline: Dict[str, Any] = {}
        if baseline_file.exists():
            try:
                baseline = json.loads(baseline_file.read_text(encoding='utf-8'))
            except json.JSONDecodeError as e:
                logger.error(f"Erreur de lecture de la baseline de performance {baseline_file}: {e}")

        # Mesure les performances actuelles.
        current_metrics = await self._measure_performance()

        comparisons = {}
        for metric, current_value in current_metrics.items():
            if metric in baseline:
                baseline_value = baseline[metric]
                ratio = current_value / baseline_value if baseline_value != 0 else float('inf')
                status = "PASS"
                if metric.endswith("_time") or metric.endswith("_duration") or metric.endswith("_usage_mb"):
                    # Pour les m√©triques o√π une valeur plus petite est meilleure.
                    if ratio > self.config["thresholds"]["max_time_increase"]:
                        status = "FAIL"
                else:
                    # Pour les m√©triques o√π une valeur plus grande est meilleure (ex: d√©bit).
                    if ratio < (1 / self.config["thresholds"]["max_time_increase"]):
                        status = "FAIL"

                comparisons[metric] = {
                    "baseline": baseline_value,
                    "current": current_value,
                    "ratio": ratio,
                    "status": status
                }
            else:
                comparisons[metric] = {
                    "current": current_value,
                    "status": "NEW"
                }

        return comparisons

    async def _run_single_qwen3_test(self, qwen3: Qwen3OllamaInterface,
                                     test_case: Dict[str, Any], test_name: str) -> RegressionTestResult:
        """Ex√©cute un test unique pour Qwen3 et compare son r√©sultat avec la baseline."

        Args:
            qwen3: L'instance de l'interface Qwen3.
            test_case: Le dictionnaire du cas de test.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du test Qwen3.
        # Assurez-vous que `analyze_sfd` prend un `SFDAnalysisRequest`.
        from src.models.sfd_models import SFDAnalysisRequest
        sfd_request = SFDAnalysisRequest(content=test_case["input"], extraction_type=test_case.get("extraction_type", "complete"))
        result = await qwen3.analyze_sfd(sfd_request)

        return self._compare_with_baseline(test_name, result, baseline_file)

    async def _run_single_starcoder_test(self, starcoder: StarCoder2OllamaInterface,
                                         test_case: Dict[str, Any], test_name: str) -> RegressionTestResult:
        """Ex√©cute un test unique pour StarCoder2 et compare son r√©sultat avec la baseline."

        Args:
            starcoder: L'instance de l'interface StarCoder2.
            test_case: Le dictionnaire du cas de test.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du test StarCoder2.
        result = await starcoder.generate_playwright_test(
            scenario=test_case["scenario"],
            config=test_case.get("config", {}),
            test_type=TestType.E2E
        )

        return self._compare_with_baseline(test_name, result, baseline_file)

    async def _run_pipeline_regression_test(self, orchestrator: Orchestrator,
                                            sfd_file: Path, test_name: str) -> RegressionTestResult:
        """Ex√©cute un test de r√©gression du pipeline complet et compare son r√©sultat avec la baseline."

        Args:
            orchestrator: L'instance de l'orchestrateur.
            sfd_file: Le chemin vers le fichier SFD √† traiter.
            test_name: Le nom du test.

        Returns:
            Un objet `RegressionTestResult`.
        """
        baseline_file = self.baseline_path / f"{test_name}.json"

        # Ex√©cution actuelle du pipeline.
        result = await orchestrator.run_full_pipeline(str(sfd_file))

        return self._compare_with_baseline(test_name, result, baseline_file)

    def _compare_with_baseline(self, test_name: str, current_result: Dict[str, Any],
                               baseline_file: Path) -> RegressionTestResult:
        """Compare les r√©sultats actuels d'un test avec sa baseline."

        Args:
            test_name: Le nom du test.
            current_result: Le dictionnaire des r√©sultats actuels du test.
            baseline_file: Le chemin vers le fichier de baseline.

        Returns:
            Un objet `RegressionTestResult` indiquant le statut du test (PASS, FAIL, NEW).
        """
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "result_hash": hashlib.md5(json.dumps(current_result, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        }

        if not baseline_file.exists():
            # Si aucune baseline n'existe, le test est consid√©r√© comme nouveau et la baseline est cr√©√©e.
            logger.info(f"Cr√©ation d'une nouvelle baseline pour le test : {test_name}")
            baseline_file.write_text(json.dumps(current_result, indent=2, ensure_ascii=False))
            return RegressionTestResult(test_name, "NEW", metrics)

        try:
            baseline = json.loads(baseline_file.read_text(encoding='utf-8'))
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de lecture de la baseline {baseline_file}: {e}. Le test sera marqu√© comme FAIL.")
            return RegressionTestResult(test_name, "FAIL", metrics, diff=f"Erreur de lecture de la baseline: {e}")

        # Compare les r√©sultats actuels avec la baseline.
        if self._are_results_equivalent(current_result, baseline):
            logger.info(f"Test de r√©gression {test_name} : PASS.")
            return RegressionTestResult(test_name, "PASS", metrics)
        else:
            diff = self._generate_diff(baseline, current_result)
            logger.warning(f"Test de r√©gression {test_name} : FAIL. Diff√©rences d√©tect√©es.")
            return RegressionTestResult(test_name, "FAIL", metrics, diff)

    def _are_results_equivalent(self, current: Dict[str, Any], baseline: Dict[str, Any]) -> bool:
        """V√©rifie si deux dictionnaires de r√©sultats sont √©quivalents."

        Pour l'instant, la comparaison est bas√©e sur le hachage MD5 du JSON s√©rialis√©.
        Une logique de comparaison plus sophistiqu√©e pourrait √™tre ajout√©e ici
        pour ignorer certaines diff√©rences (ex: horodatages, IDs dynamiques).

        Args:
            current: Le dictionnaire des r√©sultats actuels.
            baseline: Le dictionnaire des r√©sultats de la baseline.

        Returns:
            True si les r√©sultats sont consid√©r√©s comme √©quivalents, False sinon.
        """
        current_hash = hashlib.md5(json.dumps(current, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        baseline_hash = hashlib.md5(json.dumps(baseline, sort_keys=True, ensure_ascii=False).encode()).hexdigest()
        return current_hash == baseline_hash

    def _generate_diff(self, baseline: Dict[str, Any], current: Dict[str, Any]) -> str:
        """G√©n√®re un 'diff' lisible entre deux dictionnaires de r√©sultats."

        Args:
            baseline: Le dictionnaire des r√©sultats de la baseline.
            current: Le dictionnaire des r√©sultats actuels.

        Returns:
            Une cha√Æne de caract√®res repr√©sentant le 'diff' unifi√©.
        """
        import difflib
        baseline_str = json.dumps(baseline, indent=2, sort_keys=True, ensure_ascii=False).splitlines()
        current_str = json.dumps(current, indent=2, sort_keys=True, ensure_ascii=False).splitlines()

        diff = difflib.unified_diff(
            baseline_str, current_str,
            fromfile='baseline', tofile='current',
            lineterm=''
        )
        return '\n'.join(diff)

    def _load_test_cases(self, model_type: str) -> List[Dict[str, Any]]:
        """Charge les cas de test sp√©cifiques √† un type de mod√®le depuis les fixtures."

        Args:
            model_type: Le type de mod√®le (ex: 'qwen3', 'starcoder2').

        Returns:
            Une liste de dictionnaires, chaque dictionnaire repr√©sentant un cas de test.
        """
        test_cases_dir = Path("tests/regression/fixtures") / model_type
        test_cases: List[Dict[str, Any]] = []

        if not test_cases_dir.exists():
            logger.warning(f"R√©pertoire de cas de test non trouv√© : {test_cases_dir}")
            return []

        for file in test_cases_dir.glob("*.json"):
            try:
                with open(file, "r", encoding="utf-8") as f:
                    test_cases.extend(json.loads(f.read()))
            except json.JSONDecodeError as e:
                logger.error(f"Erreur de lecture du fichier de cas de test {file}: {e}")
        return test_cases

    async def _measure_performance(self) -> Dict[str, float]:
        """Mesure les m√©triques de performance actuelles de l'application (stub)."

        Dans une impl√©mentation r√©elle, cette m√©thode appellerait des outils
        de profiling ou des endpoints de m√©triques pour collecter des donn√©es
        sur le temps de d√©marrage, le temps de r√©ponse moyen, l'utilisation m√©moire, etc.

        Returns:
            Un dictionnaire de m√©triques de performance.
        """
        logger.info("Mesure des performances actuelles (stub)...")
        # Impl√©mentation simplifi√©e pour la d√©monstration.
        import asyncio
        await asyncio.sleep(1) # Simule un d√©lai de mesure.
        return {
            "startup_time": 2.5,
            "average_response_time": 1.2,
            "memory_usage_mb": 512
        }

    def _generate_regression_report(self, results: Dict[str, Any]):
        """G√©n√®re un rapport HTML d√©taill√© des r√©sultats de la r√©gression."

        Args:
            results: Le dictionnaire complet des r√©sultats de la suite de r√©gression.
        """
        report_file = self.results_path / f"regression_report_{datetime.now():%Y%m%d_%H%M%S}.html"

        # Construction du contenu HTML du rapport.
        test_rows_html = []
        for t in results['tests']:
            diff_html = f"<pre>{t.diff}</pre>" if t.diff else ""
            test_rows_html.append(f"""
            <tr>
                <td>{t.test_name}</td>
                <td class='{t.status.lower()}'>{t.status}</td>
                <td>{json.dumps(t.metrics, ensure_ascii=False)}</td>
                <td>{diff_html}</td>
            </tr>
            """)

        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Altiora Regression Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .pass {{ color: green; font-weight: bold; }}
        .fail {{ color: red; font-weight: bold; }}
        .new {{ color: blue; font-weight: bold; }}
        table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; vertical-align: top; }}
        th {{ background-color: #f2f2f2; }}
        pre {{ white-space: pre-wrap; word-wrap: break-word; background-color: #eee; padding: 10px; border-radius: 5px; }}
    </style>
</head>
<body>
    <h1>Rapport de Tests de R√©gression Altiora</h1>
    <p>G√©n√©r√© le : {results['timestamp']}</p>

    <h2>R√©sum√©</h2>
    <ul>
        <li class="pass">R√©ussis : {results['summary']['passed']}</li>
        <li class="fail">√âchou√©s : {results['summary']['failed']}</li>
        <li class="new">Nouveaux : {results['summary']['new']}</li>
    </ul>

    <h2>R√©sultats des Tests</h2>
    <table>
        <thead>
            <tr><th>Test</th><th>Statut</th><th>M√©triques</th><th>Diff√©rences</th></tr>
        </thead>
        <tbody>
            {''.join(test_rows_html)}
        </tbody>
    </table>

    <h2>M√©triques de Performance</h2>
    <pre>{json.dumps(results['performance_metrics'], indent=2, ensure_ascii=False)}</pre>
</body>
</html>
"""
        try:
            report_file.write_text(html_content, encoding='utf-8')
            logger.info(f"Rapport de r√©gression HTML g√©n√©r√© : {report_file}")
        except (IOError, OSError) as e:
            logger.error(f"Erreur lors de l'√©criture du rapport HTML : {e}")

    async def _update_baselines(self, results: Dict[str, Any]):
        """Met √† jour les fichiers de baseline avec les r√©sultats actuels des tests marqu√©s comme 'NEW' ou 'FAIL'."

        Args:
            results: Le dictionnaire complet des r√©sultats de la suite de r√©gression.
        """
        logger.info("Mise √† jour des baselines...")
        for test_result in results['tests']:
            if test_result.status == "NEW" or test_result.status == "FAIL":
                baseline_file = self.baseline_path / f"{test_result.test_name}.json"
                try:
                    # Sauvegarde le r√©sultat actuel comme nouvelle baseline.
                    json.dumps(test_result.metrics, indent=2, ensure_ascii=False) # Assurez-vous que c'est le bon contenu √† sauvegarder.
                    baseline_file.write_text(json.dumps(test_result.metrics, indent=2, ensure_ascii=False), encoding='utf-8')
                    logger.info(f"Baseline mise √† jour pour {test_result.test_name} : {baseline_file}")
                except (IOError, OSError) as e:
                    logger.error(f"Erreur lors de la mise √† jour de la baseline {baseline_file}: {e}")


# Configuration pour pytest (pour l'ex√©cution via `pytest tests/regression/test_regression_suite.py`)
@pytest.mark.regression
@pytest.mark.asyncio
async def test_full_regression_suite(wait_for_services):
    """Test de r√©gression complet ex√©cut√© par pytest."

    Cette fonction est le point d'entr√©e pour Pytest. Elle initialise la suite
    de r√©gression et ex√©cute tous les tests d√©finis.
    """
    suite = RegressionSuite()
    results = await suite.run_full_regression()

    # Assertions finales sur le r√©sum√© de la r√©gression.
    assert results["summary"]["failed"] == 0, f"Des tests de r√©gression ont √©chou√© : {results['summary']['failed']} √©checs."
    assert results["summary"]["passed"] > 0, "Aucun test de r√©gression n'a r√©ussi."
